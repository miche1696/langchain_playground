{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "757f0f5a",
   "metadata": {},
   "source": [
    "# Processing: Load, Chunk, and Prepare for Vector DB\n",
    "\n",
    "This notebook demonstrates a step-by-step approach to loading, enriching,\n",
    "chunking, and storing documents into a vector database.\n",
    "It compares different outputs at each stage interactively with helpful visualizations.\n",
    "\n",
    "Create from scratch a chain that: \n",
    "- takes an input document. (In this case many Arxiv pdf documents.)\n",
    "- Chunks the document, keeping the metadata.\n",
    "- Embed the document chunks.\n",
    "- Saves the embeddings into a vector DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f71e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c404115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Supports all arguments of `ArxivAPIWrapper`\n",
    "loader = ArxivLoader(\n",
    "    query=\"LLM\",\n",
    "    load_max_docs=50,\n",
    "    top_k_results=20\n",
    "    # doc_content_chars_max=1000,\n",
    "    # load_all_available_meta=False,\n",
    "    # ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0df738bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cca765",
   "metadata": {},
   "source": [
    "# Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "860cd865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1 : Selecting the modelID from HF and the tokenizer, choosing a model\n",
    "model_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer,\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=30,\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    #separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1f7b754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': None,\n",
       " 'metadata': {'Published': '2024-12-23',\n",
       "  'Title': 'Trustworthy and Efficient LLMs Meet Databases',\n",
       "  'Authors': 'Kyoungmin Kim, Anastasia Ailamaki',\n",
       "  'Summary': 'In the rapidly evolving AI era with large language models (LLMs) at the core,\\nmaking LLMs more trustworthy and efficient, especially in output generation\\n(inference), has gained significant attention. This is to reduce plausible but\\nfaulty LLM outputs (a.k.a hallucinations) and meet the highly increased\\ninference demands. This tutorial explores such efforts and makes them\\ntransparent to the database community. Understanding these efforts is essential\\nin harnessing LLMs in database tasks and adapting database techniques to LLMs.\\nFurthermore, we delve into the synergy between LLMs and databases, highlighting\\nnew opportunities and challenges in their intersection. This tutorial aims to\\nshare with database researchers and practitioners essential concepts and\\nstrategies around LLMs, reduce the unfamiliarity of LLMs, and inspire joining\\nin the intersection between LLMs and databases.'},\n",
       " 'page_content': 'Trustworthy and Efficient LLMs Meet Databases\\nKyoungmin Kim\\nkyoung-min.kim@epfl.ch\\nEPFL\\nSwitzerland\\nAnastasia Ailamaki\\nanastasia.ailamaki@epfl.ch\\nEPFL\\nSwitzerland\\nAbstract\\nIn the rapidly evolving AI era with large language models (LLMs) at\\nthe core, making LLMs more trustworthy and efficient, especially in\\noutput generation (inference), has gained significant attention. This\\nis to reduce plausible but faulty LLM outputs (a.k.a hallucinations)\\nand meet the highly increased inference demands. This tutorial\\nexplores such efforts and makes them transparent to the database\\ncommunity. Understanding these efforts is essential in harness-\\ning LLMs in database tasks and adapting database techniques to\\nLLMs. Furthermore, we delve into the synergy between LLMs and\\ndatabases, highlighting new opportunities and challenges in their\\nintersection. This tutorial aims to share with database researchers\\nand practitioners essential concepts and strategies around LLMs,\\nreduce the unfamiliarity of LLMs, and inspire joining in the inter-\\nsection between LLMs and databases.\\n1\\nIntroduction\\nLarge language models (LLMs) have recently transformed various\\nfields with their ability to understand and generate human-like text.\\nIn the database domain, researchers are leveraging LLMs to tackle\\ncomplex data management tasks [55, 194]. LLMs can function not\\nonly as assistants for database administrators (DBAs) [271, 340] but\\nalso as internal components of database systems, optimizing query\\nplans [8, 168] and translating natural languages to SQLs [224].\\nBeyond these applications, key concepts and advancements from\\nthe LLM community remain underexplored by database researchers.\\nThis tutorial aims to bridge that gap by focusing on enhancing the\\ntrustworthiness and efficiency of LLMs. Improving trustworthiness\\ninvolves reducing hallucinations [124] to ensure LLMs generate\\naccurate, factual responses, thereby increasing their reliability in\\ndatabase tasks requiring precise answers and reasoning. Enhancing\\nefficiency focuses on decreasing inference latency and boosting\\nthroughput.\\nInference efficiency is particularly important because, while\\ntraining LLMs demands substantial resources and expertise, infer-\\nence occurs daily across numerous users, leading to significant oper-\\national costs. For instance, OpenAI handles millions of requests, in-\\ncurring substantial monthly expenses to run ChatGPT [73, 215]. In-\\ntegrating LLMs with external data sources, such as vector databases\\nand document retrieval systems in retrieval-augmented generation\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\\nConference’17, July 2017, Washington, DC, USA\\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM\\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\\n(RAG) [154], increases the number and complexity of LLM calls, es-\\npecially with longer inputs. Recent trends in chain-of-thought and\\nmulti-path reasoning, exemplified by models like OpenAI’s o1 [205],\\nfurther amplify inference demands, as generating final answers may\\nrequire multiple LLM calls to enhance trustworthiness.\\nFrom a systems perspective, improving LLM inference efficiency\\nparallels database management system (DBMS) development, pre-\\nsenting opportunities for database researchers to contribute to\\ncreating more efficient LLMs, promoting economic and environ-\\nmental sustainability by reducing the CO2 footprint associated with\\nextensive GPU usage.\\nAfter introducing the essential ideas in making LLMs more trust-\\nworthy and efficient, we will explain the intersection of LLMs and\\ndatabases with new challenges and opportunities.\\n1.1\\nTarget Audience and Prerequisites\\nOur tutorial is designed for conference attendees, focusing on three\\nkey areas to maximize engagement:\\nTrustworthy LLMs (Section 2.1): Aimed at individuals seeking to\\neffectively utilize large language models (LLMs) in database tasks\\nwith minimal errors. Prerequisites include experience with LLMs\\nlike ChatGPT and the distinction between training and inference\\nin machine learning. No in-depth knowledge of LLM internals is\\nrequired.\\nEfficient LLMs (Section 2.2): Targeted at those interested in en-\\nhancing LLM inference efficiency or contributing to the develop-\\nment of fast LLM inference systems by applying database tech-\\nniques. Prerequisites include basic database knowledge and an\\nunderstanding of GPUs. Familiarity with Transformer architecture,\\nattention mechanisms, and key-value (KV) caching is advantageous.\\nLLMs Meet Databases (Section 2.3): Intended for participants ex-\\nploring new research opportunities at the intersection of databases\\nand LLMs. A background in databases, including OLAP, relational\\nalgebra, cost-based query optimization, and approximate/adaptive\\nquery processing, will be helpful.\\nOur goal is to bridge the gap between essential LLM knowledge\\nand the database community, enabling researchers already utiliz-\\ning LLMs to uncover and develop unexplored ideas. Rather than\\nmerely listing state-of-the-art papers, we employ consistent visu-\\nals and focus on core concepts and insights, facilitating a deeper\\nunderstanding and navigation of the evolving LLM landscape.\\n1.2\\nTutorial Length\\nThe intended length of this tutorial is 1.5 hour, with 40, 30, and 20\\nminutes each for Sections 2.1, 2.2, and 2.3, respectively.\\n2\\nTutorial Outline\\nThe tutorial is structured into three main sections, addressing crit-\\nical aspects of LLMs and their interplay with database systems.\\narXiv:2412.18022v1  [cs.DB]  23 Dec 2024\\nImproving Bare LLMs (§2.1.2)\\nFine-tuning, LoRA, RLHF\\nBackground (§2.1.1)\\nAutoregressive training, in-context learning\\nHallucination, lost-in-the-middle problem\\nScaling laws\\nMaking LLMs Interact with the World (§2.1.3)\\nKnowledge/memory/tool retrieval\\nMaking LLMs Self-drive (§2.1.4)\\nSelf-reflection, adaptivity\\nChain-of-thought, multi-hop, \\nmulti-path reasoning\\nAgentic LLMs, network of LLMs\\nSemantic variable, compound AI\\nBackground (§2.2.1)\\nAttention operation, key-value (KV) caching\\nPrefill/decode/preempt/refill stages\\nPipeline/model parallelism\\nLLMs Behave as DBMSs (§2.2.2)\\nContinuous batching, KV cache paging\\nPrefill-decode disaggregation\\nKV cache/attention offloading\\nNano-batching\\nOperation (§2.2.3)\\nFlash/sparse/flex attention \\nData (§2.2.4)\\nKV compression, model quantization, SSM\\nHardware (§2.2.5)\\nRoofline model\\nWorkload (§2.2.6)\\nScheduling, prefix KV sharing, speculation\\nDBAs and DBMS Internal (§2.3.1)\\nDatabase tuning, query optimization, text2sql\\nAdaptive Cost-based Scheduling (§2.3.2)\\nCost model for LLMs, CSP\\nMixed Relational-LLM Workload (§2.3.3)\\nSemantic operators, benchmark\\nMulti-objective Query Optimization (§2.3.4)\\nAccuracy-efficiency trade-off\\nLLM-database System (§2.3.5)\\nIntegration with OLAP databases\\nConvergence and Future (§2.3.6)\\nDisaggregation, adaptive query processing\\nTrustworthy LLMs (§2.1)\\nEfficient LLMs (§2.2)\\nLLMs Meet Databases (§2.3)\\nFigure 1: Tutorial outline (each subsection with keywords).\\nFigure 1 visualizes the outline with keywords for each subsection.\\nSection 2.1 focuses on improving the trustworthiness of LLMs, ex-\\nploring challenges such as hallucination and context limitations\\nwhile presenting state-of-the-art solutions to improve the accu-\\nracy and reliability of generated outputs. Section 2.2 emphasizes\\nefficiency, covering optimization strategies for inference, data man-\\nagement, and hardware utilization. Finally, Section 2.3 highlights\\nthe convergence of LLMs and databases, exploring opportunities for\\nintegration, new workloads, and emerging system designs. Since\\nthe field is changing fast, we will regularly reflect new information\\nuntil the tutorial date.\\n2.1\\nTrustworthy LLMs\\nThe first part of the tutorial explains the efforts to reduce halluci-\\nnations and make LLMs more trustworthy, using an analogy that\\nLLMs resemble humans. We explain background (Section 2.1.1),\\nhow LLMs can solely improve (Section 2.1.2), how LLMs can im-\\nprove by interacting with the external world (Section 2.1.3), and\\nhow LLMs can automatically make such decisions and interact with\\nother LLMs (Section 2.1.4).\\n2.1.1\\nBackground. Large Language Models (LLMs) function as\\ntext-in, text-out systems, generating texts based on their training.\\nTraining an LLM is akin to nurturing a child: by exposing it to\\nextensive text data, the model acquires world knowledge and rea-\\nsoning abilities. This process involves predicting the most probable\\nnext token in a sequence, a type of self-supervised learning. For a\\nsequence of tokens, the model learns to predict the latter tokens\\nbased on the preceding ones, enabling it to generate coherent text\\ncontinuations.\\nFine-tuning refines this process for specific tasks or domains,\\nsimilar to how individuals specialize in particular professions. In\\ncontrast, in-context learning provides additional information or\\nexamples within the input without altering the model’s parameters,\\nakin to consulting external references during an open-book exam.\\nMany prompting techniques [19, 34, 112, 240, 247, 258, 275, 319]\\nincluding chain-of-thought prompting [144, 289] and its variants\\n[18, 312] may leverage in-context learning to enhance performance.\\nDuring inference, LLMs generate texts autoregressively, produc-\\ning one token at a time. This process may involve deterministic\\nmethods like greedy or beam search, or probabilistic approaches\\nsuch as nucleus sampling [80, 114], which helps avoid selecting\\nlow-probability tokens.\\nHowever, LLMs experience hallucinations [13, 124, 270, 305],\\ngenerating plausible-sounding but incorrect or fabricated infor-\\nmation. This is an unavoidable aspect of LLMs [13, 305] which\\narises from limitations in capturing real-world knowledge, inherent\\napproximations in training and inference, input noise, etc. Even\\nslight input perturbations can significantly influence hallucinations\\n[65, 101], and the detection of hallucinations has been a major\\nproblem [47, 54, 77, 195, 204, 233, 264].\\nAdditionally, the lost-in-the-middle problem [117, 181] indicates\\nthat LLMs may struggle to utilize information located in the middle\\nof long contexts, often performing better when relevant informa-\\ntion is at the beginning or end of the input, exhibiting a U-shaped\\nperformance curve. This phenomenon has been attributed to inher-\\nent attention biases within LLMs, where tokens at the start and end\\nof the input receive higher attention, regardless of their relevance\\n[117]. This tendency can lead to increased hallucinations as context\\nlengthens [230].\\nScaling laws [113, 130, 221] explain that error rates decrease as\\nmodel size and training data increase, with optimal scaling requiring\\nproportional growth in both [113]. However, this may not hold for\\nsmaller models [221]. Laws can also relate to temporal loss in the\\ntraining curve [297], downstream tasks [121], model quantization\\n[306], transfer learning [14, 111], number of generated samples [27],\\nand inference time [205] with the advance of using long, complex\\nreasoning paths. Due to automatic prompting techniques [43, 136,\\n255, 302, 330] and that larger models tend to be less sensitive to\\nprompt variations [75], we focus less on prompting techniques.\\nTarget. The audience will distinguish pre-training, fine-tuning, and\\nin-context learning phases of LLMs, and understand the inherent\\nchallenges in making LLMs trustworthy.\\n2.1.2\\nImproving Bare LLMs. We briefly explain the approaches to\\nimprove the LLM itself to make it more trustworthy. Since LLM\\nis a specific class of machine learning (ML) models, general ML\\napproaches to enhance accuracy may work for LLMs. However, as\\nsuch approaches have been extensively studied from the classic ML\\nera, we target more LLM-specific approaches.\\nAs it is infeasible to increase the model size indefinitely, and the\\nmodels typically follow the Transformer architecture [274], efforts\\nhave been put to increase or augment training data (where LLMs\\nthemselves can be used to generate data) [36, 64, 82, 88, 142, 157,\\n163, 235, 241, 279, 341], improve data quality (again, LLMs can be\\nused to clean data) [23, 66, 72, 103, 143, 328], make inferences more\\nrobust [91, 276], and apply better training and fine-tuning methods.\\nSpecifically, fine-tuning covers a broad spectrum of work for\\nexample, parameter-efficient fine-tuning (PEFT) [116, 119, 152, 162,\\n222], instruction tuning [51, 198, 199, 243, 285, 288], reinforce-\\nment learning from human feedback (RLHF) [30, 52, 57, 81, 102,\\n134, 167, 208, 250, 262], and direct preference optimization (DPO)\\n[83, 138, 234, 338]. RLHF leverages human feedback to train a\\nreward model in reinforcement learning (RL), guiding the LLM\\nthrough RL to produce desired outputs. DPO simplifies the align-\\nment process by directly optimizing the policy model without a\\nseparate reward model. While these approaches rely on RL that\\ncontinuously interacts with human or the world external to LLMs,\\nsuch interactions are often limited to training and do not occur in\\ninferences, which we explain in the following sections.\\nOther than the training methods, a new model architecture of\\ndifferential Transformer [315] reduces the distractions of the models\\nto focus on unnecessary information in the long context, which\\nworks similarly to robust decoding strategies [91, 276].\\nTarget. The audience will learn about tuning LLMs to make them\\nmore trustworthy and aligned with user intentions.\\n2.1.3\\nMaking LLMs Interact with the World: Adding Eyes and Hands.\\nLLMs alone can encounter knowledge, memory, and capability limi-\\ntations [326]. Their knowledge is confined to the static information\\nencoded during training, leading to potential inaccuracies over\\ntime. Memory constraints arise from limited context windows, hin-\\ndering the handling of extended conversations. Additionally, their\\ntext-based nature restricts interactions with the physical world. To\\naddress these challenges, LLMs can retrieve knowledge, memory,\\nand tools.\\nThis section focuses on what and how to retrieve. When to re-\\ntrieve is the key to autonomy and will be detailed in the next section.\\nKnowledge retrieval is represented by well-known retrieval-\\naugmented generation (RAG) [76, 90, 126, 154, 166]. Based on the\\ndata type, it can fetch knowledge from knowledge graphs [38, 109,\\n169, 211, 225, 245, 266, 301, 309], tables [9, 22, 40, 45, 96, 98, 115, 128,\\n150, 158, 190, 265, 320], images [41, 42, 314], not just documents.\\nThe data may be chunked/vectorized, stored in vector databases,\\nthen similar chunks are searched online. While vector similarities\\nare typically used, more advanced similarity scores are possible,\\ne.g., using dual or cross encoders [203, 239].\\nMemory retrieval attempts to overcome the limited context\\nsize of LLMs by storing previously seen tokens as key-value pairs\\n[25, 183, 193, 281, 294] and fetching relevant pairs in upcoming\\nrequests, managing memory stores in hierarchical or partitioned\\nway [145, 287] or even as a database [118]. Fetching information\\nfrom long input can also be done without maintaining a separate\\nmemory store, but by sparsifying the model layers [15, 186]. One\\ncan relate low-rank adapters and mixture-of-experts [29, 71, 78,\\n110, 119, 155, 228, 292] with memory retrieval since lightweight\\nmodel parameters are fine-tuned per specific task and domain, and\\ndynamically fetched at online inferences.\\nTool retrieval searches for the APIs to interact with external\\nenvironments [188, 197, 218, 229, 232, 246, 286, 300]. One can con-\\nnect LLMs with databases to call SQLs that can help answering\\nuser questions [22]. Constrained decoding [20, 69, 92, 93] allows\\noutput to follow specific structure which can increase correctness\\nand efficiency.\\nThe challenges in retrieval include the followings. 1) Heterogene-\\nity: LLMs are text-based, but knowledge can be of any type. Even for\\ntext retrieval, heterogeneous lengths and intents between queries\\nand documents can lead to suboptimal retrieval accuracy [74, 89],\\nand the vector-similarity search may be too simple to retrieve neces-\\nsary information [87, 210]. 2) Scalability: Not only that LLMs have\\nlimited context or data they can utilize per inference, but main-\\ntaining a large set of retrieval entities and retrieving a subset may\\nincur overheads [269, 303]. While approximation can mitigate the\\nsearch overhead and make the search negligible to LLM inference\\ncosts, it is limited to vector-similarity search, and generalization\\nto more complex searches [131, 135, 137, 239] remains challenging.\\n3) Sparsity: This is also relevant to data sparsity and noise [56],\\nwhere relevant data is sparse compared to large information pools.\\n4) Reliability: Retrieved knowledge may be imperfect [277].\\nTarget. The audience will understand how LLMs can interact with\\nthe world and exploit external knowledge to overcome the limits\\nin using LLMs alone.\\n2.1.4\\nMaking LLMs Self-drive: Adding Brain. Now we have more\\npowerful models and interactions with the world. The last part is\\nhow we can make LLMs smart enough to maximize these capacities,\\nadding autonomy. Self-consistency and major voting enables a sim-\\nple yet effective solution for increasing consistency [283], however,\\nit fails to generate accurate and diverse answers [33, 44, 46] and\\nis yet passive. More active approaches include self-reflection and\\nadaptive retrieval [11, 32, 123, 125, 159, 185, 331], which adaptively\\nretrieves information multiple times based on the generated output,\\nmodel confidence, query complexity, or fine-tuned policies. This is\\nparticularly helpful for chain-of-thought/multi-hop reasoning and\\nquestion answering [175, 278, 282, 284, 329].\\nThe next step is to use multiple reasoning paths instead of a single\\npath. This multi-path reasoning has been an effective approach\\nfor driving LLMs [224, 226, 324, 332]. While the exact mechanism\\nremains closed, OpenAI’s o1 model is assumed to plan subtasks,\\nconduct these, and revise the results to decide whether to extend\\nthe current plan or generate different plans, forming a tree-like\\nreasoning structure. They suggest a new scaling law that LLM\\naccuracy increases with inference time, not only with training time\\nand data [205].\\nAgentic LLM indicates that LLMs can act as agents, selecting\\nactions based on observations [49, 179, 256, 313]. Multiple agents\\nexploit collaborative reasoning, parallel processing, diversity, and\\nspecialization akin to humans [35, 104, 172, 212, 214, 227, 299].\\nSemantic variables [173] regard LLM input and output tokens as\\ndependent variables to explicitly model control flows.\\nA broader view includes compound AI [323] where AI and non-\\nAI components interact with each other, including retrievals, control\\nflows, agentic LLMs, and more. An interesting example is automated\\nresearch process [187, 259].\\nTarget. The audience will learn about approaches to make LLMs\\nself-driving and build systems around LLMs for complex tasks.\\n2.2\\nEfficient LLMs\\nThe second part of the tutorial demystifies the internals of LLM\\ninference process and explains the efforts to make it more efficient,\\nusing an analogy that LLMs behave as DBMSs. We explain back-\\nground (Section 2.2.1) and how LLM inference systems resemble\\nDBMSs in improving their efficiency (Section 2.2.2). We then ex-\\nplain further work for each dimension of operation (Section 2.2.3),\\ndata (Section 2.2.4), hardware (Section 2.2.5), and workload (Section\\n2.2.6).\\n2.2.1\\nBackground. The dominant Transformer architecture em-\\nploys an attention mechanism [274] that calculates similarity scores\\nbetween a token and its preceding tokens, effectively capturing\\ninter-token relationships and managing extended contexts. This\\nprocess has quadratic complexity, but key-value (KV) caching [58]\\noptimizes it by storing and reusing these computations, reducing\\nthe complexity to linear during inference. Non-attention operations\\nmostly consist of matrix multiplications and activations.\\nInference in Large Language Models (LLMs) involves two pri-\\nmary phases: prefill and decode. During the prefill, the model pro-\\ncesses input tokens to generate the initial output token. The at-\\ntention operates with quadratic complexity due to the absence of\\nprecomputed KVs, making it compute-intensive. During the decode,\\nthe model generates subsequent tokens sequentially, each time us-\\ning the last generated token as input. Here, the attention leverages\\nKV caching, resulting in linear complexity relative to the number\\nof processed tokens and reading their KVs, which makes this phase\\nmore memory-intensive.\\nIn case of multiple requests, they face a race condition as in\\nmulti-tenant systems. If the GPU memory is insufficient to keep\\nall requests’ KVs, some running requests are preempted (evicted),\\nreleasing their KVs from the memory, and restarted (refilled) later\\n[146]. Due to the low PCIe bandwidth, the released KVs are often\\nrecomputed when restarted, instead of offloading to other storage\\ndevices and loading back. Multiple requests in either prefill or\\ndecode steps can be batched to amortize the cost of loading model\\nweights from GPU memory.\\nNote that the model weights also occupy the GPU memory. When\\nthe model size exceeds a single GPU capacity, techniques like model\\nand pipeline parallelism [105, 120, 257] distribute model weights\\nacross multiple GPUs. This partitioning introduces data transfer\\noverhead between GPUs.\\nTarget. The audience will understand the KV caching and different\\nphases of LLM inference requests, and how they compete for the\\nsame GPU resource.\\n2.2.2\\nLLM Inference Systems: LLMs Behave as DBMSs. LLM infer-\\nence systems (e.g., vLLM [146]) behave similarly to (in-memory)\\nDBMS. KVs and model weights correspond to the data, which are\\nmaintained in GPU memory. Operators include matrix multipli-\\ncations, activations, attentions, and data transfers. Compared to\\nOLAP in databases, the operations are simpler yet much more time-\\nsensitive, where the requests should be served in real-time.\\nSignificant efforts have been made to increase the efficiency of\\nLLM inference [342], largely based on operating and database sys-\\ntems. Orca [318] forms a new batch of requests after each iteration\\n(of prefills and decodes) whenever resources are available. Thus, a\\nnew request does not have to wait for all current running requests\\nto finish, just like the pipelining in OS. vLLM [146] adopts paging\\nand virtual memory to manage KVs, reducing memory fragmenta-\\ntion and enlarging the batch size. Since prefills are typically more\\ncostly than decodes, making stalls for decodes when batched to-\\ngether, Sarathi [4, 5] chunks prefills to reduce pipeline bubbles.\\nSome other work [217, 263, 339] rather disaggregates the prefills\\nand decodes into different GPUs, so the workload for each GPU\\nis homogeneous. vTensor [298] decouples the KV cache manage-\\nment and attention computation of vLLM for better flexibility and\\nperformance. NanoFlow [343] splits each batch into nano-batches\\nfor finer-grained pipelining, increasing the overlap of computation,\\nmemory operation, and data transfer between GPUs. It also hides\\nCPU scheduling latency by asynchronous scheduling. InfiniGen\\n[149] offloads the KVs to CPU memory to extend the KV cache and\\nreloads the KVs from CPU layer-wise, but fetches a subset of KVs\\nfor efficiency, similarly to sparse attentions (Section 2.2.3). InstIn-\\nfer [213] offloads KVs and attention computations to flash drives,\\njust like the storage-disaggregation and computation pushdown in\\ndatabases [310]. NEO [127] selectively offloads attention computa-\\ntions and KVs from GPU to CPU, in order to maximize both GPU\\nand CPU utilization.\\nTarget. The audience will understand why LLMs behave similarly\\nto DBMSs and how database techniques can improve LLM infer-\\nence efficiency. In subsequent sections, the audience will learn\\nabout efforts and challenges in further improving efficiency in four\\ndimensions: operation data, hardware, and workload.\\n2.2.3\\nOperation: Attention. While matrix multiplications take the\\nmajor portion in LLM latency in general [5], attentions can domi-\\nnate the runtime for large inputs due to their quadratic complexity.\\nFlashAttention [59, 60, 249] has become a de facto standard as an\\nefficient attention implementation, utilizing recent GPU technolo-\\ngies to boost the inference speed. The ideas include kernel operator\\nfusion and GPU cache-aware KV transfer. As in approximate query\\nprocessing (AQP) in databases, sparse attentions [15, 178, 186] do\\nnot compute the full attention scores for all preceding tokens but\\na subset as an approximation. Some attentions rather optimize\\nfor long contexts [2, 62]. FlexAttention [107] offers flexible and\\nperformant implementation of such attentions.\\n2.2.4\\nData: KV and Model Weights. Reading KVs from GPU mem-\\nory in decode-attentions is similar to sequential table scan. As KVs\\nare maintained per each attention layer, reading KVs for a layer\\ncan overlap with other layers’ operators [149, 263]. While offload-\\ning KVs and attention computation have been popular recently\\n[127, 149, 213], we need to be careful as it is challenging to predict\\nthe output lengths of LLM requests and thus their utilization pat-\\nterns, and a KV for a single token may consume a few MBs. KVs\\nof long documents can be precomputed, compressed, and fetched\\nfor later retrievals [184]. To reduce memory latency, one can opt\\nfor KV sharing across different attention heads [7, 26, 50], KV com-\\npression [63, 129, 176, 184, 236], model quantization [94, 97, 106,\\n147, 164, 200, 251, 295, 306], or different model architectures than\\nTransformer, such as State Space Models (SSMs) [61, 100] that do\\nnot rely on attentions, thereby not generating KVs. While hybrid\\narchitectures [10, 67, 99, 108, 171, 223, 237] can balance between\\nthe efficiency of SSMs and memorization capacity of Transformers,\\nSSMs remain niche in the market [17]. A recent work even shows\\nthat tokenizers can be removed from the models [209].\\n2.2.5\\nHardware: Theory and Practice. We briefly explain 1) the\\nroofline model [322] and 2) some efforts to overcome the hard-\\nware limits [161, 238, 253, 261, 317, 333] or leverage advanced hard-\\nware for LLM inference [170, 304]. The roofline model is based\\non the computation speed (e.g., GPU FLOPS) and memory band-\\nwidth, which acts as a theoretical hardware bound and determines\\nwhether an operator is compute-intensive or memory-intensive\\nacross different inputs.\\n2.2.6\\nWorkload: Scheduling, Prefix Sharing, Speculation. To handle\\nmultiple LLM requests, LLM inference systems implement request\\nscheduler to send LLM requests to appropriate machines or GPUs\\nto maximize throughput or minimize latency. Assuming indepen-\\ndent requests, early schedulers either prioritize prefills [146] or\\ndecodes [4], which tend to optimize latency or throughput, respec-\\ntively. More complex schedulers consider fairness [252, 291] while\\ncompromising performance, or predict the output lengths of re-\\nquests (not known in advance) and schedule shorter requests first\\n[85, 231, 337].\\nIf different LLM requests can share a prefix in their inputs, the\\nKVs of the prefix can be stored just once and reused for multiple\\nrequests [334, 335]. This forms a trie structure with shared prefixes.\\nHowever, a single-token difference in inputs may invalidate the\\nsharing of KVs of all subsequent tokens. To increase the sharing\\nopportunity, [311] uses the KVs of multiple token sequences to\\napproximate the KVs of the concatenated sequence. The mechanism\\nis similar to the speculation in OLAP [260] and healing protocol in\\ntransactions [293] in databases.\\nThis speculation and healing patterns also appear in speculative\\ndecoding [153] and model cascades [39, 177, 316, 325, 336], acceler-\\nating the generation of tokens by leveraging smaller, faster models\\nthen validate the tokens using larger models, since the validation\\ncosts less than the generation.\\n2.3\\nLLMs Meet Databases\\nThe last part of the tutorial discusses the intersection between\\nLLMs and databases, opportunities and challenges in how we can\\nexploit LLMs for databases, how the development of databases can\\nhelp LLMs, and how we can exploit new types of workloads and\\nintegrations of LLMs and databases. We explain from more well-\\nknown to more untapped, deeper integrations in Sections 2.3.1-2.3.5\\nand provide more proactive visions in Section 2.3.6.\\n2.3.1\\nLLMs for DBs: DBAs and DBMS Internal. We briefly explain\\nhow LLMs are utilized for well-known tasks of DBAs and DBMS\\ninternals such as database tuning [271, 340], text2sql [151, 224] and\\nquery optimization [8, 168]. As we mentioned in Section 1, we will\\nnot cover every detail, as many of these efforts are covered in a\\nprevious tutorial [194] and its additional list of papers [55].\\n2.3.2\\nDBs for LLMs: Adaptive Cost-based Scheduling. Unlike the\\nsophisticated query optimizers in DBMSs, LLMs lack cost models\\nand cost-based scheduling of LLM requests. [3] measures the batch\\ntimes across various inputs (number of tokens to process and KV\\nsize to read). [322] computes batch times based on the roofline\\nmodels. These can be used to model batch times and formulate the\\nproblem of finding optimal schedules as a constrained satisfaction\\nproblem (CSP) [139]. While schedulers try to avoid preemptions to\\nmaximize performance, [139] shows that harnessing preemptions\\ncan rather reduce overall latency compared to zero-preemptions.\\nAs the exact hardware utilization of each request is not known in\\nadvance, the scheduling should be adaptive based on the observa-\\ntions, and it has not been explored much to schedule dependent\\nrequests connected via semantic variables or shared prefixes [139].\\n2.3.3\\nDBs with LLMs: Mixed Relational-LLM Workload. Not only\\nsolving existing tasks with LLMs, LLMs offer new functionalities\\nwhen integrated into DBMSs. Semantic operators [216] extend rela-\\ntional operators to batch-process the tabular data with LLMs (e.g.,\\nfilters and joins using LLMs), which can be regarded as an AQP.\\nWorkloads with LLMs provide a justification to use LLMs inside\\nDBMSs (heavy LLM calls in plan optimization can be negligible com-\\npared to query execution with LLMs). However, different pipelines\\n(with semantic operators) lead to different accuracy and efficiency,\\nthus defining the equivalence between two pipelines is non-trivial.\\nFurthermore, more complex pipelines or LLM calls do not always\\nguarantee higher accuracy [37, 74], and searching similar entities\\nwith LLMs can be replaced with efficient vector-similarity searches\\n[177, 242] as a type of model cascade.\\n2.3.4\\nDBs with LLMs: Multi-objective Query Optimization and Bench-\\nmarks. The challenge is therefore how we can automatically find\\ngood pipelines for mixed relational-LLM workloads under the multi-\\nobjective of accuracy and efficiency [22, 273, 321] as in compound\\nAI systems [95, 244, 267]. This calls for development of accurate\\ncost models and accuracy-prediction models for LLMs and mixed\\nrelational-LLM workloads, in order to enable the holistic optimiza-\\ntion of query plans consisting of both relational and non-relational\\noperators. The cost model itself can be learned via LLMs (or any ML\\nmodels), possibly using RLHF or feedback from query execution\\nwithout human intervention, where such an automatic training\\ndata generation is one of the advantages of solving database tasks\\ncompared to conventional ML tasks (e.g., natural language process-\\ning with human-labeled translation data) [140]. Another model for\\npredicting the output accuracy or detecting hallucination may be\\nchosen from the scaling laws (using the general fact that larger\\nmodels are more trustworthy) or separately trained.\\nTo balance efficiency and accuracy, during the physical query\\noptimization we should select proper models (ones used for ex-\\necution) to avoid calling heavy LLMs unnecessarily. Depending\\non the complexity of the task, simple ML models with a small set\\nof supervised data [123], or larger deep generative models such\\nas in tabular foundation models tailored to domain-specific data\\n[141, 160, 308], or LLMs with world knowledge and reasoning capac-\\nity [296] can fit the task with different accuracy-efficiency trade-offs.\\nSmall language models (SLMs) [1, 16, 67, 189, 196, 202, 226] are also\\na good choice. Automatically finding the best prompt configuration\\n[136, 280] tailored to the mixed workloads and more (e.g., previ-\\nously mentioned fine-tuning or multi-hop/multi-path reasoning\\nwith adaptivity during inference) might be desired.\\nFurthermore, unlike the TPC benchmarks for databases, another\\nproblem is that there is no comprehensive benchmark for relational-\\nLLM workloads yet. [22, 182] provide exploratory benchmarks\\nwithout focusing on semantic joins.\\n2.3.5\\nDBs with LLMs: Integrated System. Other than the cost mod-\\nels, we also need DBMSs with native LLM support to increase\\nthe optimization opportunities, alike systems for relational-vector\\nworkloads [242, 327]. Current prototypes for relational-LLM work-\\nloads [177, 182, 216] separate table processing (e.g., pandas [192])\\nand LLM inference engine (e.g., vLLM [146]).\\nTo maximize efficiency and scalability, we should focus on hard-\\nware utilization, data movement [122], caching hot data, locating\\ncomputations close to data (e.g., computation pushdown in storage-\\naggregation setting) [84, 191], asynchronous API calls [95], balanc-\\ning loads, and multi-tenancy just like in DBMSs [248, 310]. One\\nalso has to decide whether to maintain a separate vector database\\nfor faster online vector retrievals, or use just-in-time vectorization\\nfor reducing storage overhead. This also applies to precomputing\\nKVs of data tokens [184] for faster LLM inferences or not, but with\\na higher caution as KVs are typically larger than vectors.\\n2.3.6\\nConvergence and Future. We envision LLMs and databases\\nto converge (e.g., neuro-symbolic systems [48, 79, 268, 321]), more\\nthan just applying the techniques from one domain to another. A\\nnew LLM inference system optimized for DBMSs might be devel-\\noped from an open-source cloud DBMS, utilizing recent implemen-\\ntations and optimizations for processing relational operators, such\\nas storage-disaggregation and computation pushdown for scalable\\ndata and model management [310], GPU-based OLAP processing\\n[86, 219, 220] for the full use of GPUs for both relational opera-\\ntors and LLMs, hybrid operators with heterogeneous data transfer\\npaths [53, 310], adaptive query execution [307] and more. A unified\\nquery optimizer and data model for both relational data, KVs, and\\nmodel weights, could offer opportunities for better data manage-\\nment and hardware utilization. Finally, if we look into the near\\nfuture, we could also harness the emerging CXL technology for\\nmemory disaggregation [6, 148, 180] to manage model weights\\nand KVs, and increased interest in pruning unnecessary data in\\nOLAP [21, 24, 174, 206] could lead to higher trustworthiness (due\\nto reduced noise) and efficiency (due to less data to process) in the\\nrelational-LLM workloads, with connections to online aggregation\\n[254] and incremental view maintenance [28].\\nTarget. The audience will understand the different depths of LLM-\\ndatabase integrations and be able to find interesting research topics\\nfrom each of the integration, which are closely related to the current\\nand near-future trends of databases.\\n3\\nRelated Tutorial\\nXupeng et al. [194] presented a tutorial at SIGMOD 2024 about\\nthe role of data management in the development (training, fine-\\ntuning) and deployment (inference) of LLMs. It focused on how the\\nknowledge is encoded into model parameters and extracted during\\nthe inference, and explained the concept of KV caching but not LLM\\ninference systems. Trummer [272] presented at VLDB 2023 about\\nTransformer architecture, pre-training/fine-tuning/prompting in\\nLLMs, and LLM applications in data management. As pointed out\\nby [194], most of other tutorials presented at SIGMOD and VLDB\\nabout AI and databases focused on traditional machine learning\\nand deep learning tasks not tailored to LLMs [31, 70, 156, 165, 201,\\n207, 290], or specific LLM-related applications such as tabular data\\nunderstanding [12] and queries with natural languages [132, 133].\\nDong et al. [68] presented at SIGKDD 2023 about the role of LLMs\\nin building intelligent AR/VR assistants.\\nIn this tutorial, we will focus on more recent, general approaches\\nto enhance the trustworthiness and efficiency of LLMs, which have\\nnot been addressed in previous tutorials. For trustworthiness, we\\nwill start with enhancing LLMs alone, LLMs with tools, and agentic\\nLLMs and collaboration. For efficiency, we will explain how LLM\\ninference systems resemble DBMSs. Then we will discuss how we\\ncan integrate LLMs and databases in depth. We expect that these\\nare what researchers, who aim to use LLMs in their applications or\\noptimize LLMs using database techniques, need to know about. In-\\nstead of a common analogy that LLMs are knowledge bases as they\\ngenerate plausible facts, we will use analogies that LLMs behave as\\nDBMSs and improve as how humans solve challenging problems.\\nReferences\\n[1] Marah I Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed\\nAwadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harki-\\nrat S. Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Mar-\\ntin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul\\nChopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan\\nIter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng\\nHao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauff-\\nmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko,\\nJames R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi\\nLin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick,\\nBarun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin,\\nMarko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi,\\nAmin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi\\nSharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang,\\nPhilipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang,\\nZiyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang,\\nLi Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. 2024.\\nPhi-3 Technical Report: A Highly Capable Language Model Locally on Your\\nPhone. CoRR abs/2404.14219 (2024). https://doi.org/10.48550/ARXIV.2404.14219\\narXiv:2404.14219\\n[2] Amey Agrawal, Junda Chen, Íñigo Goiri, Ramachandran Ramjee, Chaojie\\nZhang, Alexey Tumanov, and Esha Choukse. 2024. Mnemosyne: Paralleliza-\\ntion Strategies for Efficiently Serving Multi-Million Context Length LLM\\nInference Requests Without Approximations.\\nCoRR abs/2409.17264 (2024).\\nhttps://doi.org/10.48550/ARXIV.2409.17264 arXiv:2409.17264\\n[3] Amey Agrawal, Nitin Kedia, Jayashree Mohan, Ashish Panwar, Nipun\\nKwatra, Bhargav S. Gulavani, Ramachandran Ramjee, and Alexey Tu-\\nmanov. 2024.\\nVIDUR: A Large-Scale Simulation Framework for LLM\\nInference. In Proceedings of the Seventh Annual Conference on Machine\\nLearning and Systems, MLSys 2024, Santa Clara, CA, USA, May 13-16,\\n2024, Phillip B. Gibbons, Gennady Pekhimenko, and Christopher De Sa\\n(Eds.). mlsys.org. https://proceedings.mlsys.org/paper_files/paper/2024/hash/\\nb74a8de47d2b3c928360e0a011f48351-Abstract-Conference.html\\n[4] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwa-\\ntra, Bhargav S. Gulavani, Alexey Tumanov, and Ramachandran Ramjee. 2024.\\nTaming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve.\\nIn 18th USENIX Symposium on Operating Systems Design and Implementation,\\nOSDI 2024, Santa Clara, CA, USA, July 10-12, 2024, Ada Gavrilovska and Dou-\\nglas B. Terry (Eds.). USENIX Association, 117–134. https://www.usenix.org/\\nconference/osdi24/presentation/agrawal\\n[5] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S.\\nGulavani, and Ramachandran Ramjee. 2023. SARATHI: Efficient LLM Inference\\nby Piggybacking Decodes with Chunked Prefills. CoRR abs/2308.16369 (2023).\\nhttps://doi.org/10.48550/ARXIV.2308.16369 arXiv:2308.16369\\n[6] Minseon Ahn, Thomas Willhalm, Norman May, Donghun Lee, Suprasad Mutalik\\nDesai, Daniel Booss, Jungmin Kim, Navneet Singh, Daniel Ritter, and Oliver\\nRebholz. 2024. An Examination of CXL Memory Use Cases for In-Memory\\nDatabase Management Systems using SAP HANA. Proc. VLDB Endow. 17, 12\\n(2024), 3827–3840. https://www.vldb.org/pvldb/vol17/p3827-ahn.pdf\\n[7] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico\\nLebrón, and Sumit Sanghai. 2023. GQA: Training Generalized Multi-Query\\nTransformer Models from Multi-Head Checkpoints. In Proceedings of the 2023\\nConference on Empirical Methods in Natural Language Processing, EMNLP 2023,\\nSingapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali\\n(Eds.). Association for Computational Linguistics, 4895–4901. https://doi.org/\\n10.18653/V1/2023.EMNLP-MAIN.298\\n[8] Peter Akioyamen, Zixuan Yi, and Ryan Marcus. 2024. The Unreasonable Ef-\\nfectiveness of LLMs for Query Optimization. arXiv preprint arXiv:2411.02862\\n(2024).\\n[9] Uday Allu, Biddwan Ahmed, and Vishesh Tripathi. 2024. Beyond Extraction:\\nContextualising Tabular Data for Efficient Summarisation by Language Mod-\\nels. CoRR abs/2401.02333 (2024). https://doi.org/10.48550/ARXIV.2401.02333\\narXiv:2401.02333\\n[10] Quentin Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge. 2024.\\nBlackMamba: Mixture of Experts for State-Space Models. CoRR abs/2402.01771\\n(2024). https://doi.org/10.48550/ARXIV.2402.01771 arXiv:2402.01771\\n[11] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi.\\n2024. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-\\nReflection. In The Twelfth International Conference on Learning Representations,\\nICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.\\nnet/forum?id=hSyW5go0v8\\n[12] Gilbert Badaro and Paolo Papotti. 2022. Transformers for Tabular Data Repre-\\nsentation: A Tutorial on Models and Applications. Proc. VLDB Endow. 15, 12\\n(2022), 3746–3749. https://doi.org/10.14778/3554821.3554890\\n[13] Sourav Banerjee, Ayushi Agarwal, and Saloni Singla. 2024. LLMs Will Always\\nHallucinate, and We Need to Live With This. CoRR abs/2409.05746 (2024).\\nhttps://doi.org/10.48550/ARXIV.2409.05746 arXiv:2409.05746\\n[14] Matthew Barnett. 2024.\\nAn Empirical Study of Scaling Laws for Trans-\\nfer. CoRR abs/2408.16947 (2024). https://doi.org/10.48550/ARXIV.2408.16947\\narXiv:2408.16947\\n[15] Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The\\nLong-Document Transformer. CoRR abs/2004.05150 (2020). arXiv:2004.05150\\nhttps://arxiv.org/abs/2004.05150\\n[16] Loubna Ben Allal, Anton Lozhkov, and Elie Bakouch. 2024. SmolLM - blazingly\\nfast and remarkably powerful. https://huggingface.co/blog/smollm. Accessed:\\nDecember 15, 2024.\\n[17] Nathan Benaich and Ian Hogarth. 2024. State of AI Report 2024. https://www.\\nstateof.ai.\\n[18] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski,\\nLukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Pi-\\notr Nyczyk, and Torsten Hoefler. 2024. Graph of Thoughts: Solving Elaborate\\nProblems with Large Language Models. In Thirty-Eighth AAAI Conference on\\nArtificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applica-\\ntions of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational\\nAdvances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver,\\nCanada, Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan (Eds.).\\nAAAI Press, 17682–17690. https://doi.org/10.1609/AAAI.V38I16.29720\\n[19] Maciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger, Nils Blach,\\nPiotr Nyczyk, Marcin Copik, Grzegorz Kwasniewski, Jürgen Müller, Lukas Gian-\\ninazzi, Ales Kubicek, Hubert Niewiadomski, Onur Mutlu, and Torsten Hoefler.\\n2024. Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of\\nThoughts. CoRR abs/2401.14295 (2024). https://doi.org/10.48550/ARXIV.2401.\\n14295 arXiv:2401.14295\\n[20] Luca Beurer-Kellner, Marc Fischer, and Martin T. Vechev. 2024. Guiding LLMs\\nThe Right Way: Fast, Non-Invasive Constrained Generation. In Forty-first In-\\nternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July\\n21-27, 2024. OpenReview.net. https://openreview.net/forum?id=pXaEYzrFae\\n[21] Altan Birler, Alfons Kemper, and Thomas Neumann. 2024. Robust Join Process-\\ning with Diamond Hardened Joins. Proc. VLDB Endow. 17, 11 (2024), 3215–3228.\\nhttps://www.vldb.org/pvldb/vol17/p3215-birler.pdf\\n[22] Asim Biswal, Liana Patel, Siddarth Jha, Amog Kamsetty, Shu Liu, Joseph E.\\nGonzalez, Carlos Guestrin, and Matei Zaharia. 2024. Text2SQL is Not Enough:\\nUnifying AI and Databases with TAG. CoRR abs/2408.14717 (2024).\\nhttps:\\n//doi.org/10.48550/ARXIV.2408.14717 arXiv:2408.14717\\n[23] Quinten Bolding, Baohao Liao, Brandon James Denis, Jun Luo, and Christof\\nMonz. 2023. Ask Language Model to Clean Your Noisy Translation Data. In\\nFindings of the Association for Computational Linguistics: EMNLP 2023, Singapore,\\nDecember 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Associ-\\nation for Computational Linguistics, 3215–3236. https://doi.org/10.18653/V1/\\n2023.FINDINGS-EMNLP.212\\n[24] Angela Bonifati, Stefania Dumbrava, George Fletcher, Jan Hidders, Matthias\\nHofer, Wim Martens, Filip Murlak, Joshua Shinavier, Slawek Staworko, and\\nDominik Tomaszuk. 2023. Threshold Queries. SIGMOD Rec. 52, 1 (2023), 64–73.\\nhttps://doi.org/10.1145/3604437.3604452\\n[25] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Ruther-\\nford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan\\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman\\nRing, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cas-\\nsirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osin-\\ndero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improv-\\ning Language Models by Retrieving from Trillions of Tokens. In International\\nConference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland,\\nUSA (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaudhuri,\\nStefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (Eds.).\\nPMLR, 2206–2240. https://proceedings.mlr.press/v162/borgeaud22a.html\\n[26] William Brandon, Mayank Mishra, Aniruddha Nrusimha, Rameswar Panda, and\\nJonathan Ragan-Kelley. 2024. Reducing Transformer Key-Value Cache Size with\\nCross-Layer Attention. CoRR abs/2405.12981 (2024). https://doi.org/10.48550/\\nARXIV.2405.12981 arXiv:2405.12981\\n[27] Bradley C. A. Brown, Jordan Juravsky, Ryan Saul Ehrlich, Ronald Clark, Quoc V.\\nLe, Christopher Ré, and Azalia Mirhoseini. 2024. Large Language Monkeys:\\nScaling Inference Compute with Repeated Sampling. CoRR abs/2407.21787\\n(2024). https://doi.org/10.48550/ARXIV.2407.21787 arXiv:2407.21787\\n[28] Mihai Budiu, Tej Chajed, Frank McSherry, Leonid Ryzhyk, and Val Tannen. 2023.\\nDBSP: Automatic Incremental View Maintenance for Rich Query Languages.\\nProc. VLDB Endow. 16, 7 (2023), 1601–1614. https://doi.org/10.14778/3587136.\\n3587137\\n[29] Shiyi Cao, Shu Liu, Tyler Griggs, Peter Schafhalter, Xiaoxuan Liu, Ying Sheng,\\nJoseph E Gonzalez, Matei Zaharia, and Ion Stoica. 2024. MoE-Lightning: High-\\nThroughput MoE Inference on Memory-constrained GPUs. arXiv preprint\\narXiv:2411.11217 (2024).\\n[30] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy\\nScheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro\\nFreire, Tony Tong Wang, Samuel Marks, Charbel-Raphaël Ségerie, Micah Carroll,\\nAndi Peng, Phillip J. K. Christoffersen, Mehul Damani, Stewart Slocum, Usman\\nAnwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii\\nKrasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Biyik, Anca D.\\nDragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. 2023. Open\\nProblems and Fundamental Limitations of Reinforcement Learning from Human\\nFeedback. Trans. Mach. Learn. Res. 2023 (2023). https://openreview.net/forum?\\nid=bx24KpJ4Eb\\n[31] Chengliang Chai, Nan Tang, Ju Fan, and Yuyu Luo. 2023. Demystifying Artificial\\nIntelligence for Data Preparation. In Companion of the 2023 International Confer-\\nence on Management of Data, SIGMOD/PODS 2023, Seattle, WA, USA, June 18-23,\\n2023, Sudipto Das, Ippokratis Pandis, K. Selçuk Candan, and Sihem Amer-Yahia\\n(Eds.). ACM, 13–20. https://doi.org/10.1145/3555041.3589406\\n[32] Andong Chen, Lianzhang Lou, Kehai Chen, Xuefeng Bai, Yang Xiang, Muyun\\nYang, Tiejun Zhao, and Min Zhang. 2024. DUAL-REFLECT: Enhancing Large\\nLanguage Models for Reflective Translation through Dual Learning Feedback\\nMechanisms. In Proceedings of the 62nd Annual Meeting of the Association for\\nComputational Linguistics, ACL 2024 - Short Papers, Bangkok, Thailand, August 11-\\n16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for\\nComputational Linguistics, 693–704. https://aclanthology.org/2024.acl-short.64\\n[33] Angelica Chen, Jason Phang, Alicia Parrish, Vishakh Padmakumar, Chen Zhao,\\nSamuel R. Bowman, and Kyunghyun Cho. 2024. Two Failures of Self-Consistency\\nin the Multi-Step Reasoning of LLMs. Trans. Mach. Learn. Res. 2024 (2024).\\nhttps://openreview.net/forum?id=5nBqY1y96B\\n[34] Banghao Chen, Zhaofeng Zhang, Nicolas Langrené, and Shengxin Zhu. 2023.\\nUnleashing the potential of prompt engineering in Large Language Models: a\\ncomprehensive review. CoRR abs/2310.14735 (2023). https://doi.org/10.48550/\\nARXIV.2310.14735 arXiv:2310.14735\\n[35] Huaben Chen, Wenkang Ji, Lufeng Xu, and Shiyu Zhao. 2023. Multi-Agent\\nConsensus Seeking via Large Language Models. CoRR abs/2310.20151 (2023).\\nhttps://doi.org/10.48550/ARXIV.2310.20151 arXiv:2310.20151\\n[36] Jiaao Chen, Derek Tam, Colin Raffel, Mohit Bansal, and Diyi Yang. 2023. An\\nEmpirical Survey of Data Augmentation for Limited Data Learning in NLP.\\nTrans. Assoc. Comput. Linguistics 11 (2023), 191–211. https://doi.org/10.1162/\\nTACL_A_00542\\n[37] Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei\\nZaharia, and James Zou. 2024. Are more llm calls all you need? towards scaling\\nlaws of compound inference systems. arXiv preprint arXiv:2403.02419 (2024).\\n[38] Liyi Chen, Panrong Tong, Zhongming Jin, Ying Sun, Jieping Ye, and Hui Xiong.\\n2024. Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language\\nModel on Knowledge Graphs. CoRR abs/2410.23875 (2024). https://doi.org/10.\\n48550/ARXIV.2410.23875 arXiv:2410.23875\\n[39] Lingjiao Chen, Matei Zaharia, and James Zou. 2023.\\nFrugalGPT: How to\\nUse Large Language Models While Reducing Cost and Improving Perfor-\\nmance. CoRR abs/2305.05176 (2023). https://doi.org/10.48550/ARXIV.2305.05176\\narXiv:2305.05176\\n[40] Wenhu Chen. 2023. Large Language Models are few(1)-shot Table Reasoners. In\\nFindings of the Association for Computational Linguistics: EACL 2023, Dubrovnik,\\nCroatia, May 2-6, 2023, Andreas Vlachos and Isabelle Augenstein (Eds.). Associ-\\nation for Computational Linguistics, 1090–1100. https://doi.org/10.18653/V1/\\n2023.FINDINGS-EACL.83\\n[41] Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William W. Cohen. 2022.\\nMuRAG: Multimodal Retrieval-Augmented Generator for Open Question An-\\nswering over Images and Text. In Proceedings of the 2022 Conference on Em-\\npirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi,\\nUnited Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zornitsa Kozareva,\\nand Yue Zhang (Eds.). Association for Computational Linguistics, 5558–5570.\\nhttps://doi.org/10.18653/V1/2022.EMNLP-MAIN.375\\n[42] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W. Cohen. 2023.\\nRe-Imagen: Retrieval-Augmented Text-to-Image Generator. In The Eleventh\\nInternational Conference on Learning Representations, ICLR 2023, Kigali, Rwanda,\\nMay 1-5, 2023. OpenReview.net. https://openreview.net/forum?id=XSEBx0iSjFQ\\n[43] Weizhe Chen, Sven Koenig, and Bistra Dilkina. 2024.\\nRePrompt: Plan-\\nning by Automatic Prompt Engineering for Large Language Models Agents.\\nCoRR abs/2406.11132 (2024).\\nhttps://doi.org/10.48550/ARXIV.2406.11132\\narXiv:2406.11132\\n[44] Wenqing Chen, Weicheng Wang, Zhixuan Chu, Kui Ren, Zibin Zheng, and\\nZhichao Lu. 2024. Self-Para-Consistency: Improving Reasoning Tasks at Low\\nCost for Large Language Models. In Findings of the Association for Computational\\nLinguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16,\\n2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for\\nComputational Linguistics, 14162–14167.\\nhttps://doi.org/10.18653/V1/2024.\\nFINDINGS-ACL.842\\n[45] Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and\\nWilliam Yang Wang. 2020. HybridQA: A Dataset of Multi-Hop Question An-\\nswering over Tabular and Textual Data. In Findings of the Association for Compu-\\ntational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020 (Findings of\\nACL, Vol. EMNLP 2020), Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association\\nfor Computational Linguistics, 1026–1036. https://doi.org/10.18653/V1/2020.\\nFINDINGS-EMNLP.91\\n[46] Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin,\\nSushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. 2023. Univer-\\nsal Self-Consistency for Large Language Model Generation. CoRR abs/2311.17311\\n(2023). https://doi.org/10.48550/ARXIV.2311.17311 arXiv:2311.17311\\n[47] Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang, Qiang Li,\\nYue Shen, Lei Liang, Jinjie Gu, and Huajun Chen. 2024. Unified Hallucination\\nDetection for Multimodal Large Language Models. In Proceedings of the 62nd\\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long\\nPapers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, Lun-Wei Ku, Andre\\nMartins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,\\n3235–3252. https://doi.org/10.18653/V1/2024.ACL-LONG.178\\n[48] Zui Chen, Zihui Gu, Lei Cao, Ju Fan, Samuel Madden, and Nan Tang. 2023.\\nSymphony: Towards Natural Language Query Answering over Multi-modal\\nData Lakes. In 13th Conference on Innovative Data Systems Research, CIDR 2023,\\nAmsterdam, The Netherlands, January 8-11, 2023. www.cidrdb.org. https://www.\\ncidrdb.org/cidr2023/papers/p51-chen.pdf\\n[49] Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong,\\nWenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, and Xiuqiang He.\\n2024. Exploring Large Language Model based Intelligent Agents: Definitions,\\nMethods, and Prospects. CoRR abs/2401.03428 (2024). https://doi.org/10.48550/\\nARXIV.2401.03428 arXiv:2401.03428\\n[50] Sai Sena Chinnakonduru and Astarag Mohapatra. 2024. Weighted Grouped\\nQuery Attention in Transformers. CoRR abs/2407.10855 (2024). https://doi.org/\\n10.48550/ARXIV.2407.10855 arXiv:2407.10855\\n[51] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021. Unifying Vision-and-\\nLanguage Tasks via Text Generation. In Proceedings of the 38th International\\nConference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event\\n(Proceedings of Machine Learning Research, Vol. 139), Marina Meila and Tong\\nZhang (Eds.). PMLR, 1931–1942. http://proceedings.mlr.press/v139/cho21a.html\\n[52] Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg,\\nand Dario Amodei. 2017. Deep Reinforcement Learning from Human Pref-\\nerences. In Advances in Neural Information Processing Systems 30: Annual\\nConference on Neural Information Processing Systems 2017, December 4-9,\\n2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Ben-\\ngio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman\\nGarnett (Eds.). 4299–4307.\\nhttps://proceedings.neurips.cc/paper/2017/hash/\\nd5e2c0adad503c91f91df240d0cd4e49-Abstract.html\\n[53] Periklis Chrysogelos, Manos Karpathiotakis, Raja Appuswamy, and Anastasia\\nAilamaki. 2019. HetExchange: Encapsulating heterogeneous CPU-GPU par-\\nallelism in JIT compiled engines. Proc. VLDB Endow. 12, 5 (2019), 544–556.\\nhttps://doi.org/10.14778/3303753.3303760\\n[54] Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, and\\nJames R. Glass. 2024. Lookback Lens: Detecting and Mitigating Contextual Hallu-\\ncinations in Large Language Models Using Only Attention Maps. In Proceedings\\nof the 2024 Conference on Empirical Methods in Natural Language Processing,\\nEMNLP 2024, Miami, FL, USA, November 12-16, 2024, Yaser Al-Onaizan, Mohit\\nBansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics,\\n1419–1436. https://aclanthology.org/2024.emnlp-main.84\\n[55] code4DB. 2024. LLM4DB: A Curated List of Resources on Large Language\\nModels for Databases. https://github.com/code4DB/LLM4DB. Accessed: 2024-\\n11-30.\\n[56] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare\\nCampagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024. The\\nPower of Noise: Redefining Retrieval for RAG Systems. In Proceedings of the 47th\\nInternational ACM SIGIR Conference on Research and Development in Information\\nRetrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024, Grace Hui Yang,\\nHongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, and Yi Zhang (Eds.).\\nACM, 719–729. https://doi.org/10.1145/3626772.3657834\\n[57] Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou\\nWang, and Yaodong Yang. 2024. Safe RLHF: Safe Reinforcement Learning\\nfrom Human Feedback. In The Twelfth International Conference on Learning\\nRepresentations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net.\\nhttps://openreview.net/forum?id=TyFrPOKYXw\\n[58] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and\\nRuslan Salakhutdinov. 2019. Transformer-XL: Attentive Language Models\\nbeyond a Fixed-Length Context. In Proceedings of the 57th Conference of the\\nAssociation for Computational Linguistics, ACL 2019, Florence, Italy, July 28-\\nAugust 2, 2019, Volume 1: Long Papers, Anna Korhonen, David R. Traum, and\\nLluís Màrquez (Eds.). Association for Computational Linguistics, 2978–2988.\\nhttps://doi.org/10.18653/V1/P19-1285\\n[59] Tri Dao. 2024.\\nFlashAttention-2: Faster Attention with Better Parallelism\\nand Work Partitioning. In The Twelfth International Conference on Learning\\nRepresentations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net.\\nhttps://openreview.net/forum?id=mZn2Xyh9Ec\\n[60] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher\\nRé. 2022.\\nFlashAttention: Fast and Memory-Efficient Exact Attention\\nwith IO-Awareness. In Advances in Neural Information Processing Sys-\\ntems 35: Annual Conference on Neural Information Processing Systems\\n2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9,\\n2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho,\\nand A. Oh (Eds.).\\nhttp://papers.nips.cc/paper_files/paper/2022/hash/\\n67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html\\n[61] Tri Dao and Albert Gu. 2024. Transformers are SSMs: Generalized Models\\nand Efficient Algorithms Through Structured State Space Duality. In Forty-first\\nInternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July\\n21-27, 2024. OpenReview.net. https://openreview.net/forum?id=ztn8FCR1td\\n[62] Tri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. 2023.\\nFlash-\\nDecoding for long-context inference. https://pytorch.org/blog/flash-decoding/.\\nAccessed: December 15, 2024.\\n[63] Alessio Devoto, Yu Zhao, Simone Scardapane, and Pasquale Minervini. 2024. A\\nSimple and Effective L_2 Norm-Based Strategy for KV Cache Compression. In\\nProceedings of the 2024 Conference on Empirical Methods in Natural Language Pro-\\ncessing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, Yaser Al-Onaizan,\\nMohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Lin-\\nguistics, 18476–18499. https://aclanthology.org/2024.emnlp-main.1027\\n[64] Bosheng Ding, Chengwei Qin, Ruochen Zhao, Tianze Luo, Xinze Li, Guizhen\\nChen, Wenhan Xia, Junjie Hu, Anh Tuan Luu, and Shafiq Joty. 2024. Data\\nAugmentation using LLMs: Data Perspectives, Learning Paradigms and Chal-\\nlenges. In Findings of the Association for Computational Linguistics, ACL 2024,\\nBangkok, Thailand and virtual meeting, August 11-16, 2024, Lun-Wei Ku, Andre\\nMartins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,\\n1679–1705. https://doi.org/10.18653/V1/2024.FINDINGS-ACL.97\\n[65] Peng Ding, Jingyu Wu, Jun Kuang, Dan Ma, Xuezhi Cao, Xunliang Cai, Shi Chen,\\nJiajun Chen, and Shujian Huang. 2024. Hallu-PI: Evaluating Hallucination in\\nMulti-modal Large Language Models within Perturbed Inputs. In Proceedings of\\nthe 32nd ACM International Conference on Multimedia, MM 2024, Melbourne, VIC,\\nAustralia, 28 October 2024 - 1 November 2024, Jianfei Cai, Mohan S. Kankanhalli,\\nBalakrishnan Prabhakaran, Susanne Boll, Ramanathan Subramanian, Liang\\nZheng, Vivek K. Singh, Pablo César, Lexing Xie, and Dong Xu (Eds.). ACM,\\n10707–10715. https://doi.org/10.1145/3664647.3681251\\n[66] Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco,\\nDirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Documenting\\nLarge Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus.\\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language\\nProcessing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11\\nNovember, 2021, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and\\nScott Wen-tau Yih (Eds.). Association for Computational Linguistics, 1286–1305.\\nhttps://doi.org/10.18653/V1/2021.EMNLP-MAIN.98\\n[67] Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil\\nMahabaleshwarkar, Shih-Yang Liu, Matthijs Van Keirsbilck, Min-Hung Chen,\\nYoshi Suhara, et al. 2024. Hymba: A Hybrid-head Architecture for Small Lan-\\nguage Models. arXiv preprint arXiv:2411.13676 (2024).\\n[68] Xin Luna Dong, Seungwhan Moon, Yifan Ethan Xu, Kshitiz Malik, and Zhou\\nYu. 2023. Towards Next-Generation Intelligent Assistants Leveraging LLM\\nTechniques. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge\\nDiscovery and Data Mining, KDD 2023, Long Beach, CA, USA, August 6-10, 2023,\\nAmbuj K. Singh, Yizhou Sun, Leman Akoglu, Dimitrios Gunopulos, Xifeng\\nYan, Ravi Kumar, Fatma Ozcan, and Jieping Ye (Eds.). ACM, 5792–5793. https:\\n//doi.org/10.1145/3580305.3599572\\n[69] Yixin Dong, Charlie F Ruan, Yaxing Cai, Ruihang Lai, Ziyi Xu, Yilong Zhao, and\\nTianqi Chen. 2024. XGrammar: Flexible and Efficient Structured Generation\\nEngine for Large Language Models. arXiv preprint arXiv:2411.15100 (2024).\\n[70] Alexey Drutsa, Valentina Fedorova, Dmitry Ustalov, Olga Megorskaya,\\nEvfrosiniya Zerminova, and Daria Baidakova. 2020. Crowdsourcing Practice\\nfor Efficient Data Labeling: Aggregation, Incremental Relabeling, and Pric-\\ning. In Proceedings of the 2020 International Conference on Management of\\nData, SIGMOD Conference 2020, online conference [Portland, OR, USA], June\\n14-19, 2020, David Maier, Rachel Pottinger, AnHai Doan, Wang-Chiew Tan,\\nAbdussalam Alawini, and Hung Q. Ngo (Eds.). ACM, 2623–2627.\\nhttps:\\n//doi.org/10.1145/3318464.3383127\\n[71] Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin,\\nYuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat,\\nBarret Zoph, Liam Fedus, Maarten P. Bosma, Zongwei Zhou, Tao Wang,\\nYu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen S.\\nMeier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V. Le, Yonghui\\nWu, Zhifeng Chen, and Claire Cui. 2022.\\nGLaM: Efficient Scaling of Lan-\\nguage Models with Mixture-of-Experts. In International Conference on Machine\\nLearning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA (Proceedings\\nof Machine Learning Research, Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka,\\nLe Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (Eds.). PMLR, 5547–5569.\\nhttps://proceedings.mlr.press/v162/du22c.html\\n[72] Yaxin Du, Rui Ye, Yuchi Fengting, Wanru Zhao, Jingjing Qu, Yanfeng Wang, and\\nSiheng Chen. 2024. Data Quality Control in Federated Instruction-tuning of\\nLarge Language Models. CoRR abs/2410.11540 (2024). https://doi.org/10.48550/\\nARXIV.2410.11540 arXiv:2410.11540\\n[73] Fabio Duarte. 2024.\\nNumber of ChatGPT Users (Dec 2024).\\nhttps://\\nexplodingtopics.com/blog/chatgpt-users. Accessed: 2024-12-15.\\n[74] Matous Eibich, Shivay Nagpal, and Alexander Fred-Ojala. 2024. ARAGOG:\\nAdvanced RAG Output Grading. CoRR abs/2404.01037 (2024). https://doi.org/\\n10.48550/ARXIV.2404.01037 arXiv:2404.01037\\n[75] Oluwole Fagbohun, Rachel M. Harrison, and Anton Dereventsov. 2024. An\\nEmpirical Categorization of Prompting Techniques for Large Language Models:\\nA Practitioner’s Guide. CoRR abs/2402.14837 (2024). https://doi.org/10.48550/\\nARXIV.2402.14837 arXiv:2402.14837\\n[76] Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin,\\nTat-Seng Chua, and Qing Li. 2024.\\nA Survey on RAG Meeting LLMs: To-\\nwards Retrieval-Augmented Large Language Models. In Proceedings of the 30th\\nACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2024,\\nBarcelona, Spain, August 25-29, 2024, Ricardo Baeza-Yates and Francesco Bonchi\\n(Eds.). ACM, 6491–6501. https://doi.org/10.1145/3637528.3671470\\n[77] Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. 2024. Detecting\\nhallucinations in large language models using semantic entropy. Nat. 630, 8017\\n(2024), 625–630. https://doi.org/10.1038/S41586-024-07421-0\\n[78] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch Transformers:\\nScaling to Trillion Parameter Models with Simple and Efficient Sparsity. J. Mach.\\nLearn. Res. 23 (2022), 120:1–120:39. https://jmlr.org/papers/v23/21-0998.html\\n[79] Jonathan Feldstein, Paulius Dilkas, Vaishak Belle, and Efthymia Tsamoura. 2024.\\nMapping the Neuro-Symbolic AI Landscape by Architectures: A Handbook on\\nAugmenting Deep Learning Through Symbolic Reasoning. CoRR abs/2410.22077\\n(2024). https://doi.org/10.48550/ARXIV.2410.22077 arXiv:2410.22077\\n[80] Matthew Finlayson, John Hewitt, Alexander Koller, Swabha Swayamdipta, and\\nAshish Sabharwal. 2024. Closing the Curious Case of Neural Text Degeneration.\\nIn The Twelfth International Conference on Learning Representations, ICLR 2024,\\nVienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?\\nid=dONpC9GL1o\\n[81] Evan Frick, Tianle Li, Connor Chen, Wei-Lin Chiang, Anastasios N. Angelopou-\\nlos, Jiantao Jiao, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. 2024.\\nHow to Evaluate Reward Models for RLHF.\\nCoRR abs/2410.14872 (2024).\\nhttps://doi.org/10.48550/ARXIV.2410.14872 arXiv:2410.14872\\n[82] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi,\\nRuiqi Zhong, Scott Yih, Luke Zettlemoyer, and Mike Lewis. 2023. InCoder: A\\nGenerative Model for Code Infilling and Synthesis. In The Eleventh International\\nConference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5,\\n2023. OpenReview.net. https://openreview.net/forum?id=hQwb-lbM6EL\\n[83] Yuhan Fu, Ruobing Xie, Xingwu Sun, Zhanhui Kang, and Xirong Li. 2024. Miti-\\ngating Hallucination in Multimodal Large Language Model via Hallucination-\\ntargeted Direct Preference Optimization. arXiv preprint arXiv:2411.10436 (2024).\\n[84] Yao Fu, Leyang Xue, Yeqi Huang, Andrei-Octavian Brabete, Dmitrii Ustiugov,\\nYuvraj Patel, and Luo Mai. 2024. ServerlessLLM: Locality-Enhanced Serverless\\nInference for Large Language Models. CoRR abs/2401.14351 (2024).\\nhttps:\\n//doi.org/10.48550/ARXIV.2401.14351 arXiv:2401.14351\\n[85] Yichao Fu, Siqi Zhu, Runlong Su, Aurick Qiao, Ion Stoica, and Hao Zhang. 2024.\\nEfficient LLM Scheduling by Learning to Rank. CoRR abs/2408.15792 (2024).\\nhttps://doi.org/10.48550/ARXIV.2408.15792 arXiv:2408.15792\\n[86] Henning Funke and Jens Teubner. 2020. Data-parallel query processing on\\nnon-uniform data. Proceedings of the VLDB Endowment 13, 6 (2020), 884–897.\\n[87] Hang Gao and Yongfeng Zhang. 2024. VRSD: Rethinking Similarity and Diversity\\nfor Retrieval in Large Language Models. CoRR abs/2407.04573 (2024). https:\\n//doi.org/10.48550/ARXIV.2407.04573 arXiv:2407.04573\\n[88] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles\\nFoster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser,\\nand Connor Leahy. 2021. The Pile: An 800GB Dataset of Diverse Text for\\nLanguage Modeling. CoRR abs/2101.00027 (2021). arXiv:2101.00027 https:\\n//arxiv.org/abs/2101.00027\\n[89] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2023. Precise Zero-Shot\\nDense Retrieval without Relevance Labels. In Proceedings of the 61st Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\nACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber,\\nand Naoaki Okazaki (Eds.). Association for Computational Linguistics, 1762–\\n1777. https://doi.org/10.18653/V1/2023.ACL-LONG.99\\n[90] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi,\\nYi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023.\\nRetrieval-Augmented Generation for Large Language Models: A Survey.\\nCoRR abs/2312.10997 (2023).\\nhttps://doi.org/10.48550/ARXIV.2312.10997\\narXiv:2312.10997\\n[91] Aryo Pradipta Gema, Chen Jin, Ahmed Abdulaal, Tom Diethe, Philip Teare,\\nBeatrice Alex, Pasquale Minervini, and Amrutha Saseendran. 2024. DeCoRe:\\nDecoding by Contrasting Retrieval Heads to Mitigate Hallucinations. arXiv\\npreprint arXiv:2410.18860 (2024).\\n[92] Saibo Geng, Berkay Döner, Chris Wendler, Martin Josifoski, and Robert West.\\n2024.\\nSketch-Guided Constrained Decoding for Boosting Blackbox Large\\nLanguage Models without Logit Access. In Proceedings of the 62nd Annual\\nMeeting of the Association for Computational Linguistics, ACL 2024 - Short Pa-\\npers, Bangkok, Thailand, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and\\nVivek Srikumar (Eds.). Association for Computational Linguistics, 234–245.\\nhttps://aclanthology.org/2024.acl-short.23\\n[93] Saibo Geng, Martin Josifoski, Maxime Peyrard, and Robert West. 2023. Grammar-\\nConstrained Decoding for Structured NLP Tasks without Finetuning. In Proceed-\\nings of the 2023 Conference on Empirical Methods in Natural Language Processing,\\nEMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and\\nKalika Bali (Eds.). Association for Computational Linguistics, 10932–10952.\\nhttps://doi.org/10.18653/V1/2023.EMNLP-MAIN.674\\n[94] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney,\\nand Kurt Keutzer. 2021. A Survey of Quantization Methods for Efficient Neural\\nNetwork Inference. CoRR abs/2103.13630 (2021). arXiv:2103.13630 https://arxiv.\\norg/abs/2103.13630\\n[95] In Gim, Seung-seob Lee, and Lin Zhong. 2024. Asynchronous LLM Function\\nCalling. arXiv preprint arXiv:2412.07017 (2024).\\n[96] Michael R. Glass, Xueqing Wu, Ankita Rajaram Naik, Gaetano Rossiello,\\nand Alfio Gliozzo. 2023. Retrieval-Based Transformer for Table Augmenta-\\ntion. In Findings of the Association for Computational Linguistics: ACL 2023,\\nToronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and\\nNaoaki Okazaki (Eds.). Association for Computational Linguistics, 5635–5648.\\nhttps://doi.org/10.18653/V1/2023.FINDINGS-ACL.348\\n[97] Ruihao Gong, Yang Yong, Shiqiao Gu, Yushi Huang, Chengtao Lv, Yunchen\\nZhang, Dacheng Tao, and Xianglong Liu. 2024. LLMC: Benchmarking Large\\nLanguage Model Quantization with a Versatile Compression Toolkit. In Proceed-\\nings of the 2024 Conference on Empirical Methods in Natural Language Processing:\\nEMNLP 2024 - Industry Track, Miami, Florida, USA, November 12-16, 2024, Franck\\nDernoncourt, Daniel Preotiuc-Pietro, and Anastasia Shimorina (Eds.). Associ-\\nation for Computational Linguistics, 132–152. https://aclanthology.org/2024.\\nemnlp-industry.12\\n[98] Yury Gorishniy, Ivan Rubachev, Nikolay Kartashev, Daniil Shlenskii, Akim\\nKotelnikov, and Artem Babenko. 2023. TabR: Unlocking the Power of Retrieval-\\nAugmented Tabular Deep Learning. CoRR abs/2307.14338 (2023). https://doi.\\norg/10.48550/ARXIV.2307.14338 arXiv:2307.14338\\n[99] Albert Gu and Tri Dao. 2023. Mamba: Linear-Time Sequence Modeling with\\nSelective State Spaces. CoRR abs/2312.00752 (2023). https://doi.org/10.48550/\\nARXIV.2312.00752 arXiv:2312.00752\\n[100] Albert Gu, Karan Goel, and Christopher Ré. 2022. Efficiently Modeling Long\\nSequences with Structured State Spaces. In The Tenth International Conference\\non Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenRe-\\nview.net. https://openreview.net/forum?id=uYLFoz1vlAC\\n[101] Nuno Miguel Guerreiro, Duarte M. Alves, Jonas Waldendorf, Barry Haddow,\\nAlexandra Birch, Pierre Colombo, and André F. T. Martins. 2023. Hallucinations\\nin Large Multilingual Translation Models. Trans. Assoc. Comput. Linguistics 11\\n(2023), 1500–1517. https://doi.org/10.1162/TACL_A_00615\\n[102] Çaglar Gülçehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova,\\nLotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen\\nWang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, and\\nNando de Freitas. 2023. Reinforced Self-Training (ReST) for Language Model-\\ning. CoRR abs/2308.08998 (2023). https://doi.org/10.48550/ARXIV.2308.08998\\narXiv:2308.08998\\n[103] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del\\nGiorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa,\\nOlli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien\\nBubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023.\\nTextbooks Are All You Need. CoRR abs/2306.11644 (2023). https://doi.org/10.\\n48550/ARXIV.2306.11644 arXiv:2306.11644\\n[104] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V.\\nChawla, Olaf Wiest, and Xiangliang Zhang. 2024. Large Language Model Based\\nMulti-agents: A Survey of Progress and Challenges. In Proceedings of the Thirty-\\nThird International Joint Conference on Artificial Intelligence, IJCAI 2024, Jeju,\\nSouth Korea, August 3-9, 2024. ijcai.org, 8048–8057.\\nhttps://www.ijcai.org/\\nproceedings/2024/890\\n[105] Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil R.\\nDevanur, Gregory R. Ganger, and Phillip B. Gibbons. 2018. PipeDream: Fast\\nand Efficient Pipeline Parallel DNN Training. CoRR abs/1806.03377 (2018).\\narXiv:1806.03377 http://arxiv.org/abs/1806.03377\\n[106] Jahid Hasan. 2024. Optimizing Large Language Models through Quantiza-\\ntion: A Comparative Analysis of PTQ and QAT Techniques. arXiv preprint\\narXiv:2411.06084 (2024).\\n[107] Horace He, Driss Guessous, Yanbo Liang, and Joy Dong. 2024. FlexAttention:\\nThe Flexibility of PyTorch with the Performance of FlashAttention. https:\\n//pytorch.org/blog/flexattention/.\\n[108] Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo,\\nand Yunhe Wang. 2024. DenseMamba: State Space Models with Dense Hidden\\nConnection for Efficient Large Language Models. CoRR abs/2403.00818 (2024).\\nhttps://doi.org/10.48550/ARXIV.2403.00818 arXiv:2403.00818\\n[109] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann\\nLeCun, Xavier Bresson, and Bryan Hooi. 2024.\\nG-Retriever: Retrieval-\\nAugmented Generation for Textual Graph Understanding and Question Answer-\\ning. CoRR abs/2402.07630 (2024). https://doi.org/10.48550/ARXIV.2402.07630\\narXiv:2402.07630\\n[110] Xu Owen He. 2024. Mixture of A Million Experts. CoRR abs/2407.04153 (2024).\\nhttps://doi.org/10.48550/ARXIV.2407.04153 arXiv:2407.04153\\n[111] Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 2021.\\nScaling Laws for Transfer.\\nCoRR abs/2102.01293 (2021).\\narXiv:2102.01293\\nhttps://arxiv.org/abs/2102.01293\\n[112] Michael Hewing and Vincent Leinhos. 2024. The Prompt Canvas: A Literature-\\nBased Practitioner Guide for Creating Effective Prompts in Large Language\\nModels. arXiv preprint arXiv:2412.05127 (2024).\\n[113] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,\\nTrevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes\\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den\\nDriessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan,\\nErich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training\\nCompute-Optimal Large Language Models. CoRR abs/2203.15556 (2022). https:\\n//doi.org/10.48550/ARXIV.2203.15556 arXiv:2203.15556\\n[114] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The\\nCurious Case of Neural Text Degeneration. In 8th International Conference on\\nLearning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\\nOpenReview.net. https://openreview.net/forum?id=rygGQyrFvH\\n[115] Zijin Hong, Zheng Yuan, Hao Chen, Qinggang Zhang, Feiran Huang, and Xiao\\nHuang. 2024. Knowledge-to-SQL: Enhancing SQL Generation with Data Expert\\nLLM. In Findings of the Association for Computational Linguistics, ACL 2024,\\nBangkok, Thailand and virtual meeting, August 11-16, 2024, Lun-Wei Ku, Andre\\nMartins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,\\n10997–11008. https://doi.org/10.18653/V1/2024.FINDINGS-ACL.653\\n[116] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin\\nde Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\\nParameter-Efficient Transfer Learning for NLP. In Proceedings of the 36th In-\\nternational Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long\\nBeach, California, USA (Proceedings of Machine Learning Research, Vol. 97), Ka-\\nmalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, 2790–2799. http:\\n//proceedings.mlr.press/v97/houlsby19a.html\\n[117] Cheng-Yu Hsieh, Yung-Sung Chuang, Chun-Liang Li, Zifeng Wang, Long T.\\nLe, Abhishek Kumar, James R. Glass, Alexander Ratner, Chen-Yu Lee, Ranjay\\nKrishna, and Tomas Pfister. 2024. Found in the middle: Calibrating Positional\\nAttention Bias Improves Long Context Utilization. In Findings of the Association\\nfor Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting,\\nAugust 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.).\\nAssociation for Computational Linguistics, 14982–14995. https://doi.org/10.\\n18653/V1/2024.FINDINGS-ACL.890\\n[118] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao.\\n2023. ChatDB: Augmenting LLMs with Databases as Their Symbolic Mem-\\nory. CoRR abs/2306.03901 (2023). https://doi.org/10.48550/ARXIV.2306.03901\\narXiv:2306.03901\\n[119] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\\nWang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of\\nLarge Language Models. In The Tenth International Conference on Learning\\nRepresentations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\\nhttps://openreview.net/forum?id=nZeVKeeFYf9\\n[120] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen,\\nMia Xu Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and\\nZhifeng Chen. 2019. GPipe: Efficient Training of Giant Neural Networks us-\\ning Pipeline Parallelism. In Advances in Neural Information Processing Systems\\n32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS\\n2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo\\nLarochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Ro-\\nman Garnett (Eds.). 103–112. https://proceedings.neurips.cc/paper/2019/hash/\\n093f65e080a295f8076b1c5722a46aa2-Abstract.html\\n[121] Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei\\nVassilvitskii, and Sanmi Koyejo. 2024. Scaling Laws for Downstream Task\\nPerformance of Large Language Models. CoRR abs/2402.04177 (2024). https:\\n//doi.org/10.48550/ARXIV.2402.04177 arXiv:2402.04177\\n[122] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler.\\n2021. Data Movement Is All You Need: A Case Study on Optimizing Transform-\\ners. In Proceedings of the Fourth Conference on Machine Learning and Systems,\\nMLSys 2021, virtual, April 5-9, 2021, Alex Smola, Alex Dimakis, and Ion Stoica\\n(Eds.). mlsys.org. https://proceedings.mlsys.org/paper_files/paper/2021/hash/\\nbc86e95606a6392f51f95a8de106728d-Abstract.html\\n[123] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong Park.\\n2024. Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language\\nModels through Question Complexity. In Proceedings of the 2024 Conference of\\nthe North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico\\nCity, Mexico, June 16-21, 2024, Kevin Duh, Helena Gómez-Adorno, and Steven\\nBethard (Eds.). Association for Computational Linguistics, 7036–7050. https:\\n//doi.org/10.18653/V1/2024.NAACL-LONG.389\\n[124] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,\\nYejin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of Hallucination in\\nNatural Language Generation. ACM Comput. Surv. 55, 12 (2023), 248:1–248:38.\\nhttps://doi.org/10.1145/3571730\\n[125] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung.\\n2023. Towards Mitigating Hallucination in Large Language Models via Self-\\nReflection. CoRR abs/2310.06271 (2023). https://doi.org/10.48550/ARXIV.2310.\\n06271 arXiv:2310.06271\\n[126] Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, and Ji-Rong Wen.\\n2023. StructGPT: A General Framework for Large Language Model to Rea-\\nson over Structured Data. In Proceedings of the 2023 Conference on Empirical\\nMethods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10,\\n2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Com-\\nputational Linguistics, 9237–9251. https://doi.org/10.18653/V1/2023.EMNLP-\\nMAIN.574\\n[127] Xuanlin Jiang, Yang Zhou, Shiyi Cao, Ion Stoica, and Minlan Yu. 2024. NEO:\\nSaving GPU Memory Crisis with CPU Offloading for Online LLM Inference.\\narXiv preprint arXiv:2411.01142 (2024).\\n[128] Deokhyung Kang, Baikjin Jung, Yunsu Kim, and Gary Geunbae Lee. 2024. De-\\nnoising Table-Text Retrieval for Open-Domain Question Answering. In Pro-\\nceedings of the 2024 Joint International Conference on Computational Linguis-\\ntics, Language Resources and Evaluation, LREC/COLING 2024, 20-25 May, 2024,\\nTorino, Italy, Nicoletta Calzolari, Min-Yen Kan, Véronique Hoste, Alessandro\\nLenci, Sakriani Sakti, and Nianwen Xue (Eds.). ELRA and ICCL, 4634–4640.\\nhttps://aclanthology.org/2024.lrec-main.414\\n[129] Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar\\nKrishna, and Tuo Zhao. 2024. Gear: An efficient kv cache compression recipefor\\nnear-lossless generative inference of llm. arXiv preprint arXiv:2403.05527 (2024).\\n[130] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin\\nChess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.\\n2020. Scaling Laws for Neural Language Models. CoRR abs/2001.08361 (2020).\\narXiv:2001.08361 https://arxiv.org/abs/2001.08361\\n[131] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu,\\nSergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval\\nfor Open-Domain Question Answering. In Proceedings of the 2020 Conference\\non Empirical Methods in Natural Language Processing, EMNLP 2020, Online,\\nNovember 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu\\n(Eds.). Association for Computational Linguistics, 6769–6781. https://doi.org/\\n10.18653/V1/2020.EMNLP-MAIN.550\\n[132] George Katsogiannis-Meimarakis and Georgia Koutrika. 2021. A Deep Dive into\\nDeep Learning Approaches for Text-to-SQL Systems. In SIGMOD ’21: Interna-\\ntional Conference on Management of Data, Virtual Event, China, June 20-25, 2021,\\nGuoliang Li, Zhanhuai Li, Stratos Idreos, and Divesh Srivastava (Eds.). ACM,\\n2846–2851. https://doi.org/10.1145/3448016.3457543\\n[133] George Katsogiannis-Meimarakis, Mike Xydas, and Georgia Koutrika. 2023.\\nNatural Language Interfaces for Databases with Deep Learning. Proc. VLDB\\nEndow. 16, 12 (2023), 3878–3881. https://doi.org/10.14778/3611540.3611575\\n[134] Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke Hüllermeier. 2023. A\\nSurvey of Reinforcement Learning from Human Feedback. CoRR abs/2312.14925\\n(2023). https://doi.org/10.48550/ARXIV.2312.14925 arXiv:2312.14925\\n[135] Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael\\nBendersky. 2024. Bridging the Preference Gap between Retrievers and LLMs.\\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16,\\n2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for\\nComputational Linguistics, 10438–10451.\\nhttps://doi.org/10.18653/V1/2024.\\nACL-LONG.562\\n[136] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav\\nSanthanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi,\\nHanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. 2024.\\nDSPy: Compiling Declarative Language Model Calls into State-of-the-Art\\nPipelines. In The Twelfth International Conference on Learning Representa-\\ntions, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net.\\nhttps:\\n//openreview.net/forum?id=sY5N0zY5Od\\n[137] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage\\nSearch via Contextualized Late Interaction over BERT. In Proceedings of the 43rd\\nInternational ACM SIGIR conference on research and development in Information\\nRetrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020, Jimmy X. Huang,\\nYi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun\\nLiu (Eds.). ACM, 39–48. https://doi.org/10.1145/3397271.3401075\\n[138] Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon\\nKim, and Chanjun Park. 2024.\\nsDPO: Don’t Use Your Data All at Once.\\nCoRR abs/2403.19270 (2024).\\nhttps://doi.org/10.48550/ARXIV.2403.19270\\narXiv:2403.19270\\n[139] Kyoungmin Kim, Kijae Hong, Caglar Gulcehre, and Anastasia Ailamaki. 2024.\\nThe Effect of Scheduling and Preemption on the Efficiency of LLM Inference\\nServing. arXiv preprint arXiv:2411.07447 (2024).\\n[140] Kyoungmin Kim, Jisung Jung, In Seo, Wook-Shin Han, Kangwoo Choi, and\\nJaehyok Chong. 2022. Learned Cardinality Estimation: An In-depth Study. In\\nSIGMOD ’22: International Conference on Management of Data, Philadelphia, PA,\\nUSA, June 12 - 17, 2022, Zachary G. Ives, Angela Bonifati, and Amr El Abbadi\\n(Eds.). ACM, 1214–1227. https://doi.org/10.1145/3514221.3526154\\n[141] Kyoungmin Kim, Sangoh Lee, Injung Kim, and Wook-Shin Han. 2024. ASM:\\nHarmonizing Autoregressive Model, Sampling, and Multi-dimensional Statistics\\nMerging for Cardinality Estimation. Proc. ACM Manag. Data 2, 1 (2024), 45:1–\\n45:27. https://doi.org/10.1145/3639300\\n[142] Seungone Kim, Juyoung Suk, Xiang Yue, Vijay Viswanathan, Seongyun Lee,\\nYizhong Wang, Kiril Gashteovski, Carolin Lawrence, Sean Welleck, and Graham\\nNeubig. 2024. Evaluating Language Models as Synthetic Data Generators. arXiv\\npreprint arXiv:2412.03679 (2024).\\n[143] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Yacine\\nJernite, Margaret Mitchell, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf,\\nDzmitry Bahdanau, Leandro von Werra, and Harm de Vries. 2023. The Stack: 3\\nTB of permissively licensed source code. Trans. Mach. Learn. Res. 2023 (2023).\\nhttps://openreview.net/forum?id=pxpbTdUEpD\\n[144] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke\\nIwasawa. 2022. Large Language Models are Zero-Shot Reasoners. In Advances in\\nNeural Information Processing Systems 35: Annual Conference on Neural Informa-\\ntion Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 -\\nDecember 9, 2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave,\\nK. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper_files/paper/2022/hash/\\n8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html\\n[145] Juri Kong, Hong Liang, Yuan Zhang, Hongxiang Li, Pengcheng Shen, and Fang\\nLu. 2024. Dynamic semantic memory retention in large language models: An\\nexploration of spontaneous retrieval mechanisms. (2024).\\n[146] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,\\nCody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient\\nMemory Management for Large Language Model Serving with PagedAttention.\\nIn Proceedings of the 29th Symposium on Operating Systems Principles, SOSP\\n2023, Koblenz, Germany, October 23-26, 2023, Jason Flinn, Margo I. Seltzer, Pe-\\nter Druschel, Antoine Kaufmann, and Jonathan Mace (Eds.). ACM, 611–626.\\nhttps://doi.org/10.1145/3600006.3613165\\n[147] Jiedong Lang, Zhehao Guo, and Shuyu Huang. 2024.\\nA Comprehensive\\nStudy on Quantization Techniques for Large Language Models. arXiv preprint\\narXiv:2411.02530 (2024).\\n[148] Sangjin Lee, Alberto Lerner, Philippe Bonnet, and Philippe Cudré-Mauroux.\\n2024. Database Kernels: Seamless Integration of Database Systems and Fast\\nStorage via CXL. In 14th Conference on Innovative Data Systems Research, CIDR\\n2024, Chaminade, HI, USA, January 14-17, 2024. www.cidrdb.org. https://www.\\ncidrdb.org/cidr2024/papers/p43-lee.pdf\\n[149] Wonbeom Lee, Jungi Lee, Junghwan Seo, and Jaewoong Sim. 2024. InfiniGen:\\nEfficient Generative Inference of Large Language Models with Dynamic KV\\nCache Management. In 18th USENIX Symposium on Operating Systems Design\\nand Implementation, OSDI 2024, Santa Clara, CA, USA, July 10-12, 2024, Ada\\nGavrilovska and Douglas B. Terry (Eds.). USENIX Association, 155–172. https:\\n//www.usenix.org/conference/osdi24/presentation/lee\\n[150] Younghun Lee, Sungchul Kim, Tong Yu, Ryan A. Rossi, and Xiang Chen. 2024.\\nLearning to Reduce: Optimal Representations of Structured Data in Prompting\\nLarge Language Models. CoRR abs/2402.14195 (2024). https://doi.org/10.48550/\\nARXIV.2402.14195 arXiv:2402.14195\\n[151] Fangyu Lei, Jixuan Chen, Yuxiao Ye, Ruisheng Cao, Dongchan Shin, Hongjin Su,\\nZhaoqing Suo, Hongcheng Gao, Wenjing Hu, Pengcheng Yin, et al. 2024. Spider\\n2.0: Evaluating language models on real-world enterprise text-to-sql workflows.\\narXiv preprint arXiv:2411.07763 (2024).\\n[152] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale\\nfor Parameter-Efficient Prompt Tuning. In Proceedings of the 2021 Conference\\non Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual\\nEvent / Punta Cana, Dominican Republic, 7-11 November, 2021, Marie-Francine\\nMoens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association\\nfor Computational Linguistics, 3045–3059. https://doi.org/10.18653/V1/2021.\\nEMNLP-MAIN.243\\n[153] Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast Inference from\\nTransformers via Speculative Decoding. In International Conference on Machine\\nLearning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA (Proceedings of Ma-\\nchine Learning Research, Vol. 202), Andreas Krause, Emma Brunskill, Kyunghyun\\nCho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR,\\n19274–19286. https://proceedings.mlr.press/v202/leviathan23a.html\\n[154] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\\nKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim\\nRocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented\\nGeneration for Knowledge-Intensive NLP Tasks. In Advances in Neural In-\\nformation Processing Systems 33: Annual Conference on Neural Information\\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo\\nLarochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and\\nHsuan-Tien Lin (Eds.).\\nhttps://proceedings.neurips.cc/paper/2020/hash/\\n6b493230205f780e1bc26945df7481e5-Abstract.html\\n[155] Dengchun Li, Yingzi Ma, Naizheng Wang, Zhiyuan Cheng, Lei Duan, Jie Zuo,\\nCal Yang, and Mingjie Tang. 2024. MixLoRA: Enhancing Large Language Models\\nFine-Tuning with LoRA based Mixture of Experts. CoRR abs/2404.15159 (2024).\\nhttps://doi.org/10.48550/ARXIV.2404.15159 arXiv:2404.15159\\n[156] Guoliang Li, Xuanhe Zhou, and Lei Cao. 2021. AI Meets Database: AI4DB and\\nDB4AI. In SIGMOD ’21: International Conference on Management of Data, Virtual\\nEvent, China, June 20-25, 2021, Guoliang Li, Zhanhuai Li, Stratos Idreos, and\\nDivesh Srivastava (Eds.). ACM, 2859–2866.\\nhttps://doi.org/10.1145/3448016.\\n3457542\\n[157] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak\\nGadre, Hritik Bansal, Etash Kumar Guha, Sedrick Keh, Kushal Arora, Saurabh\\nGarg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee\\nChen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bit-\\nton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh\\nGardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah M. Pratt, Sunny\\nSanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan,\\nJieyu Zhang, Khyathi Raghavi Chandu, Thao Nguyen, Igor Vasiljevic, Sham M.\\nKakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke\\nZettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander To-\\nshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia\\nJitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Lud-\\nwig Schmidt, and Vaishaal Shankar. 2024. DataComp-LM: In search of the next\\ngeneration of training sets for language models. CoRR abs/2406.11794 (2024).\\nhttps://doi.org/10.48550/ARXIV.2406.11794 arXiv:2406.11794\\n[158] Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin\\nWang, Bowen Qin, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma,\\nGuoliang Li, Kevin Chen-Chuan Chang, Fei Huang, Reynold Cheng, and\\nYongbin Li. 2023.\\nCan LLM Already Serve as A Database Interface? A\\nBIg Bench for Large-Scale Database Grounded Text-to-SQLs. In Advances\\nin Neural Information Processing Systems 36: Annual Conference on Neural\\nInformation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA,\\nDecember 10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate\\nSaenko, Moritz Hardt, and Sergey Levine (Eds.).\\nhttp://papers.nips.cc/\\npaper_files/paper/2023/hash/83fc8fab1710363050bbd1d4b8cc0021-Abstract-\\nDatasets_and_Benchmarks.html\\n[159] Lingyu Li, Yixu Wang, Haiquan Zhao, Shuqi Kong, Yan Teng, Chunbo Li, and\\nYingchun Wang. 2024. Reflection-Bench: probing AI intelligence with reflection.\\narXiv preprint arXiv:2410.16270 (2024).\\n[160] Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\\nDanielle Rifinski Fainman, Dongmei Zhang, and Surajit Chaudhuri. 2024. Table-\\nGPT: Table Fine-tuned GPT for Diverse Table Tasks. Proc. ACM Manag. Data 2,\\n3 (2024), 176. https://doi.org/10.1145/3654979\\n[161] Xiangyu Li, Yuanchun Li, Yuanzhe Li, Ting Cao, and Yunxin Liu. 2024. FlexNN:\\nEfficient and Adaptive DNN Inference on Memory-Constrained Edge Devices.\\nIn Proceedings of the 30th Annual International Conference on Mobile Computing\\nand Networking, ACM MobiCom 2024, Washington D.C., DC, USA, November\\n18-22, 2024, Weisong Shi, Deepak Ganesan, and Nicholas D. Lane (Eds.). ACM,\\n709–723. https://doi.org/10.1145/3636534.3649391\\n[162] Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous\\nPrompts for Generation. In Proceedings of the 59th Annual Meeting of the As-\\nsociation for Computational Linguistics and the 11th International Joint Con-\\nference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long\\nPapers), Virtual Event, August 1-6, 2021, Chengqing Zong, Fei Xia, Wenjie Li, and\\nRoberto Navigli (Eds.). Association for Computational Linguistics, 4582–4597.\\nhttps://doi.org/10.18653/V1/2021.ACL-LONG.353\\n[163] Yichuan Li, Kaize Ding, Jianling Wang, and Kyumin Lee. 2024. Empowering\\nLarge Language Models for Textual Data Augmentation. In Findings of the\\nAssociation for Computational Linguistics, ACL 2024, Bangkok, Thailand and\\nvirtual meeting, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek\\nSrikumar (Eds.). Association for Computational Linguistics, 12734–12751. https:\\n//doi.org/10.18653/V1/2024.FINDINGS-ACL.756\\n[164] Yuhang Li, Mingzhu Shen, Jian Ma, Yan Ren, Mingxin Zhao, Qi Zhang, Ruihao\\nGong, Fengwei Yu, and Junjie Yan. 2021. MQBench: Towards Reproducible\\nand Deployable Model Quantization Benchmark. In Proceedings of the Neural\\nInformation Processing Systems Track on Datasets and Benchmarks 1, NeurIPS\\nDatasets and Benchmarks 2021, December 2021, virtual, Joaquin Vanschoren\\nand Sai-Kit Yeung (Eds.). https://datasets-benchmarks-proceedings.neurips.cc/\\npaper/2021/hash/c20ad4d76fe97759aa27a0c99bff6710-Abstract-round1.html\\n[165] Yuliang Li, Xiaolan Wang, Zhengjie Miao, and Wang-Chiew Tan. 2021. Data\\nAugmentation for ML-driven Data Preparation and Integration. Proc. VLDB\\nEndow. 14, 12 (2021), 3182–3185. https://doi.org/10.14778/3476311.3476403\\n[166] Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky.\\n2024. Retrieval Augmented Generation or Long-Context LLMs? A Compre-\\nhensive Study and Hybrid Approach. In Proceedings of the 2024 Conference\\non Empirical Methods in Natural Language Processing: EMNLP 2024 - Industry\\nTrack, Miami, Florida, USA, November 12-16, 2024, Franck Dernoncourt, Daniel\\nPreotiuc-Pietro, and Anastasia Shimorina (Eds.). Association for Computational\\nLinguistics, 881–893. https://aclanthology.org/2024.emnlp-industry.66\\n[167] Zihao Li, Zhuoran Yang, and Mengdi Wang. 2023.\\nReinforcement Learn-\\ning with Human Feedback: Learning Dynamic Choices via Pessimism.\\nCoRR abs/2305.18438 (2023).\\nhttps://doi.org/10.48550/ARXIV.2305.18438\\narXiv:2305.18438\\n[168] Zhaodonghui Li, Haitao Yuan, Huiming Wang, Gao Cong, and Lidong Bing.\\n2024. LLM-R2: A Large Language Model Enhanced Rule-based Rewrite System\\nfor Boosting Query Efficiency. CoRR abs/2404.12872 (2024). https://doi.org/10.\\n48550/ARXIV.2404.12872 arXiv:2404.12872\\n[169] Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Sihang\\nZhou, Xinwang Liu, Fuchun Sun, and Kunlun He. 2024. A Survey of Knowledge\\nGraph Reasoning on Graph Types: Static, Dynamic, and Multi-Modal. IEEE\\nTrans. Pattern Anal. Mach. Intell. 46, 12 (2024), 9456–9478. https://doi.org/10.\\n1109/TPAMI.2024.3417451\\n[170] Sean Lie. 2023. Cerebras Architecture Deep Dive: First Look Inside the Hard-\\nware/Software Co-Design for Deep Learning. IEEE Micro 43, 3 (2023), 18–30.\\nhttps://doi.org/10.1109/MM.2023.3256384\\n[171] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedi-\\ngos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri\\nAbend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael\\nGokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor\\nZusman, and Yoav Shoham. 2024. Jamba: A Hybrid Transformer-Mamba Lan-\\nguage Model. CoRR abs/2403.19887 (2024). https://doi.org/10.48550/ARXIV.\\n2403.19887 arXiv:2403.19887\\n[172] Jonathan Light, Min Cai, Weiqin Chen, Guanzhi Wang, Xiusi Chen, Wei Cheng,\\nYisong Yue, and Ziniu Hu. 2024. Strategist: Learning Strategic Skills by LLMs\\nvia Bi-Level Tree Search. CoRR abs/2408.10635 (2024). https://doi.org/10.48550/\\nARXIV.2408.10635 arXiv:2408.10635\\n[173] Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen\\nChen, and Lili Qiu. 2024. Parrot: Efficient Serving of LLM-based Applications\\nwith Semantic Variable. In 18th USENIX Symposium on Operating Systems Design\\nand Implementation, OSDI 2024, Santa Clara, CA, USA, July 10-12, 2024, Ada\\nGavrilovska and Douglas B. Terry (Eds.). USENIX Association, 929–945. https:\\n//www.usenix.org/conference/osdi24/presentation/lin-chaofan\\n[174] Yiming Lin and Sharad Mehrotra. 2024. PLAQUE: Automated Predicate Learning\\nat Query Time. Proc. ACM Manag. Data 2, 1 (2024), 46:1–46:25. https://doi.org/\\n10.1145/3639301\\n[175] Zicheng Lin, Tian Liang, Jiahao Xu, Xing Wang, Ruilin Luo, Chufan Shi, Siheng\\nLi, Yujiu Yang, and Zhaopeng Tu. 2024. Critical Tokens Matter: Token-Level\\nContrastive Estimation Enhence LLM’s Reasoning Capability. arXiv preprint\\narXiv:2411.19943 (2024).\\n[176] Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza Haffari, and Bohan\\nZhuang. 2024. MiniCache: KV Cache Compression in Depth Dimension for\\nLarge Language Models. CoRR abs/2405.14366 (2024). https://doi.org/10.48550/\\nARXIV.2405.14366 arXiv:2405.14366\\n[177] Chunwei Liu, Matthew Russo, Michael J. Cafarella, Lei Cao, Peter Baile Chen, Zui\\nChen, Michael J. Franklin, Tim Kraska, Samuel Madden, and Gerardo Vitagliano.\\n2024. A Declarative System for Optimizing AI Workloads. CoRR abs/2405.14696\\n(2024). https://doi.org/10.48550/ARXIV.2405.14696 arXiv:2405.14696\\n[178] Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang,\\nQi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang,\\nYuqing Yang, and Lili Qiu. 2024. RetrievalAttention: Accelerating Long-Context\\nLLM Inference via Vector Retrieval. CoRR abs/2409.10516 (2024). https://doi.\\norg/10.48550/ARXIV.2409.10516 arXiv:2409.10516\\n[179] Hao Liu and Pieter Abbeel. 2023. Emergent Agentic Transformer from Chain of\\nHindsight Experience. In International Conference on Machine Learning, ICML\\n2023, 23-29 July 2023, Honolulu, Hawaii, USA (Proceedings of Machine Learning\\nResearch, Vol. 202), Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara\\nEngelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 21362–21374.\\nhttps://proceedings.mlr.press/v202/liu23a.html\\n[180] Jinshu Liu, Hamid Hadian, Hanchen Xu, Daniel S. Berger, and Huaicheng Li.\\n2024. Dissecting CXL Memory Performance at Scale: Analysis, Modeling, and\\nOptimization. CoRR abs/2409.14317 (2024). https://doi.org/10.48550/ARXIV.\\n2409.14317 arXiv:2409.14317\\n[181] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,\\nFabio Petroni, and Percy Liang. 2024.\\nLost in the Middle: How Language\\nModels Use Long Contexts. Trans. Assoc. Comput. Linguistics 12 (2024), 157–173.\\nhttps://doi.org/10.1162/TACL_A_00638\\n[182] Shu Liu, Asim Biswal, Audrey Cheng, Xiangxi Mo, Shiyi Cao, Joseph E. Gonzalez,\\nIon Stoica, and Matei Zaharia. 2024. Optimizing LLM Queries in Relational\\nWorkloads. CoRR abs/2403.05821 (2024). https://doi.org/10.48550/ARXIV.2403.\\n05821 arXiv:2403.05821\\n[183] Weijie Liu, Zecheng Tang, Juntao Li, Kehai Chen, and Min Zhang.\\n2024.\\nMemLong: Memory-Augmented Retrieval for Long Text Modeling.\\nCoRR abs/2408.16967 (2024).\\nhttps://doi.org/10.48550/ARXIV.2408.16967\\narXiv:2408.16967\\n[184] Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng\\nZhang, Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, Michael\\nMaire, Henry Hoffmann, Ari Holtzman, and Junchen Jiang. 2024. CacheGen:\\nKV Cache Compression and Streaming for Fast Large Language Model Serving.\\nIn Proceedings of the ACM SIGCOMM 2024 Conference, ACM SIGCOMM 2024,\\nSydney, NSW, Australia, August 4-8, 2024. ACM, 38–56. https://doi.org/10.1145/\\n3651890.3672274\\n[185] Yanming Liu, Xinyue Peng, Xuhong Zhang, Weihao Liu, Jianwei Yin, Jiannan\\nCao, and Tianyu Du. 2024. RA-ISF: Learning to Answer and Understand from\\nRetrieval Augmentation via Iterative Self-Feedback. In Findings of the Association\\nfor Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting,\\nAugust 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.).\\nAssociation for Computational Linguistics, 4730–4749. https://doi.org/10.18653/\\nV1/2024.FINDINGS-ACL.281\\n[186] Chao Lou, Zixia Jia, Zilong Zheng, and Kewei Tu. 2024. Sparser is Faster\\nand Less is More: Efficient Sparse Attention for Long-Range Transform-\\ners. CoRR abs/2406.16747 (2024). https://doi.org/10.48550/ARXIV.2406.16747\\narXiv:2406.16747\\n[187] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David\\nHa. 2024. The AI Scientist: Towards Fully Automated Open-Ended Scientific\\nDiscovery. CoRR abs/2408.06292 (2024). https://doi.org/10.48550/ARXIV.2408.\\n06292 arXiv:2408.06292\\n[188] Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Felix\\nBai, Shuang Ma, Shen Ma, Mengyu Li, Guoli Yin, Zirui Wang, and Ruoming\\nPang. 2024. ToolSandbox: A Stateful, Conversational, Interactive Evaluation\\nBenchmark for LLM Tool Use Capabilities. CoRR abs/2408.04682 (2024). https:\\n//doi.org/10.48550/ARXIV.2408.04682 arXiv:2408.04682\\n[189] Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang,\\nNicholas D. Lane, and Mengwei Xu. 2024. Small Language Models: Survey,\\nMeasurements, and Insights. CoRR abs/2409.15790 (2024). https://doi.org/10.\\n48550/ARXIV.2409.15790 arXiv:2409.15790\\n[190] Kaixin Ma, Hao Cheng, Xiaodong Liu, Eric Nyberg, and Jianfeng Gao. 2022.\\nOpen-domain Question Answering via Chain of Reasoning over Heterogeneous\\nKnowledge. In Findings of the Association for Computational Linguistics: EMNLP\\n2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zor-\\nnitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics,\\n5360–5374. https://doi.org/10.18653/V1/2022.FINDINGS-EMNLP.392\\n[191] Ziming Mao, Tian Xia, Zhanghao Wu, Wei-Lin Chiang, Tyler Griggs, Romil\\nBhardwaj, Zongheng Yang, Scott Shenker, and Ion Stoica. 2024.\\nSky-\\nServe: Serving AI Models across Regions and Clouds with Spot Instances.\\nCoRR abs/2411.01438 (2024).\\nhttps://doi.org/10.48550/ARXIV.2411.01438\\narXiv:2411.01438\\n[192] Wes McKinney. 2010. Data Structures for Statistical Computing in Python. In\\nProceedings of the 9th Python in Science Conference, Stéfan van der Walt and\\nJarrod Millman (Eds.). 56–61. https://doi.org/10.25080/Majora-92bf1922-00a\\n[193] Kevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, and David\\nBau. 2023. Mass-Editing Memory in a Transformer. In The Eleventh International\\nConference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5,\\n2023. OpenReview.net. https://openreview.net/forum?id=MkbcAHIYgyS\\n[194] Xupeng Miao, Zhihao Jia, and Bin Cui. 2024. Demystifying Data Management\\nfor Large Language Models. In Companion of the 2024 International Conference\\non Management of Data, SIGMOD/PODS 2024, Santiago AA, Chile, June 9-15, 2024,\\nPablo Barceló, Nayat Sánchez-Pi, Alexandra Meliou, and S. Sudarshan (Eds.).\\nACM, 547–555. https://doi.org/10.1145/3626246.3654683\\n[195] Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham\\nNeubig, Yulia Tsvetkov, and Hannaneh Hajishirzi. 2024. Fine-grained Hallucina-\\ntion Detection and Editing for Language Models. CoRR abs/2401.06855 (2024).\\nhttps://doi.org/10.48550/ARXIV.2401.06855 arXiv:2401.06855\\n[196] M. Mehdi Mojarradi, Lingyi Yang, Robert McCraith, and Adam Mahdi.\\n2024. Improving In-Context Learning with Small Language Model Ensem-\\nbles. CoRR abs/2410.21868 (2024). https://doi.org/10.48550/ARXIV.2410.21868\\narXiv:2410.21868\\n[197] Laurent Mombaerts, Terry Ding, Adi Banerjee, Florian Felice, Jonathan Taws,\\nand Tarik Borogovac. 2024. Meta Knowledge for Retrieval Augmented Large\\nLanguage Models. CoRR abs/2408.09017 (2024). https://doi.org/10.48550/ARXIV.\\n2408.09017 arXiv:2408.09017\\n[198] Niklas Muennighoff, Qian Liu, Armel Randy Zebaze, Qinkai Zheng, Binyuan\\nHui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and\\nShayne Longpre. 2024. OctoPack: Instruction Tuning Code Large Language\\nModels. In The Twelfth International Conference on Learning Representations,\\nICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.\\nnet/forum?id=mw1PWNSWZP\\n[199] Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. 2024. Generative Representational Instruc-\\ntion Tuning. CoRR abs/2402.09906 (2024). https://doi.org/10.48550/ARXIV.2402.\\n09906 arXiv:2402.09906\\n[200] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart\\nvan Baalen, and Tijmen Blankevoort. 2021. A White Paper on Neural Network\\nQuantization. CoRR abs/2106.08295 (2021). arXiv:2106.08295 https://arxiv.org/\\nabs/2106.08295\\n[201] Fatemeh Nargesian, Abolfazl Asudeh, and H. V. Jagadish. 2022. Responsible\\nData Integration: Next-generation Challenges. In SIGMOD ’22: International\\nConference on Management of Data, Philadelphia, PA, USA, June 12 - 17, 2022,\\nZachary G. Ives, Angela Bonifati, and Amr El Abbadi (Eds.). ACM, 2458–2464.\\nhttps://doi.org/10.1145/3514221.3522567\\n[202] Chien Van Nguyen, Xuan Shen, Ryan Aponte, Yu Xia, Samyadeep Basu, Zheng-\\nmian Hu, Jian Chen, Mihir Parmar, Sasidhar Kunapuli, Joe Barrow, Junda Wu,\\nAshish Singh, Yu Wang, Jiuxiang Gu, Franck Dernoncourt, Nesreen K. Ahmed,\\nNedim Lipka, Ruiyi Zhang, Xiang Chen, Tong Yu, Sungchul Kim, Hanieh Deil-\\namsalehy, Namyong Park, Mike Rimer, Zhehao Zhang, Huanrui Yang, Ryan A.\\nRossi, and Thien Huu Nguyen. 2024.\\nA Survey of Small Language Mod-\\nels. CoRR abs/2410.20011 (2024). https://doi.org/10.48550/ARXIV.2410.20011\\narXiv:2410.20011\\n[203] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma,\\nVincent Y. Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. 2022.\\nLarge Dual Encoders Are Generalizable Retrievers. In Proceedings of the 2022\\nConference on Empirical Methods in Natural Language Processing, EMNLP 2022,\\nAbu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zornitsa\\nKozareva, and Yue Zhang (Eds.). Association for Computational Linguistics,\\n9844–9855. https://doi.org/10.18653/V1/2022.EMNLP-MAIN.669\\n[204] Noa Nonkes, Sergei Agaronian, Evangelos Kanoulas, and Roxana Petcu. 2024.\\nLeveraging Graph Structures to Detect Hallucinations in Large Language Mod-\\nels. CoRR abs/2407.04485 (2024). https://doi.org/10.48550/ARXIV.2407.04485\\narXiv:2407.04485\\n[205] OpenAI. 2024. Introducing OpenAI o1. https://openai.com/o1.\\nAccessed:\\n2024-12-15.\\n[206] Laurel J. Orr, Srikanth Kandula, and Surajit Chaudhuri. 2019. Pushing Data-\\nInduced Predicates Through Joins in Big-Data Clusters. Proc. VLDB Endow. 13,\\n3 (2019), 252–265. https://doi.org/10.14778/3368289.3368292\\n[207] Laurel J. Orr, Atindriyo Sanyal, Xiao Ling, Karan Goel, and Megan Leszczyn-\\nski. 2021.\\nManaging ML Pipelines: Feature Stores and the Coming Wave\\nof Embedding Ecosystems.\\nProc. VLDB Endow. 14, 12 (2021), 3178–3181.\\nhttps://doi.org/10.14778/3476311.3476402\\n[208] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,\\nPamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,\\nAmanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\\n2022. Training language models to follow instructions with human feedback. In\\nAdvances in Neural Information Processing Systems 35: Annual Conference on Neu-\\nral Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, No-\\nvember 28 - December 9, 2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle\\nBelgrave, K. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper_files/paper/\\n2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html\\n[209] Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin\\nMuller, Margaret Li, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer,\\nGargi Ghosh, Mike Lewis, Ari Holtzman, and Srini Iyer. 2024. Byte Latent Trans-\\nformer: Patches Scale Better Than Tokens. (2024). https://ai.meta.com/research/\\npublications/byte-latent-transformer-patches-scale-better-than-tokens/\\n[210] James Jie Pan, Jianguo Wang, and Guoliang Li. 2024. Survey of vector database\\nmanagement systems. VLDB J. 33, 5 (2024), 1591–1615. https://doi.org/10.1007/\\nS00778-024-00864-X\\n[211] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu.\\n2024. Unifying large language models and knowledge graphs: A roadmap. IEEE\\nTransactions on Knowledge and Data Engineering (2024).\\n[212] Xuchen Pan, Dawei Gao, Yuexiang Xie, Zhewei Wei, Yaliang Li, Bolin Ding,\\nJi-Rong Wen, and Jingren Zhou. 2024. Very Large-Scale Multi-Agent Simulation\\nin AgentScope. CoRR abs/2407.17789 (2024). https://doi.org/10.48550/ARXIV.\\n2407.17789 arXiv:2407.17789\\n[213] Xiurui Pan, Endian Li, Qiao Li, Shengwen Liang, Yizhou Shan, Ke Zhou, Yingwei\\nLuo, Xiaolin Wang, and Jie Zhang. 2024. InstInfer: In-Storage Attention Of-\\nfloading for Cost-Effective Long-Context LLM Inference. CoRR abs/2409.04992\\n(2024). https://doi.org/10.48550/ARXIV.2409.04992 arXiv:2409.04992\\n[214] Joon Sung Park, Joseph C. O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy\\nLiang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra\\nof Human Behavior. In Proceedings of the 36th Annual ACM Symposium on\\nUser Interface Software and Technology, UIST 2023, San Francisco, CA, USA, 29\\nOctober 2023- 1 November 2023, Sean Follmer, Jeff Han, Jürgen Steimle, and\\nNathalie Henry Riche (Eds.). ACM, 2:1–2:22. https://doi.org/10.1145/3586183.\\n3606763\\n[215] Dylan Patel and Afzal Ahmad. 2023. The Inference Cost of Search Disruption –\\nLarge Language Model Cost Analysis. https://www.semianalysis.com/p/the-\\ninference-cost-of-search-disruption. Accessed: 2024-12-15.\\n[216] Liana Patel, Siddharth Jha, Carlos Guestrin, and Matei Zaharia. 2024. LOTUS: En-\\nabling Semantic Queries with LLMs Over Tables of Unstructured and Structured\\nData. CoRR abs/2407.11418 (2024). https://doi.org/10.48550/ARXIV.2407.11418\\narXiv:2407.11418\\n[217] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed\\nMaleki, and Ricardo Bianchini. 2024. Splitwise: Efficient Generative LLM Infer-\\nence Using Phase Splitting. In 51st ACM/IEEE Annual International Symposium\\non Computer Architecture, ISCA 2024, Buenos Aires, Argentina, June 29 - July 3,\\n2024. IEEE, 118–132. https://doi.org/10.1109/ISCA59077.2024.00019\\n[218] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023. Gorilla:\\nLarge Language Model Connected with Massive APIs. CoRR abs/2305.15334\\n(2023). https://doi.org/10.48550/ARXIV.2305.15334 arXiv:2305.15334\\n[219] Johns Paul, Bingsheng He, Shengliang Lu, and Chiew Tong Lau. 2020. Improving\\nexecution efficiency of just-in-time compilation based query processing on GPUs.\\nProceedings of the VLDB Endowment 14, 2 (2020), 202–214.\\n[220] Johns Paul, Shengliang Lu, and Bingsheng He. 2021. Database Systems on GPUs.\\nFound. Trends Databases 11, 1 (2021), 1–108. https://doi.org/10.1561/1900000076\\n[221] Tim Pearce and Jinyeop Song. 2024. Reconciling Kaplan and Chinchilla Scaling\\nLaws. CoRR abs/2406.12907 (2024). https://doi.org/10.48550/ARXIV.2406.12907\\narXiv:2406.12907\\n[222] Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna\\nGurevych. 2021. AdapterFusion: Non-Destructive Task Composition for Transfer\\nLearning. In Proceedings of the 16th Conference of the European Chapter of the\\nAssociation for Computational Linguistics: Main Volume, EACL 2021, Online,\\nApril 19 - 23, 2021, Paola Merlo, Jörg Tiedemann, and Reut Tsarfaty (Eds.).\\nAssociation for Computational Linguistics, 487–503. https://doi.org/10.18653/\\nV1/2021.EACL-MAIN.39\\n[223] Jonathan Pilault, Mahan Fathi, Orhan Firat, Chris Pal, Pierre-Luc Bacon, and\\nRoss Goroshin. 2023. Block-State Transformers. In Advances in Neural Infor-\\nmation Processing Systems 36: Annual Conference on Neural Information Pro-\\ncessing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,\\n2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt,\\nand Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/\\n16ccd203e9e3696a7ab0dcf568316379-Abstract-Conference.html\\n[224] Mohammadreza Pourreza, Hailong Li, Ruoxi Sun, Yeounoh Chung, Shayan\\nTalaei, Gaurav Tarlok Kakkar, Yu Gan, Amin Saberi, Fatma Ozcan, and Sercan Ö.\\nArik. 2024. CHASE-SQL: Multi-Path Reasoning and Preference Optimized\\nCandidate Selection in Text-to-SQL. CoRR abs/2410.01943 (2024). https://doi.\\norg/10.48550/ARXIV.2410.01943 arXiv:2410.01943\\n[225] Tavva Prudhvith, Chakrabarty Swattik, and Selvakumar Prakash. 2024. En-\\nhancing Retrieval Augmented Generation Systems with Knowledge Graphs. In\\n2024 International Conference on Electrical, Computer and Energy Technologies\\n(ICECET. IEEE, 1–8.\\n[226] Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and\\nMao Yang. 2024. Mutual Reasoning Makes Smaller LLMs Stronger Problem-\\nSolvers. CoRR abs/2408.06195 (2024).\\nhttps://doi.org/10.48550/ARXIV.2408.\\n06195 arXiv:2408.06195\\n[227] Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize\\nChen, Cheng Yang, Zhiyuan Liu, and Maosong Sun. 2024.\\nScaling Large-\\nLanguage-Model-based Multi-Agent Collaboration. CoRR abs/2406.07155 (2024).\\nhttps://doi.org/10.48550/ARXIV.2406.07155 arXiv:2406.07155\\n[228] Yulei Qian, Fengcun Li, Xiangyang Ji, Xiaoyu Zhao, Jianchao Tan, Kefeng Zhang,\\nand Xunliang Cai. 2024. EPS-MoE: Expert Pipeline Scheduler for Cost-Efficient\\nMoE Inference. CoRR abs/2410.12247 (2024). https://doi.org/10.48550/ARXIV.\\n2410.12247 arXiv:2410.12247\\n[229] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin,\\nXin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian,\\nRuobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong\\nSun. 2024. ToolLLM: Facilitating Large Language Models to Master 16000+ Real-\\nworld APIs. In The Twelfth International Conference on Learning Representations,\\nICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.\\nnet/forum?id=dHng2O0Jjr\\n[230] Han Qiu, Jiaxing Huang, Peng Gao, Qin Qi, Xiaoqin Zhang, Ling Shao, and\\nShijian Lu. 2024.\\nLongHalQA: Long-Context Hallucination Evaluation for\\nMultiModal Large Language Models. CoRR abs/2410.09962 (2024).\\nhttps:\\n//doi.org/10.48550/ARXIV.2410.09962 arXiv:2410.09962\\n[231] Haoran Qiu, Weichao Mao, Archit Patke, Shengkun Cui, Saurabh Jha, Chen\\nWang, Hubertus Franke, Zbigniew T. Kalbarczyk, Tamer Basar, and Ravis-\\nhankar K. Iyer. 2024. Efficient Interactive LLM Serving with Proxy Model-\\nbased Sequence Length Prediction.\\nCoRR abs/2404.08509 (2024).\\nhttps:\\n//doi.org/10.48550/ARXIV.2404.08509 arXiv:2404.08509\\n[232] Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei\\nYin, Jun Xu, and Ji-Rong Wen. 2024. Towards Completeness-Oriented Tool Re-\\ntrieval for Large Language Models. In Proceedings of the 33rd ACM International\\nConference on Information and Knowledge Management, CIKM 2024, Boise, ID,\\nUSA, October 21-25, 2024, Edoardo Serra and Francesca Spezzano (Eds.). ACM,\\n1930–1940. https://doi.org/10.1145/3627673.3679847\\n[233] Ernesto Quevedo, Jorge Yero, Rachel Koerner, Pablo Rivas, and Tomás Cerný.\\n2024. Detecting Hallucinations in Large Language Model Generation: A Token\\nProbability Approach. CoRR abs/2405.19648 (2024). https://doi.org/10.48550/\\nARXIV.2405.19648 arXiv:2405.19648\\n[234] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Ste-\\nfano Ermon, and Chelsea Finn. 2023. Direct Preference Optimization: Your\\nLanguage Model is Secretly a Reward Model. In Advances in Neural Infor-\\nmation Processing Systems 36: Annual Conference on Neural Information Pro-\\ncessing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,\\n2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt,\\nand Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/\\na85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html\\n[235] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits\\nof Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn.\\nRes. 21 (2020), 140:1–140:67. https://jmlr.org/papers/v21/20-074.html\\n[236] Isaac Rehg. 2024. KV-Compress: Paged KV-Cache Compression with Variable\\nCompression Rates per Attention Head. CoRR abs/2410.00161 (2024). https:\\n//doi.org/10.48550/ARXIV.2410.00161 arXiv:2410.00161\\n[237] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen.\\n2024. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context\\nLanguage Modeling. arXiv preprint arXiv:2406.07522 (2024).\\n[238] Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar, and Stephen W.\\nKeckler. 2016. vDNN: Virtualized deep neural networks for scalable, memory-\\nefficient neural network design. In 49th Annual IEEE/ACM International Sympo-\\nsium on Microarchitecture, MICRO 2016, Taipei, Taiwan, October 15-19, 2016. IEEE\\nComputer Society, 18:1–18:13. https://doi.org/10.1109/MICRO.2016.7783721\\n[239] Guilherme Rosa, Luiz Bonifacio, Vitor Jeronymo, Hugo Abonizio, Marzieh\\nFadaee, Roberto Lotufo, and Rodrigo Nogueira. 2022.\\nIn defense of cross-\\nencoders for zero-shot retrieval. arXiv preprint arXiv:2212.06121 (2022).\\n[240] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal,\\nand Aman Chadha. 2024. A Systematic Survey of Prompt Engineering in Large\\nLanguage Models: Techniques and Applications. CoRR abs/2402.07927 (2024).\\nhttps://doi.org/10.48550/ARXIV.2402.07927 arXiv:2402.07927\\n[241] Gaurav Sahu, Pau Rodríguez, Issam H. Laradji, Parmida Atighehchian, David\\nVázquez, and Dzmitry Bahdanau. 2022. Data Augmentation for Intent Classi-\\nfication with Off-the-shelf Large Language Models. In Proceedings of the 4th\\nWorkshop on NLP for Conversational AI, ConvAI@ACL 2022, Dublin, Ireland,\\nMay 27, 2022, Bing Liu, Alexandros Papangelis, Stefan Ultes, Abhinav Rastogi,\\nYun-Nung Chen, Georgios Spithourakis, Elnaz Nouri, and Weiyan Shi (Eds.).\\nAssociation for Computational Linguistics, 47–57. https://doi.org/10.18653/V1/\\n2022.NLP4CONVAI-1.5\\n[242] Viktor Sanca and Anastasia Ailamaki. 2024. Efficient Data Access Paths for\\nMixed Vector-Relational Search. In Proceedings of the 20th International Workshop\\non Data Management on New Hardware, DaMoN 2024, Santiago, Chile, 10 June\\n2024, Carsten Binnig and Nesime Tatbul (Eds.). ACM, 6:1–6:9. https://doi.org/\\n10.1145/3662010.3663448\\n[243] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika,\\nZaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful\\nBari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla,\\nTaewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan\\nChang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin\\nYong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos\\nRozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason Alan Fries,\\nRyan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and\\nAlexander M. Rush. 2022. Multitask Prompted Training Enables Zero-Shot Task\\nGeneralization. In The Tenth International Conference on Learning Representations,\\nICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. https://openreview.\\nnet/forum?id=9Vrb9D0WI4\\n[244] Keshav Santhanam, Deepti Raghavan, Muhammad Shahir Rahman, Thejas\\nVenkatesh, Neha Kunjal, Pratiksha Thaker, Philip Alexander Levis, and Matei\\nZaharia. 2024. ALTO: An Efficient Network Orchestrator for Compound AI\\nSystems. In Proceedings of the 4th Workshop on Machine Learning and Systems,\\nEuroMLSys 2024, Athens, Greece, 22 April 2024. ACM, 117–125. https://doi.org/\\n10.1145/3642970.3655844\\n[245] Bhaskarjit Sarmah, Dhagash Mehta, Benika Hall, Rohan Rao, Sunil Patel, and\\nStefano Pasquali. 2024. HybridRAG: Integrating Knowledge Graphs and Vec-\\ntor Retrieval Augmented Generation for Efficient Information Extraction. In\\nProceedings of the 5th ACM International Conference on AI in Finance. 608–616.\\n[246] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli,\\nEric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.\\nToolformer: Language Models Can Teach Themselves to Use Tools. In Advances\\nin Neural Information Processing Systems 36: Annual Conference on Neural Infor-\\nmation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December\\n10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz\\nHardt, and Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/\\nhash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html\\n[247] Sander Schulhoff, Michael Ilie, Nishant Balepur, Konstantine Kahadze, Amanda\\nLiu, Chenglei Si, Yinheng Li, Aayush Gupta, HyoJung Han, Sevien Schulhoff,\\nPranav Sandeep Dulepet, Saurav Vidyadhara, Dayeon Ki, Sweta Agrawal, Chau\\nPham, Gerson C. Kroiz, Feileen Li, Hudson Tao, Ashay Srivastava, Hevan-\\nder Da Costa, Saloni Gupta, Megan L. Rogers, Inna Goncearenco, Giuseppe\\nSarli, Igor Galynker, Denis Peskoff, Marine Carpuat, Jules White, Shyamal Anad-\\nkat, Alexander Miserlis Hoyle, and Philip Resnik. 2024. The Prompt Report:\\nA Systematic Survey of Prompting Techniques. CoRR abs/2406.06608 (2024).\\nhttps://doi.org/10.48550/ARXIV.2406.06608 arXiv:2406.06608\\n[248] Robert Schulze, Tom Schreiber, Ilya Yatsishin, Ryadh Dahimene, and Alexey\\nMilovidov. 2024. ClickHouse-Lightning Fast Analytics for Everyone. Proceedings\\nof the VLDB Endowment 17, 12 (2024), 3731–3744.\\n[249] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and\\nTri Dao. 2024. FlashAttention-3: Fast and Accurate Attention with Asynchrony\\nand Low-precision. CoRR abs/2407.08608 (2024).\\nhttps://doi.org/10.48550/\\nARXIV.2407.08608 arXiv:2407.08608\\n[250] Lior Shani, Aviv Rosenberg, Asaf B. Cassel, Oran Lang, Daniele Calandriello,\\nAvital Zipori, Hila Noga, Orgad Keller, Bilal Piot, Idan Szpektor, Avinatan\\nHassidim, Yossi Matias, and Rémi Munos. 2024. Multi-turn Reinforcement\\nLearning from Preference Human Feedback.\\nCoRR abs/2405.14655 (2024).\\nhttps://doi.org/10.48550/ARXIV.2405.14655 arXiv:2405.14655\\n[251] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li,\\nKaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. 2024. OmniQuant: Omnidirec-\\ntionally Calibrated Quantization for Large Language Models. In The Twelfth Inter-\\nnational Conference on Learning Representations, ICLR 2024, Vienna, Austria, May\\n7-11, 2024. OpenReview.net. https://openreview.net/forum?id=8Wuvhh0LYW\\n[252] Ying Sheng, Shiyi Cao, Dacheng Li, Banghua Zhu, Zhuohan Li, Danyang\\nZhuo, Joseph E. Gonzalez, and Ion Stoica. 2024. Fairness in Serving Large\\nLanguage Models. In 18th USENIX Symposium on Operating Systems Design\\nand Implementation, OSDI 2024, Santa Clara, CA, USA, July 10-12, 2024, Ada\\nGavrilovska and Douglas B. Terry (Eds.). USENIX Association, 965–988. https:\\n//www.usenix.org/conference/osdi24/presentation/sheng\\n[253] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi\\nChen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. 2023. FlexGen:\\nHigh-Throughput Generative Inference of Large Language Models with a Single\\nGPU. In International Conference on Machine Learning, ICML 2023, 23-29 July\\n2023, Honolulu, Hawaii, USA (Proceedings of Machine Learning Research, Vol. 202),\\nAndreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan\\nSabato, and Jonathan Scarlett (Eds.). PMLR, 31094–31116. https://proceedings.\\nmlr.press/v202/sheng23a.html\\n[254] Nikhil Sheoran, Supawit Chockchowwat, Arav Chheda, Suwen Wang, Riya\\nVerma, and Yongjoo Park. 2023. A Step Toward Deep Online Aggregation. Proc.\\nACM Manag. Data 1, 2 (2023), 124:1–124:28. https://doi.org/10.1145/3589269\\n[255] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer\\nSingh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with\\nAutomatically Generated Prompts. In Proceedings of the 2020 Conference on\\nEmpirical Methods in Natural Language Processing, EMNLP 2020, Online, Novem-\\nber 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.).\\nAssociation for Computational Linguistics, 4222–4235. https://doi.org/10.18653/\\nV1/2020.EMNLP-MAIN.346\\n[256] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan,\\nand Shunyu Yao. 2023.\\nReflexion: language agents with verbal rein-\\nforcement learning. In Advances in Neural Information Processing Sys-\\ntems 36: Annual Conference on Neural Information Processing Systems 2023,\\nNeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, Alice\\nOh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and\\nSergey Levine (Eds.).\\nhttp://papers.nips.cc/paper_files/paper/2023/hash/\\n1b44b878bb782e6954cd888628510e90-Abstract-Conference.html\\n[257] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared\\nCasper, and Bryan Catanzaro. 2019.\\nMegatron-LM: Training Multi-Billion\\nParameter Language Models Using Model Parallelism. CoRR abs/1909.08053\\n(2019). arXiv:1909.08053 http://arxiv.org/abs/1909.08053\\n[258] Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jor-\\ndan L. Boyd-Graber, and Lijuan Wang. 2023. Prompting GPT-3 To Be Reliable.\\nIn The Eleventh International Conference on Learning Representations, ICLR 2023,\\nKigali, Rwanda, May 1-5, 2023. OpenReview.net. https://openreview.net/forum?\\nid=98p5x51L5af\\n[259] Tomer Simon. 2024. The scientist of the scientist. AI Soc. 39, 2 (2024), 803–804.\\nhttps://doi.org/10.1007/S00146-022-01544-6\\n[260] Panagiotis Sioulas, Viktor Sanca, Ioannis Mytilinis, and Anastasia Ailamaki.\\n2021. Accelerating Complex Analytics using Speculation. In 11th Conference\\non Innovative Data Systems Research, CIDR 2021, Virtual Event, January 11-15,\\n2021, Online Proceedings. www.cidrdb.org. http://cidrdb.org/cidr2021/papers/\\ncidr2021_paper03.pdf\\n[261] Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen. 2024. PowerInfer: Fast\\nLarge Language Model Serving with a Consumer-grade GPU. In Proceedings\\nof the ACM SIGOPS 30th Symposium on Operating Systems Principles, SOSP\\n2024, Austin, TX, USA, November 4-6, 2024, Emmett Witchel, Christopher J.\\nRossbach, Andrea C. Arpaci-Dusseau, and Kimberly Keeton (Eds.). ACM, 590–\\n606. https://doi.org/10.1145/3694715.3695964\\n[262] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe,\\nChelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. 2020.\\nLearning to summarize with human feedback. In Advances in Neural In-\\nformation Processing Systems 33: Annual Conference on Neural Information\\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo\\nLarochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and\\nHsuan-Tien Lin (Eds.).\\nhttps://proceedings.neurips.cc/paper/2020/hash/\\n1f89885d556929e98d3ef9b86448f951-Abstract.html\\n[263] Foteini Strati, Sara McAllister, Amar Phanishayee, Jakub Tarnawski, and Ana\\nKlimovic. 2024. DéjàVu: KV-cache Streaming for Fast, Fault-tolerant Generative\\nLLM Serving. In Forty-first International Conference on Machine Learning, ICML\\n2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. https://openreview.\\nnet/forum?id=AbGbGZFYOD\\n[264] Weihang Su, Changyue Wang, Qingyao Ai, Yiran Hu, Zhijing Wu, Yujia Zhou,\\nand Yiqun Liu. 2024. Unsupervised Real-Time Hallucination Detection based\\non the Internal States of Large Language Models. In Findings of the Association\\nfor Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting,\\nAugust 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.).\\nAssociation for Computational Linguistics, 14379–14391. https://doi.org/10.\\n18653/V1/2024.FINDINGS-ACL.854\\n[265] Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. 2024.\\nTable meets llm: Can large language models understand structured table data?\\na benchmark and empirical study. In Proceedings of the 17th ACM International\\nConference on Web Search and Data Mining. 645–654.\\n[266] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin,\\nYeyun Gong, Heung-Yeung Shum, and Jian Guo. 2023.\\nThink-on-Graph:\\nDeep and Responsible Reasoning of Large Language Model with Knowledge\\nGraph. CoRR abs/2307.07697 (2023). https://doi.org/10.48550/ARXIV.2307.07697\\narXiv:2307.07697\\n[267] Xin Tan, Yimin Jiang, Yitao Yang, and Hong Xu. 2024. Teola: Towards End-\\nto-End Optimization of LLM-based Applications. CoRR abs/2407.00326 (2024).\\nhttps://doi.org/10.48550/ARXIV.2407.00326 arXiv:2407.00326\\n[268] James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel,\\nand Alon Y. Levy. 2021. From Natural Language Processing to Neural Databases.\\nProc. VLDB Endow. 14, 6 (2021), 1033–1039. https://doi.org/10.14778/3447689.\\n3447706\\n[269] Bing Tian, Haikun Liu, Yuhang Tang, Shihai Xiao, Zhuohui Duan, Xiaofei Liao,\\nXuecang Zhang, Junhua Zhu, and Yu Zhang. 2024. FusionANNS: An Efficient\\nCPU/GPU Cooperative Processing Architecture for Billion-scale Approximate\\nNearest Neighbor Search. CoRR abs/2409.16576 (2024). https://doi.org/10.48550/\\nARXIV.2409.16576 arXiv:2409.16576\\n[270] S. M. Towhidul Islam Tonmoy, S. M. Mehedi Zaman, Vinija Jain, Anku\\nRani, Vipula Rawte, Aman Chadha, and Amitava Das. 2024. A Comprehen-\\nsive Survey of Hallucination Mitigation Techniques in Large Language Mod-\\nels. CoRR abs/2401.01313 (2024). https://doi.org/10.48550/ARXIV.2401.01313\\narXiv:2401.01313\\n[271] Immanuel Trummer. 2022. DB-BERT: A Database Tuning Tool that \"Reads\\nthe Manual\". In SIGMOD ’22: International Conference on Management of Data,\\nPhiladelphia, PA, USA, June 12 - 17, 2022, Zachary G. Ives, Angela Bonifati, and\\nAmr El Abbadi (Eds.). ACM, 190–203. https://doi.org/10.1145/3514221.3517843\\n[272] Immanuel Trummer. 2023. From BERT to GPT-3 Codex: Harnessing the Potential\\nof Very Large Language Models for Data Management. CoRR abs/2306.09339\\n(2023). https://doi.org/10.48550/ARXIV.2306.09339 arXiv:2306.09339\\n[273] Matthias Urban and Carsten Binnig. 2024. CAESURA: Language Models as\\nMulti-Modal Query Planners. In 14th Conference on Innovative Data Systems\\nResearch, CIDR 2024, Chaminade, HI, USA, January 14-17, 2024. www.cidrdb.org.\\nhttps://www.cidrdb.org/cidr2024/papers/p14-urban.pdf\\n[274] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.\\nAttention is\\nAll you Need. In Advances in Neural Information Processing Systems 30: An-\\nnual Conference on Neural Information Processing Systems 2017, December 4-\\n9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy\\nBengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman\\nGarnett (Eds.). 5998–6008.\\nhttps://proceedings.neurips.cc/paper/2017/hash/\\n3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\\n[275] Shubham Vatsal and Harsh Dubey. 2024. A Survey of Prompt Engineering Meth-\\nods in Large Language Models for Different NLP Tasks. CoRR abs/2407.12994\\n(2024). https://doi.org/10.48550/ARXIV.2407.12994 arXiv:2407.12994\\n[276] Jonas Waldendorf, Barry Haddow, and Alexandra Birch. 2024. Contrastive\\nDecoding Reduces Hallucinations in Large Multilingual Machine Translation\\nModels. In Proceedings of the 18th Conference of the European Chapter of the\\nAssociation for Computational Linguistics, EACL 2024 - Volume 1: Long Papers, St.\\nJulian’s, Malta, March 17-22, 2024, Yvette Graham and Matthew Purver (Eds.).\\nAssociation for Computational Linguistics, 2526–2539. https://aclanthology.\\norg/2024.eacl-long.155\\n[277] Fei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen, and Sercan Ö. Arik. 2024.\\nAstute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\\nConflicts for Large Language Models. CoRR abs/2410.07176 (2024).\\nhttps:\\n//doi.org/10.48550/ARXIV.2410.07176 arXiv:2410.07176\\n[278] Jinyuan Wang, Junlong Li, and Hai Zhao. 2023. Self-prompted chain-of-thought\\non large language models for open-domain multi-hop reasoning. arXiv preprint\\narXiv:2310.13552 (2023).\\n[279] Ke Wang, Jiahui Zhu, Minjie Ren, Zeming Liu, Shiwei Li, Zongye Zhang,\\nChenkai Zhang, Xiaoyu Wu, Qiqi Zhan, Qingjie Liu, and Yunhong Wang.\\n2024. A Survey on Data Synthesis and Augmentation for Large Language Mod-\\nels. CoRR abs/2410.12896 (2024). https://doi.org/10.48550/ARXIV.2410.12896\\narXiv:2410.12896\\n[280] Wenyi Wang, Hisham A Alyahya, Dylan R Ashley, Oleg Serikov, Dmitrii\\nKhizbullin, Francesco Faccio, and Jürgen Schmidhuber. 2024. How to Cor-\\nrectly do Semantic Backpropagation on Language-based Agentic Systems. arXiv\\npreprint arXiv:2412.03624 (2024).\\n[281] Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and\\nFuru Wei. 2023. Augmenting Language Models with Long-Term Memory. In Ad-\\nvances in Neural Information Processing Systems 36: Annual Conference on Neural\\nInformation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, De-\\ncember 10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko,\\nMoritz Hardt, and Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/\\n2023/hash/ebd82705f44793b6f9ade5a669d0f0bf-Abstract-Conference.html\\n[282] Xiaochen Wang, Junqing He, Liang Chen, Reza Haf Zhe Yang, Yiru Wang,\\nXiangdi Meng, Kunhao Pan, and Zhifang Sui. 2024. SG-FSM: A Self-Guiding\\nZero-Shot Prompting Paradigm for Multi-Hop Question Answering Based on\\nFinite State Machine. arXiv preprint arXiv:2410.17021 (2024).\\n[283] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan\\nNarang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-Consistency Im-\\nproves Chain of Thought Reasoning in Language Models. In The Eleventh Inter-\\nnational Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May\\n1-5, 2023. OpenReview.net. https://openreview.net/forum?id=1PL1NIMMrw\\n[284] Xuezhi Wang and Denny Zhou. 2024. Chain-of-Thought Reasoning Without\\nPrompting. CoRR abs/2402.10200 (2024). https://doi.org/10.48550/ARXIV.2402.\\n10200 arXiv:2402.10200\\n[285] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith,\\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-Instruct: Aligning Lan-\\nguage Models with Self-Generated Instructions. In Proceedings of the 61st Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\nACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber,\\nand Naoaki Okazaki (Eds.). Association for Computational Linguistics, 13484–\\n13508. https://doi.org/10.18653/V1/2023.ACL-LONG.754\\n[286] Zhiruo Wang, Zhoujun Cheng, Hao Zhu, Daniel Fried, and Graham Neubig.\\n2024. What Are Tools Anyway? A Survey from the Language Model Perspec-\\ntive. CoRR abs/2403.15452 (2024). https://doi.org/10.48550/ARXIV.2403.15452\\narXiv:2403.15452\\n[287] Zheng Wang, Shu Xian Teo, Jieer Ouyang, Yongjun Xu, and Wei Shi. 2024.\\nM-RAG: Reinforcing Large Language Model Performance through Retrieval-\\nAugmented Generation with Multiple Partitions. In Proceedings of the 62nd\\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long\\nPapers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, Lun-Wei Ku, Andre\\nMartins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,\\n1966–1978. https://doi.org/10.18653/V1/2024.ACL-LONG.108\\n[288] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian\\nLester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022. Finetuned Language\\nModels are Zero-Shot Learners. In The Tenth International Conference on Learning\\nRepresentations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\\nhttps://openreview.net/forum?id=gEZrGCozdqR\\n[289] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter,\\nFei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought\\nPrompting Elicits Reasoning in Large Language Models. In Advances in Neural\\nInformation Processing Systems 35: Annual Conference on Neural Information\\nProcessing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 -\\nDecember 9, 2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave,\\nK. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper_files/paper/2022/hash/\\n9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html\\n[290] Steven Whang and Jae-Gil Lee. 2020. Data Collection and Quality Challenges\\nfor Deep Learning. Proc. VLDB Endow. 13, 12 (2020), 3429–3432. https://doi.org/\\n10.14778/3415478.3415562\\n[291] Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang, Xuanzhe Liu, and\\nXin Jin. 2023. Fast Distributed Inference Serving for Large Language Mod-\\nels. CoRR abs/2305.05920 (2023). https://doi.org/10.48550/ARXIV.2305.05920\\narXiv:2305.05920\\n[292] Xun Wu, Shaohan Huang, and Furu Wei. 2024. Mixture of LoRA Experts. In The\\nTwelfth International Conference on Learning Representations, ICLR 2024, Vienna,\\nAustria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?id=\\nuWvKBCYh4S\\n[293] Yingjun Wu, Chee Yong Chan, and Kian-Lee Tan. 2016. Transaction Healing:\\nScaling Optimistic Concurrency Control on Multicores. In Proceedings of the 2016\\nInternational Conference on Management of Data, SIGMOD Conference 2016, San\\nFrancisco, CA, USA, June 26 - July 01, 2016, Fatma Özcan, Georgia Koutrika, and\\nSam Madden (Eds.). ACM, 1689–1704. https://doi.org/10.1145/2882903.2915202\\n[294] Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy.\\n2022. Memorizing Transformers. In The Tenth International Conference on Learn-\\ning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\\nhttps://openreview.net/forum?id=TrjbxzRcnf-\\n[295] Guangxuan Xiao, Ji Lin, Mickaël Seznec, Hao Wu, Julien Demouth, and Song\\nHan. 2023. SmoothQuant: Accurate and Efficient Post-Training Quantization for\\nLarge Language Models. In International Conference on Machine Learning, ICML\\n2023, 23-29 July 2023, Honolulu, Hawaii, USA (Proceedings of Machine Learning\\nResearch, Vol. 202), Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara\\nEngelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 38087–38099.\\nhttps://proceedings.mlr.press/v202/xiao23c.html\\n[296] Junjie Xing, Yeye He, Mengyu Zhou, Haoyu Dong, Shi Han, Dongmei Zhang,\\nand Surajit Chaudhuri. 2024. Table-LLM-Specialist: Language Model Specialists\\nfor Tables using Iterative Generator-Validator Fine-tuning. CoRR abs/2410.12164\\n(2024). https://doi.org/10.48550/ARXIV.2410.12164 arXiv:2410.12164\\n[297] Yizhe Xiong, Xiansheng Chen, Xin Ye, Hui Chen, Zijia Lin, Haoran Lian, Jianwei\\nNiu, and Guiguang Ding. 2024. Temporal Scaling Law for Large Language\\nModels. CoRR abs/2404.17785 (2024).\\nhttps://doi.org/10.48550/ARXIV.2404.\\n17785 arXiv:2404.17785\\n[298] Jiale Xu, Rui Zhang, Cong Guo, Weiming Hu, Zihan Liu, Feiyang Wu, Yu Feng,\\nShixuan Sun, Changxu Shao, Yuhong Guo, Junping Zhao, Ke Zhang, Minyi\\nGuo, and Jingwen Leng. 2024. vTensor: Flexible Virtual Tensor Management for\\nEfficient LLM Serving. CoRR abs/2407.15309 (2024). https://doi.org/10.48550/\\nARXIV.2407.15309 arXiv:2407.15309\\n[299] Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer,\\nSee-Kiong Ng, and Jiashi Feng. 2024. MAgIC: Investigation of Large Language\\nModel Powered Multi-Agent in Cognition, Adaptability, Rationality and Collab-\\noration. In Proceedings of the 2024 Conference on Empirical Methods in Natural\\nLanguage Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, Yaser\\nAl-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Compu-\\ntational Linguistics, 7315–7332. https://aclanthology.org/2024.emnlp-main.416\\n[300] Qiancheng Xu, Yongqi Li, Heming Xia, and Wenjie Li. 2024. Enhancing Tool\\nRetrieval with Iterative Feedback from Large Language Models. In Findings\\nof the Association for Computational Linguistics: EMNLP 2024, Miami, Florida,\\nUSA, November 12-16, 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung\\nChen (Eds.). Association for Computational Linguistics, 9609–9619.\\nhttps:\\n//aclanthology.org/2024.findings-emnlp.561\\n[301] Sheng Xu, Mike Chen, and Shuwen Chen. 2024. Enhancing Retrieval-Augmented\\nGeneration Models with Knowledge Graphs: Innovative Practices Through a\\nDual-Pathway Approach. In International Conference on Intelligent Computing.\\nSpringer, 398–409.\\n[302] Weijia Xu, Andrzej Banburski, and Nebojsa Jojic. 2024. Reprompting: Automated\\nChain-of-Thought Prompt Inference Through Gibbs Sampling. In Forty-first\\nInternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July\\n21-27, 2024. OpenReview.net. https://openreview.net/forum?id=D8zn1DnTuj\\n[303] Yuming Xu, Hengyu Liang, Jin Li, Shuotao Xu, Qi Chen, Qianxi Zhang, Cheng Li,\\nZiyue Yang, Fan Yang, Yuqing Yang, Peng Cheng, and Mao Yang. 2023. SPFresh:\\nIncremental In-Place Update for Billion-Scale Vector Search. In Proceedings\\nof the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz,\\nGermany, October 23-26, 2023, Jason Flinn, Margo I. Seltzer, Peter Druschel,\\nAntoine Kaufmann, and Jonathan Mace (Eds.). ACM, 545–561. https://doi.org/\\n10.1145/3600006.3613166\\n[304] Yi Xu, Ziming Mao, Xiangxi Mo, Shu Liu, and Ion Stoica. 2024. Pie: Pooling\\nCPU Memory for LLM Inference. arXiv preprint arXiv:2411.09317 (2024).\\n[305] Ziwei Xu, Sanjay Jain, and Mohan S. Kankanhalli. 2024. Hallucination is In-\\nevitable: An Innate Limitation of Large Language Models. CoRR abs/2401.11817\\n(2024). https://doi.org/10.48550/ARXIV.2401.11817 arXiv:2401.11817\\n[306] Zifei Xu, Alexander Lan, Wanzin Yazar, Tristan Webb, Sayeh Sharify, and Xin\\nWang. 2024. Scaling laws for post-training quantized large language mod-\\nels. CoRR abs/2410.12119 (2024). https://doi.org/10.48550/ARXIV.2410.12119\\narXiv:2410.12119\\n[307] Maryann Xue, Yingyi Bu, Abhishek Somani, Wenchen Fan, Ziqi Liu, Steven\\nChen, Herman Van Hovell, Bart Samwel, Mostafa Mokhtar, Rk Korlapati, Andy\\nLam, Yunxiao Ma, Vuk Ercegovac, Jiexing Li, Alexander Behm, Yuanjian Li,\\nXiao Li, Sriram Krishnamurthy, Amit Shukla, Michalis Petropoulos, Sameer\\nParanjpye, Reynold Xin, and Matei Zaharia. 2024. Adaptive and Robust Query\\nExecution for Lakehouses At Scale. Proc. VLDB Endow. 17, 12 (2024), 3947–3959.\\nhttps://www.vldb.org/pvldb/vol17/p3947-bu.pdf\\n[308] Scott Yak, Yihe Dong, Javier Gonzalvo, and Sercan Arik. 2023. IngesTables:\\nScalable and Efficient Training of LLM-Enabled Tabular Foundation Models. In\\nNeurIPS 2023 Second Table Representation Learning Workshop.\\n[309] Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sa-\\njal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun\\nKong, Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Et-\\ning Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Ni-\\nrav Shah, Rakesh Wanga, Anuj Kumar, Wen-tau Yih, and Xin Luna Dong.\\n2024. CRAG - Comprehensive RAG Benchmark. CoRR abs/2406.04744 (2024).\\nhttps://doi.org/10.48550/ARXIV.2406.04744 arXiv:2406.04744\\n[310] Yifei Yang, Xiangyao Yu, Marco Serafini, Ashraf Aboulnaga, and Michael Stone-\\nbraker. 2024. FlexpushdownDB: rethinking computation pushdown for cloud\\nOLAP DBMSs. VLDB J. 33, 5 (2024), 1643–1670. https://doi.org/10.1007/S00778-\\n024-00867-8\\n[311] Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang,\\nKuntai Du, Shan Lu, and Junchen Jiang. 2024. CacheBlend: Fast Large Language\\nModel Serving for RAG with Cached Knowledge Fusion. CoRR abs/2405.16444\\n(2024). https://doi.org/10.48550/ARXIV.2405.16444 arXiv:2405.16444\\n[312] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao,\\nand Karthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solv-\\ning with Large Language Models. In Advances in Neural Information Pro-\\ncessing Systems 36: Annual Conference on Neural Information Processing Sys-\\ntems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023,\\nAlice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt,\\nand Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/\\n271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html\\n[313] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R.\\nNarasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Act-\\ning in Language Models. In The Eleventh International Conference on Learning\\nRepresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\\nhttps://openreview.net/forum?id=WE_vluYUL-X\\n[314] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Richard James, Jure\\nLeskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-Tau Yih. 2023.\\nRetrieval-Augmented Multimodal Language Modeling. In International Con-\\nference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii,\\nUSA (Proceedings of Machine Learning Research, Vol. 202), Andreas Krause,\\nEmma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and\\nJonathan Scarlett (Eds.). PMLR, 39755–39769. https://proceedings.mlr.press/\\nv202/yasunaga23a.html\\n[315] Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, and Furu Wei.\\n2024. Differential transformer. arXiv preprint arXiv:2410.05258 (2024).\\n[316] Zihao Ye, Ruihang Lai, Bo-Ru Lu, Chien-Yu Lin, Size Zheng, Lequn Chen, Tianqi\\nChen, and Luis Ceze. 2024. Cascade inference: Memory bandwidth efficient\\nshared prefix batch decoding.\\n[317] Chengye Yu, Tianyu Wang, Zili Shao, Linjie Zhu, Xu Zhou, and Song Jiang.\\n2024. Twinpilots: A new computing paradigm for gpu-cpu parallel llm inference.\\nIn Proceedings of the 17th ACM International Systems and Storage Conference.\\n91–103.\\n[318] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-\\nGon Chun. 2022. Orca: A Distributed Serving System for Transformer-Based\\nGenerative Models. In 16th USENIX Symposium on Operating Systems Design\\nand Implementation, OSDI 2022, Carlsbad, CA, USA, July 11-13, 2022, Marcos K.\\nAguilera and Hakim Weatherspoon (Eds.). USENIX Association, 521–538. https:\\n//www.usenix.org/conference/osdi22/presentation/yu\\n[319] Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, and Jiajun Chen. 2023. Towards\\nBetter Chain-of-Thought Prompting Strategies: A Survey. CoRR abs/2310.04959\\n(2023). https://doi.org/10.48550/ARXIV.2310.04959 arXiv:2310.04959\\n[320] Ruize Yuan, Xiang Ao, Li Zeng, and Qing He. 2024. DRAMA: Dynamic Multi-\\nGranularity Graph Estimate Retrieval over Tabular and Textual Question An-\\nswering. In Proceedings of the 2024 Joint International Conference on Computa-\\ntional Linguistics, Language Resources and Evaluation, LREC/COLING 2024, 20-25\\nMay, 2024, Torino, Italy, Nicoletta Calzolari, Min-Yen Kan, Véronique Hoste,\\nAlessandro Lenci, Sakriani Sakti, and Nianwen Xue (Eds.). ELRA and ICCL,\\n5365–5375. https://aclanthology.org/2024.lrec-main.477\\n[321] Ye Yuan, Bo Tang, Tianfei Zhou, Zhiwei Zhang, and Jianbin Qin. 2024. nsDB:\\nArchitecting the Next Generation Database by Integrating Neural and Symbolic\\nSystems (Vision). Proc. VLDB Endow. 17, 11 (2024), 3283–3289. https://www.\\nvldb.org/pvldb/vol17/p3283-tang.pdf\\n[322] Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao\\nXue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, Beidi Chen,\\nGuangyu Sun, and Kurt Keutzer. 2024. LLM Inference Unveiled: Survey and\\nRoofline Model Insights. CoRR abs/2402.16363 (2024). https://doi.org/10.48550/\\nARXIV.2402.16363 arXiv:2402.16363\\n[323] Matei Zaharia, Omar Khattab, Lingjiao Chen, Jared Quincy Davis, Heather\\nMiller, Chris Potts, James Zou, Michael Carbin, Jonathan Frankle, Naveen Rao,\\nand Ali Ghodsi. 2024. The Shift from Models to Compound AI Systems. https:\\n//bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/.\\n[324] Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui\\nHuang, Shufei Zhang, Marco Pavone, Yuqiang Li, Wanli Ouyang, and Dongzhan\\nZhou. 2024. LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level\\nMathematical Reasoning. CoRR abs/2410.02884 (2024). https://doi.org/10.48550/\\nARXIV.2410.02884 arXiv:2410.02884\\n[325] Kai Zhang, Liqian Peng, Congchao Wang, Alec Go, and Xiaozhong Liu. 2024.\\nLLM Cascade with Multi-Objective Optimal Consideration. CoRR abs/2410.08014\\n(2024). https://doi.org/10.48550/ARXIV.2410.08014 arXiv:2410.08014\\n[326] Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. 2023.\\nRetrieve Anything To Augment Large Language Models. CoRR abs/2310.07554\\n(2023). https://doi.org/10.48550/ARXIV.2310.07554 arXiv:2310.07554\\n[327] Qianxi Zhang, Shuotao Xu, Qi Chen, Guoxin Sui, Jiadong Xie, Zhizhen Cai,\\nYaoqi Chen, Yinxuan He, Yuqing Yang, Fan Yang, Mao Yang, and Lidong Zhou.\\n2023. VBASE: Unifying Online Vector Similarity Search and Relational Queries\\nvia Relaxed Monotonicity. In 17th USENIX Symposium on Operating Systems\\nDesign and Implementation, OSDI 2023, Boston, MA, USA, July 10-12, 2023, Roxana\\nGeambasu and Ed Nightingale (Eds.). USENIX Association, 377–395.\\nhttps:\\n//www.usenix.org/conference/osdi23/presentation/zhang-qianxi\\n[328] Shuo Zhang, Zezhou Huang, and Eugene Wu. 2024. Data Cleaning Using Large\\nLanguage Models. arXiv preprint arXiv:2410.15547 (2024).\\n[329] Xiaoming Zhang, Ming Wang, Xiaocui Yang, Daling Wang, Shi Feng, and Yifei\\nZhang. 2024. Hierarchical Retrieval-Augmented Generation Model with Rethink\\nfor Multi-hop Question Answering. arXiv preprint arXiv:2408.11875 (2024).\\n[330] Yue Zhang, Hongliang Fei, Dingcheng Li, and Ping Li. 2022. PromptGen: Au-\\ntomatically Generate Prompts using Generative Models. In Findings of the\\nAssociation for Computational Linguistics: NAACL 2022, Seattle, WA, United\\nStates, July 10-15, 2022, Marine Carpuat, Marie-Catherine de Marneffe, and Iván\\nVladimir Meza Ruíz (Eds.). Association for Computational Linguistics, 30–37.\\nhttps://doi.org/10.18653/V1/2022.FINDINGS-NAACL.3\\n[331] Zihan Zhang, Meng Fang, and Ling Chen. 2024. RetrievalQA: Assessing Adap-\\ntive Retrieval-Augmented Generation for Short-form Open-Domain Question\\nAnswering. In Findings of the Association for Computational Linguistics, ACL 2024,\\nBangkok, Thailand and virtual meeting, August 11-16, 2024, Lun-Wei Ku, Andre\\nMartins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,\\n6963–6975. https://doi.org/10.18653/V1/2024.FINDINGS-ACL.415\\n[332] Ziqi Zhang, Cunxiang Wang, Xiao Xiong, Yue Zhang, and Donglin Wang. 2024.\\nNash CoT: Multi-Path Inference with Preference Equilibrium. In Proceedings\\nof the 2024 Conference on Empirical Methods in Natural Language Processing,\\nEMNLP 2024, Miami, FL, USA, November 12-16, 2024, Yaser Al-Onaizan, Mohit\\nBansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics,\\n14572–14587. https://aclanthology.org/2024.emnlp-main.807\\n[333] Xuanlei Zhao, Bin Jia, Haotian Zhou, Ziming Liu, Shenggan Cheng, and Yang\\nYou. 2024. HeteGen: Efficient Heterogeneous Parallel Inference for Large Lan-\\nguage Models on Resource-Constrained Devices. In Proceedings of the Seventh\\nAnnual Conference on Machine Learning and Systems, MLSys 2024, Santa Clara,\\nCA, USA, May 13-16, 2024, Phillip B. Gibbons, Gennady Pekhimenko, and Christo-\\npher De Sa (Eds.). mlsys.org. https://proceedings.mlsys.org/paper_files/paper/\\n2024/hash/5431dca75a8d2abc1fb51e89e8324f10-Abstract-Conference.html\\n[334] Yilong Zhao, Shuo Yang, Kan Zhu, Lianmin Zheng, Baris Kasikci, Yang Zhou,\\nJiarong Xing, and Ion Stoica. 2024. BlendServe: Optimizing Offline Inference for\\nAuto-regressive Large Models with Resource-aware Batching. arXiv preprint\\narXiv:2411.16102 (2024).\\n[335] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun,\\nCody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez,\\nClark W. Barrett, and Ying Sheng. 2023. Efficiently Programming Large Lan-\\nguage Models using SGLang. CoRR abs/2312.07104 (2023). https://doi.org/10.\\n48550/ARXIV.2312.07104 arXiv:2312.07104\\n[336] Wenhao Zheng, Yixiao Chen, Weitong Zhang, Souvik Kundu, Yun Li,\\nZhengzhong Liu, Eric P Xing, Hongyi Wang, and Huaxiu Yao. 2024. CITER:\\nCollaborative Inference for Efficient Large Language Model Decoding with\\nToken-Level Routing. In Adaptive Foundation Models: Evolving AI for Personal-\\nized and Efficient Learning.\\n[337] Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang, and Yang\\nYou. 2023. Response Length Perception and Sequence Scheduling: An LLM-\\nEmpowered LLM Inference Pipeline. In Advances in Neural Information Pro-\\ncessing Systems 36: Annual Conference on Neural Information Processing Sys-\\ntems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023,\\nAlice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt,\\nand Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/\\nce7ff3405c782f761fac7f849b41ae9a-Abstract-Conference.html\\n[338] Han Zhong, Guhao Feng, Wei Xiong, Li Zhao, Di He, Jiang Bian, and Li-\\nwei Wang. 2024.\\nDPO Meets PPO: Reinforced Token Optimization for\\nRLHF. CoRR abs/2404.18922 (2024). https://doi.org/10.48550/ARXIV.2404.18922\\narXiv:2404.18922\\n[339] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu,\\nXin Jin, and Hao Zhang. 2024. DistServe: Disaggregating Prefill and Decoding for\\nGoodput-optimized Large Language Model Serving. In 18th USENIX Symposium\\non Operating Systems Design and Implementation, OSDI 2024, Santa Clara, CA,\\nUSA, July 10-12, 2024, Ada Gavrilovska and Douglas B. Terry (Eds.). USENIX\\nAssociation, 193–210. https://www.usenix.org/conference/osdi24/presentation/\\nzhong-yinmin\\n[340] Xuanhe Zhou, Guoliang Li, and Zhiyuan Liu. 2023.\\nLLM As DBA.\\nCoRR abs/2308.05481 (2023).\\nhttps://doi.org/10.48550/ARXIV.2308.05481\\narXiv:2308.05481\\n[341] Yue Zhou, Chenlu Guo, Xu Wang, Yi Chang, and Yuan Wu. 2024. A Survey on\\nData Augmentation in Large Model Era. CoRR abs/2401.15422 (2024). https:\\n//doi.org/10.48550/ARXIV.2401.15422 arXiv:2401.15422\\n[342] Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming\\nLou, Luning Wang, Zhihang Yuan, Xiuhong Li, Shengen Yan, Guohao Dai,\\nXiao-Ping Zhang, Yuhan Dong, and Yu Wang. 2024. A Survey on Efficient\\nInference for Large Language Models. CoRR abs/2404.14294 (2024).\\nhttps:\\n//doi.org/10.48550/ARXIV.2404.14294 arXiv:2404.14294\\n[343] Kan Zhu, Yilong Zhao, Liangyu Zhao, Gefei Zuo, Yile Gu, Dedong Xie, Yufei\\nGao, Qinyu Xu, Tian Tang, Zihao Ye, et al. 2024. NanoFlow: Towards Optimal\\nLarge Language Model Serving Throughput. arXiv preprint arXiv:2408.12757\\n(2024).\\n',\n",
       " 'type': 'Document'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be2a4bd",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b22f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chunks = []\n",
    "chunk_by_doc = {}\n",
    "for doc in docs:\n",
    "    doc_chunks = []\n",
    "    for i, chunk in enumerate(text_splitter.split_text(doc.page_content)):\n",
    "        metadata = doc.metadata.copy()\n",
    "        metadata[\"chunk_index\"] = i\n",
    "        doc_chunks.append(Document(page_content=chunk, metadata=metadata))\n",
    "    chunk_by_doc[doc.metadata.get(\"Title\", \"Document\")] = (\n",
    "        doc_chunks\n",
    "    )\n",
    "    final_chunks.extend(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe76230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk Stats : model_id defined splitter ---\n",
      "Total chunks: 1697\n",
      "Avg size: 293.11, Max: 302, Min: 48\n"
     ]
    }
   ],
   "source": [
    "chunk_lengths = [\n",
    "    len(tokenizer.encode(chunk.page_content, truncation=False))\n",
    "    for chunk in final_chunks\n",
    "]\n",
    "\n",
    "print(f\"\\n--- Chunk Stats : model_id defined splitter ---\")\n",
    "print(f\"Total chunks: {len(final_chunks)}\")\n",
    "print(\n",
    "    f\"Avg size: {np.mean(chunk_lengths):.2f}, Max: {np.max(chunk_lengths)}, Min: {np.min(chunk_lengths)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd11525f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of model_id Chunk Sizes\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(chunk_lengths, bins=20, color=\"skyblue\")\n",
    "plt.title(\"Token Count Distribution per Chunk\")\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c2c31df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7bec38a6a51402982c0182172f359b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Document:', layout=Layout(width='50%'), options=('Trustworthy and Efficie…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZING CHUNKING 1\n",
    "\n",
    "doc_selector = widgets.Dropdown(\n",
    "    options=list(chunk_by_doc.keys()),\n",
    "    description=\"Document:\",\n",
    "    layout=widgets.Layout(width=\"50%\"),\n",
    ")\n",
    "\n",
    "chunk_slider = widgets.IntSlider(min=0, max=1, step=1, description=\"Chunk:\")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "\n",
    "def update_slider(*args):\n",
    "    selected_doc = doc_selector.value\n",
    "    chunk_slider.max = len(chunk_by_doc[selected_doc]) - 1\n",
    "    chunk_slider.value = 0\n",
    "    show_chunk(0)\n",
    "\n",
    "\n",
    "def show_chunk(i):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        selected_doc = doc_selector.value\n",
    "        chunk = chunk_by_doc[selected_doc][i]\n",
    "        print(chunk.metadata)\n",
    "        print(\"\\n\" + chunk.page_content[:1000])\n",
    "\n",
    "\n",
    "chunk_slider.observe(lambda change: show_chunk(change[\"new\"]), names=\"value\")\n",
    "doc_selector.observe(update_slider, names=\"value\")\n",
    "\n",
    "display(widgets.VBox([doc_selector, chunk_slider, output_area]))\n",
    "update_slider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "728fc94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stored in Qdrant Vector DB ---\n",
      "Collection: LLM-papers all-mpnet-base-v2\n"
     ]
    }
   ],
   "source": [
    "# ## Step 3: Store in Qdrant Vector DB\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=model_id)\n",
    "\n",
    "db = Qdrant.from_documents(\n",
    "    documents=final_chunks,\n",
    "    embedding=embedding,\n",
    "    location=\"localhost:6333\",\n",
    "    collection_name=\"LLM-papers all-mpnet-base-v2\",\n",
    "    prefer_grpc=False,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Stored in Qdrant Vector DB ---\")\n",
    "print(f\"Collection: {db.collection_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "618e70d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA\n",
    "texts = [doc.page_content for doc in final_chunks]\n",
    "embeddings = embedding.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "803ce556",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "points = pca.fit_transform(embeddings)\n",
    "\n",
    "# Create color group\n",
    "doc_names = [\n",
    "    doc.metadata.get(\"Title\", \"Doc \" + str(i))\n",
    "    for i, doc in enumerate(final_chunks)\n",
    "]\n",
    "unique_doc_ids = {name: i for i, name in enumerate(set(doc_names))}\n",
    "colors = [unique_doc_ids[name] for name in doc_names]\n",
    "\n",
    "# DataFrame for plot\n",
    "df = pd.DataFrame({\n",
    "    \"x\": points[:, 0],\n",
    "    \"y\": points[:, 1],\n",
    "    \"document\": doc_names,\n",
    "    \"chunk\": [doc.metadata.get(\"chunk_index\", 0) for doc in final_chunks],\n",
    "    \"preview\": [doc.page_content[:100].replace(\"\\n\", \" \") for doc in final_chunks],\n",
    "    \"color_id\": colors\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f0744db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           0,
           "Trustworthy and Efficient LLMs Meet Databases Kyoungmin Kim kyoung-min.kim@epfl.ch EPFL Switzerland "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           1,
           "only as assistants for database administrators (DBAs) [271, 340] but also as internal components of "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           2,
           "on the first page. Copyrights for components of this work owned by others than the author(s) must be"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           3,
           "mental sustainability by reducing the CO2 footprint associated with extensive GPU usage. After intro"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           4,
           "query processing, will be helpful. Our goal is to bridge the gap between essential LLM knowledge and"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           5,
           "Self-reflection, adaptivity Chain-of-thought, multi-hop,  multi-path reasoning Agentic LLMs, network"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           6,
           "LLM-database System (§2.3.5) Integration with OLAP databases Convergence and Future (§2.3.6) Disaggr"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           7,
           "how LLMs can solely improve (Section 2.1.2), how LLMs can im- prove by interacting with the external"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           8,
           "including chain-of-thought prompting [144, 289] and its variants [18, 312] may leverage in-context l"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           9,
           "tion is at the beginning or end of the input, exhibiting a U-shaped performance curve. This phenomen"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           10,
           "Improving Bare LLMs. We briefly explain the approaches to improve the LLM itself to make it more tru"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           11,
           "134, 167, 208, 250, 262], and direct preference optimization (DPO) [83, 138, 234, 338]. RLHF leverag"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           12,
           "dering the handling of extended conversations. Additionally, their text-based nature restricts inter"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           13,
           "requests, managing memory stores in hierarchical or partitioned way [145, 287] or even as a database"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           14,
           "sary information [87, 210]. 2) Scalability: Not only that LLMs have limited context or data they can"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           15,
           "adaptive retrieval [11, 32, 123, 125, 159, 185, 331], which adaptively retrieves information multipl"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           16,
           "A broader view includes compound AI [323] where AI and non- AI components interact with each other, "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           17,
           "optimizes it by storing and reusing these computations, reducing the complexity to linear during inf"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           18,
           "decode steps can be batched to amortize the cost of loading model weights from GPU memory. Note that"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           19,
           "(of prefills and decodes) whenever resources are available. Thus, a new request does not have to wai"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           20,
           "for efficiency, similarly to sparse attentions (Section 2.2.3). InstIn- fer [213] offloads KVs and a"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           21,
           "not compute the full attention scores for all preceding tokens but a subset as an approximation. Som"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           22,
           "Transformer, such as State Space Models (SSMs) [61, 100] that do not rely on attentions, thereby not"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           23,
           "dent requests, early schedulers either prioritize prefills [146] or decodes [4], which tend to optim"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           24,
           "costs less than the generation. 2.3 LLMs Meet Databases The last part of the tutorial discusses the "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           25,
           "and cost-based scheduling of LLM requests. [3] measures the batch times across various inputs (numbe"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           26,
           "DBMSs (heavy LLM calls in plan optimization can be negligible com- pared to query execution with LLM"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           27,
           "data generation is one of the advantages of solving database tasks compared to conventional ML tasks"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           28,
           "Furthermore, unlike the TPC benchmarks for databases, another problem is that there is no comprehens"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           29,
           "for faster online vector retrievals, or use just-in-time vectorization for reducing storage overhead"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           30,
           "ment and hardware utilization. Finally, if we look into the near future, we could also harness the e"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           31,
           "Transformer architecture, pre-training/fine-tuning/prompting in LLMs, and LLM applications in data m"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           32,
           "stead of a common analogy that LLMs are knowledge bases as they generate plausible facts, we will us"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           33,
           "James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, A"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           34,
           "arXiv:2404.14219 [2] Amey Agrawal, Junda Chen, Íñigo Goiri, Ramachandran Ramjee, Chaojie Zhang, Alex"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           35,
           "b74a8de47d2b3c928360e0a011f48351-Abstract-Conference.html [4] Amey Agrawal, Nitin Kedia, Ashish Panw"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           36,
           "by Piggybacking Decodes with Chunked Prefills. CoRR abs/2308.16369 (2023). https://doi.org/10.48550/"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           37,
           "Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Co"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           38,
           "BlackMamba: Mixture of Experts for State-Space Models. CoRR abs/2402.01771 (2024). https://doi.org/1"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           39,
           "[13] Sourav Banerjee, Ayushi Agarwal, and Saloni Singla. 2024. LLMs Will Always Hallucinate, and We "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           40,
           "fast and remarkably powerful. https://huggingface.co/blog/smollm. Accessed: December 15, 2024. [17] "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           41,
           "Piotr Nyczyk, Marcin Copik, Grzegorz Kwasniewski, Jürgen Müller, Lukas Gian- inazzi, Ales Kubicek, H"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           42,
           "ing with Diamond Hardened Joins. Proc. VLDB Endow. 17, 11 (2024), 3215–3228. https://www.vldb.org/pv"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           43,
           "2023.FINDINGS-EMNLP.212 [24] Angela Bonifati, Stefania Dumbrava, George Fletcher, Jan Hidders, Matth"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           44,
           "Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA (Proceedings of"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           45,
           "Scaling Inference Compute with Repeated Sampling. CoRR abs/2407.21787 (2024). https://doi.org/10.485"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           46,
           "Freire, Tony Tong Wang, Samuel Marks, Charbel-Raphaël Ségerie, Micah Carroll, Andi Peng, Phillip J. "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           47,
           "(Eds.). ACM, 13–20. https://doi.org/10.1145/3555041.3589406 [32] Andong Chen, Lianzhang Lou, Kehai C"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           48,
           "[34] Banghao Chen, Zhaofeng Zhang, Nicolas Langrené, and Shengxin Zhu. 2023. Unleashing the potentia"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           49,
           "[37] Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, and Ja"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           50,
           "[40] Wenhu Chen. 2023. Large Language Models are few(1)-shot Table Reasoners. In Findings of the Ass"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           51,
           "Re-Imagen: Retrieval-Augmented Text-to-Image Generator. In The Eleventh International Conference on "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           52,
           "https://doi.org/10.18653/V1/2024. FINDINGS-ACL.842 [45] Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan X"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           53,
           "[47] Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang, Qiang Li, Yue Shen, Lei Liang, J"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           54,
           "[49] Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Ze"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           55,
           "(Proceedings of Machine Learning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 1931"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           56,
           "Ailamaki. 2019. HetExchange: Encapsulating heterogeneous CPU-GPU par- allelism in JIT compiled engin"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           57,
           "11-30. [56] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagna"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           58,
           "https://openreview.net/forum?id=TyFrPOKYXw [58] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbo"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           59,
           "Ré. 2022. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. In Advances i"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           60,
           "Flash- Decoding for long-context inference. https://pytorch.org/blog/flash-decoding/. Accessed: Dece"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           61,
           "Bangkok, Thailand and virtual meeting, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srik"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           62,
           "[66] Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Marga"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           63,
           "[68] Xin Luna Dong, Seungwhan Moon, Yifan Ethan Xu, Kshitiz Malik, and Zhou Yu. 2023. Towards Next-G"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           64,
           "Evfrosiniya Zerminova, and Daria Baidakova. 2020. Crowdsourcing Practice for Efficient Data Labeling"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           65,
           "Wu, Zhifeng Chen, and Claire Cui. 2022. GLaM: Efficient Scaling of Lan- guage Models with Mixture-of"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           66,
           "https:// explodingtopics.com/blog/chatgpt-users. Accessed: 2024-12-15. [74] Matous Eibich, Shivay Na"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           67,
           "ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2024, Barcelona, Spain, August 25-"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           68,
           "Mapping the Neuro-Symbolic AI Landscape by Architectures: A Handbook on Augmenting Deep Learning Thr"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           69,
           "[82] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Sco"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           70,
           "Inference for Large Language Models. CoRR abs/2401.14351 (2024). https: //doi.org/10.48550/ARXIV.240"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           71,
           "[88] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phan"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           72,
           "Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023. Retrieval-Augmented Generation for"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           73,
           "Vivek Srikumar (Eds.). Association for Computational Linguistics, 234–245. https://aclanthology.org/"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           74,
           "[95] In Gim, Seung-seob Lee, and Lin Zhong. 2024. Asynchronous LLM Function Calling. arXiv preprint "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           75,
           "EMNLP 2024 - Industry Track, Miami, Florida, USA, November 12-16, 2024, Franck Dernoncourt, Daniel P"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           76,
           "Sequences with Structured State Spaces. In The Tenth International Conference on Learning Representa"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           77,
           "arXiv:2308.08998 [103] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           78,
           "https://www.ijcai.org/ proceedings/2024/890 [105] Aaron Harlap, Deepak Narayanan, Amar Phanishayee, "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           79,
           "and Yunhe Wang. 2024. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Larg"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           80,
           "Scaling Laws for Transfer. CoRR abs/2102.01293 (2021). arXiv:2102.01293 https://arxiv.org/abs/2102.0"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           81,
           "[114] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The Curious Case of Neura"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           82,
           "Parameter-Efficient Transfer Learning for NLP. In Proceedings of the 36th In- ternational Conference"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           83,
           "18653/V1/2024.FINDINGS-ACL.890 [118] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and H"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           84,
           "Zhifeng Chen. 2019. GPipe: Efficient Training of Giant Neural Networks us- ing Pipeline Parallelism."
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           85,
           "[122] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. 2021. Data Movemen"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           86,
           "Bethard (Eds.). Association for Computational Linguistics, 7036–7050. https: //doi.org/10.18653/V1/2"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           87,
           "2023. StructGPT: A General Framework for Large Language Model to Rea- son over Structured Data. In P"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           88,
           "Torino, Italy, Nicoletta Calzolari, Min-Yen Kan, Véronique Hoste, Alessandro Lenci, Sakriani Sakti, "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           89,
           "[131] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Dan"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           90,
           "[133] George Katsogiannis-Meimarakis, Mike Xydas, and Georgia Koutrika. 2023. Natural Language Inter"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           91,
           "Computational Linguistics, 10438–10451. https://doi.org/10.18653/V1/2024. ACL-LONG.562 [136] Omar Kh"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           92,
           "Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu (Eds.). ACM, 39–48. h"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           93,
           "SIGMOD ’22: International Conference on Management of Data, Philadelphia, PA, USA, June 12 - 17, 202"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           94,
           "[143] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Yacine Jernite, Margaret M"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           95,
           "8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html [145] Juri Kong, Hong Liang, Yuan Zhang, H"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           96,
           "arXiv:2411.02530 (2024). [148] Sangjin Lee, Alberto Lerner, Philippe Bonnet, and Philippe Cudré-Maur"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           97,
           "Learning to Reduce: Optimal Representations of Structured Data in Prompting Large Language Models. C"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           98,
           "EMNLP-MAIN.243 [153] Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast Inference from Tran"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           99,
           "Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/ 6b493230205f780e1bc26945df748"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           100,
           "https://doi.org/10.1145/3448016. 3457542 [157] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, M"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           101,
           "Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander To- shev, Stephanie Wang, Dirk "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           102,
           "in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Syst"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           103,
           "3 (2024), 176. https://doi.org/10.1145/3654979 [161] Xiangyu Li, Yuanchun Li, Yuanzhe Li, Ting Cao, "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           104,
           "Roberto Navigli (Eds.). Association for Computational Linguistics, 4582–4597. https://doi.org/10.186"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           105,
           "and Sai-Kit Yeung (Eds.). https://datasets-benchmarks-proceedings.neurips.cc/ paper/2021/hash/c20ad4"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           106,
           "Linguistics, 881–893. https://aclanthology.org/2024.emnlp-industry.66 [167] Zihao Li, Zhuoran Yang, "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           107,
           "Graph Reasoning on Graph Types: Static, Dynamic, and Multi-Modal. IEEE Trans. Pattern Anal. Mach. In"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           108,
           "Zusman, and Yoav Shoham. 2024. Jamba: A Hybrid Transformer-Mamba Lan- guage Model. CoRR abs/2403.198"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           109,
           "Gavrilovska and Douglas B. Terry (Eds.). USENIX Association, 929–945. https: //www.usenix.org/confer"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           110,
           "ARXIV.2405.14366 arXiv:2405.14366 [177] Chunwei Liu, Matthew Russo, Michael J. Cafarella, Lei Cao, P"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           111,
           "Hindsight Experience. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, H"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           112,
           "https://doi.org/10.1162/TACL_A_00638 [182] Shu Liu, Asim Biswal, Audrey Cheng, Xiangxi Mo, Shiyi Cao"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           113,
           "KV Cache Compression and Streaming for Fast Large Language Model Serving. In Proceedings of the ACM "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           114,
           "and Less is More: Efficient Sparse Attention for Long-Range Transform- ers. CoRR abs/2406.16747 (202"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           115,
           "[189] Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D. Lane, and"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           116,
           "Sky- Serve: Serving AI Models across Regions and Clouds with Spot Instances. CoRR abs/2411.01438 (20"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           117,
           "on Management of Data, SIGMOD/PODS 2024, Santiago AA, Chile, June 9-15, 2024, Pablo Barceló, Nayat S"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           118,
           "arXiv:2410.21868 [197] Laurent Mombaerts, Terry Ding, Adi Banerjee, Florian Felice, Jonathan Taws, a"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           119,
           "Amanpreet Singh, and Douwe Kiela. 2024. Generative Representational Instruc- tion Tuning. CoRR abs/2"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           120,
           "https://doi.org/10.1145/3514221.3522567 [202] Chien Van Nguyen, Xuan Shen, Ryan Aponte, Yu Xia, Samy"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           121,
           "Large Dual Encoders Are Generalizable Retrievers. In Proceedings of the 2022 Conference on Empirical"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           122,
           "Induced Predicates Through Joins in Big-Data Clusters. Proc. VLDB Endow. 13, 3 (2019), 252–265. http"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           123,
           "ral Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, No- vember 28 - Decembe"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           124,
           "S00778-024-00864-X [211] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           125,
           "[214] Joon Sung Park, Joseph C. O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Mi"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           126,
           "arXiv:2407.11418 [217] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           127,
           "Proceedings of the VLDB Endowment 14, 2 (2020), 202–214. [220] Johns Paul, Shengliang Lu, and Bingsh"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           128,
           "V1/2021.EACL-MAIN.39 [223] Jonathan Pilault, Mahan Fathi, Orhan Firat, Chris Pal, Pierre-Luc Bacon, "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           129,
           "Candidate Selection in Text-to-SQL. CoRR abs/2410.01943 (2024). https://doi. org/10.48550/ARXIV.2410"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           130,
           "Scaling Large- Language-Model-based Multi-Agent Collaboration. CoRR abs/2406.07155 (2024). https://d"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           131,
           "world APIs. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           132,
           "[232] Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Ro"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           133,
           "Language Model is Secretly a Reward Model. In Advances in Neural Infor- mation Processing Systems 36"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           134,
           "Compression Rates per Attention Head. CoRR abs/2410.00161 (2024). https: //doi.org/10.48550/ARXIV.24"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           135,
           "Fadaee, Roberto Lotufo, and Rodrigo Nogueira. 2022. In defense of cross- encoders for zero-shot retr"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           136,
           "May 27, 2022, Bing Liu, Alexandros Papangelis, Stefan Ultes, Abhinav Rastogi, Yun-Nung Chen, Georgio"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           137,
           "Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablan"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           138,
           "Systems. In Proceedings of the 4th Workshop on Machine Learning and Systems, EuroMLSys 2024, Athens,"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           139,
           "hash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html [247] Sander Schulhoff, Michael Ilie,"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           140,
           "Milovidov. 2024. ClickHouse-Lightning Fast Analytics for Everyone. Proceedings of the VLDB Endowment"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           141,
           "[251] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Pen"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           142,
           "[253] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Ch"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           143,
           "[255] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPr"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           144,
           "Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/ 1b44b878bb782e6954cd8886285"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           145,
           "https://doi.org/10.1007/S00146-022-01544-6 [260] Panagiotis Sioulas, Viktor Sanca, Ioannis Mytilinis"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           146,
           "[262] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radf"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           147,
           "net/forum?id=AbGbGZFYOD [264] Weihang Su, Changyue Wang, Qingyao Ai, Yiran Hu, Zhijing Wu, Yujia Zho"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           148,
           "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph. CoRR ab"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           149,
           "Xuecang Zhang, Junhua Zhu, and Yu Zhang. 2024. FusionANNS: An Efficient CPU/GPU Cooperative Processi"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           150,
           "Philadelphia, PA, USA, June 12 - 17, 2022, Zachary G. Ives, Angela Bonifati, and Amr El Abbadi (Eds."
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           151,
           "Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           152,
           "Decoding Reduces Hallucinations in Large Multilingual Machine Translation Models. In Proceedings of "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           153,
           "[279] Ke Wang, Jiahui Zhu, Minjie Ren, Zeming Liu, Shiwei Li, Zongye Zhang, Chenkai Zhang, Xiaoyu Wu"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           154,
           "Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, De- cember 10 - 16, 2023, A"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           155,
           "national Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenRevie"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           156,
           "13508. https://doi.org/10.18653/V1/2023.ACL-LONG.754 [286] Zhiruo Wang, Zhoujun Cheng, Hao Zhu, Dani"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           157,
           "[288] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, And"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           158,
           "9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html [290] Steven Whang and Jae-Gil Lee. 2020. "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           159,
           "[293] Yingjun Wu, Chee Yong Chan, and Kian-Lee Tan. 2016. Transaction Healing: Scaling Optimistic Co"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           160,
           "2023, 23-29 July 2023, Honolulu, Hawaii, USA (Proceedings of Machine Learning Research, Vol. 202), A"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           161,
           "https://doi.org/10.48550/ARXIV.2404. 17785 arXiv:2404.17785 [298] Jiale Xu, Rui Zhang, Cong Guo, Wei"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           162,
           "Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Compu- tational Linguistics, 731"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           163,
           "International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenRevi"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           164,
           "[305] Ziwei Xu, Sanjay Jain, and Mohan S. Kankanhalli. 2024. Hallucination is In- evitable: An Innat"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           165,
           "Xiao Li, Sriram Krishnamurthy, Amit Shukla, Michalis Petropoulos, Sameer Paranjpye, Reynold Xin, and"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           166,
           "rav Shah, Rakesh Wanga, Anuj Kumar, Wen-tau Yih, and Xin Luna Dong. 2024. CRAG - Comprehensive RAG B"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           167,
           "[312] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasim"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           168,
           "https://openreview.net/forum?id=WE_vluYUL-X [314] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           169,
           "Chen, and Luis Ceze. 2024. Cascade inference: Memory bandwidth efficient shared prefix batch decodin"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           170,
           "Better Chain-of-Thought Prompting Strategies: A Survey. CoRR abs/2310.04959 (2023). https://doi.org/"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           171,
           "vldb.org/pvldb/vol17/p3283-tang.pdf [322] Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zho"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           172,
           "Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, Wanli Ouyang, and Dongzhan Zhou. 2024. LLaMA-Berry: P"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           173,
           "[327] Qianxi Zhang, Shuotao Xu, Qi Chen, Guoxin Sui, Jiadong Xie, Zhizhen Cai, Yaoqi Chen, Yinxuan H"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           174,
           "[330] Yue Zhang, Hongliang Fei, Dingcheng Li, and Ping Li. 2022. PromptGen: Au- tomatically Generate"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           175,
           "Nash CoT: Multi-Path Inference with Preference Equilibrium. In Proceedings of the 2024 Conference on"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           176,
           "[334] Yilong Zhao, Shuo Yang, Kan Zhu, Lianmin Zheng, Baris Kasikci, Yang Zhou, Jiarong Xing, and Io"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           177,
           "Token-Level Routing. In Adaptive Foundation Models: Evolving AI for Personal- ized and Efficient Lea"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           178,
           "arXiv:2404.18922 [339] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           179,
           "Data Augmentation in Large Model Era. CoRR abs/2401.15422 (2024). https: //doi.org/10.48550/ARXIV.24"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           0,
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications Irene Weber"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           1,
           "characterize an LLM component, including the LLM skills leveraged, the format of the output, and mor"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           2,
           "arXiv:2406.10300v1  [cs.SE]  13 Jun 2024 rate to solve complex tasks [11, 62, 57, 21]. In addi- tion"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           3,
           "expediting system development, they have the poten- tial to revolutionize not only the way users int"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           4,
           "crucial for a clear understanding of an application’s architecture. The taxonomy adopts an original "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           5,
           "applications used for this study. The developed tax- onomy is presented, demonstrated and formally e"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           6,
           "at a time. Training of an LLM optimizes its parameters such that its computed likelihoods align with"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           7,
           "any examples is called zero-shot prompting. One-shot and few-shot prompting fall under the broader c"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           8,
           "sult. This paper defines a particular software compo- nent that accomplishes this as an LLM-based so"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           9,
           "ies on applications of ChatGPT is provided by [27], whereas LLMs are compared based on their applica"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           10,
           "external systems. This approach proved unsuccessful. The second version was based on the classical t"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           11,
           "as LLM-integrated applications often comprise mul- tiple LLM components. Therefore, we developed a m"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           12,
           "erogeneous terminology and concepts in this emerg- ing field make a comprehensive formal literature "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           13,
           "observed software systems through Honeycomb’s Query Builder UI. The recently added LLM-based QueryAs"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           14,
           "instance, in essay-writing scenarios, this involves in- serting additional sections, rearranging sec"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           15,
           "MyCrunchGpt offers a web interface featuring a dialogue window for inputting commands in plain Engli"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           16,
           "workflow, it uses the LLM as a smart parser to ex- tract parameters for APIs and backend tools from "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           17,
           "production is oriented towards greater flexibility. Autonomous transport vehicles carry materials an"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           18,
           "6 independent data repositories storing digital twins of manufacturing assets for use in Industry 4."
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           19,
           "mented with a description of the environment’s cur- rent state and the new order as a comment. The P"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           20,
           "ecutor repeatedly after each app operation with the prompt comprising the updated state of the Graph"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           21,
           "desired as output, its prompt includes an HTML rep- resentation of the GUI state, the GUI actions pr"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           22,
           "statements verify the preconditions of basic opera- tions and trigger remedial actions when precondi"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           23,
           "conversations, they are designed for assisting users in specific tasks. In general, TOD systems requ"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           24,
           "SgpTod is a multi-domain TOD system, concur- rently handling multiple task domains found in stan- da"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           25,
           "ter fuel efficiency and traffic flow. TruckPla- toon comprises an algorithmic control loop which aut"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           26,
           "guage. ExcelCopilot only works with data tables, so its initial suggestion is to convert the active "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           27,
           "LLM components: • Several distinct Action Executors generate code for specific application actions, "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           28,
           "alyzing an LLM-integrated application should begin with identifying and describing its distinct LLM "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           29,
           "in table 2. It comprises both dimensions with gen- uinely mutually exclusive characteristics and tho"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           30,
           "A feature vector representing an LLM component is visualized in one line. For dimensions with non- m"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           31,
           "P E LowCode Planning C S A P U I P S U L LowCode Executing D I A B P L U C V I F U MyGrunchGpt Desig"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           32,
           "A S A P P L P F C E ExcelCopilot Advisor A S A P P P R F S P ExcelCopilot IntentDetector C S C P P U"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           33,
           "tinct LLM components once for each user input by injecting varying prompt instructions. Iterative: T"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           34,
           "UI indicates whether an LLM component contributes significantly to the user interface of an applicat"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           35,
           "is processed like data. E.g., MyCrunchGpt Set- tingsEditor modifies a JSON file, replacing a pro- gr"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           36,
           "comb QueryAssistant devises analytical database queries. Write and Both: No LLM component among the "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           37,
           "term and instead use a domain-specific description. The term “examples” indicates a one-shot or few-"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           38,
           "Prompt ActionPlanning has no State prompt, nor does LowCode Planning (except the dialogue history wh"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           39,
           "and better understandability. Prompt Check describes whether the application em- ploys a review mech"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           40,
           "dialogue history + provided workflow (circumscribed) [69] role and goal + instruction + examples con"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           41,
           "decides how to react to user inputs and formulates chatbot responses. Inform: The application depend"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           42,
           "applications relies on their commonsense knowledge and their ability to correctly interpret and hand"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           43,
           "Item: a single text item from a predefined set of items, such as a class in a classification task. E"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           44,
           "tion. Program: Programmed code checks or revises the LLM output. E.g., Honeycomb QueryAssistant corr"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           45,
           "for LowCode Executing. Program describes instances where the LLM output is consumed and processed fu"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           46,
           "frequencies, as similar LLM components within the same application are aggregated together, indicate"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           47,
           "the Prompt dimensions, the Output Revision dimen- sion, and the Data Function dimension, enhancing t"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           48,
           "We visualize the taxonomy (or, strictly speaking, cat- egorized instances) in a compact form using f"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           49,
           "0 0 21 0 2 17 11 3 7 0 0 2 3 1 4 4 7 8 10 4 6 8 1 0 1 5 3 3 10 Figure 2: Occurrences of characterist"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           50,
           "cess and lead to more standardized and comparable descriptions. Applying the taxonomy is often more "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           51,
           "taxonomy. Moreover, it will likely result in nearly all characteristics being marked for some LLM co"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           52,
           "unified framework for describing LLM-integrated ap- plications, facilitating the comparison and shar"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           53,
           "the creativity of researchers and developers in ap- plying and exploiting the potentials of LLMs, ra"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           54,
           "quantity and the variety of LLM components in an LLM-integrated application. LLM components interact"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           55,
           "also include dimensions for describing the structure of prompts in more detail, as well as dimension"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           56,
           "from research examining LLMs as autonomous agents [11, 62, 57, 21]. This paper defines the concept o"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           57,
           "tion was not available, necessitating speculative in- terpretation. However, since the sample is use"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           58,
           "19 in LLM-integrated applications requires the output to adhere to a specified format. Furthermore, "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           59,
           "are crucial. Availability depends not only on the technical stability of the service, but also on fa"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           60,
           "benchmarks may fail to cover relevant aspects within the wide range of possible options. Another cru"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           61,
           "Marko Ristin. Details of the asset adminis- tration shell-part 1: The exchange of informa- tion betw"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           62,
           "Wang You, Ting Song, Yan Xia, Jonathan Tien, and Nan Duan. Low-code LLM: Visual Pro- gramming over L"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           63,
           "ing the potential of prompt engineering in Large Language Models: A comprehensive review. (arXiv:231"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           64,
           "Human-technology in- tegration with industrial conversational agents: A conceptual architecture and "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           65,
           "Wang, Xiangyu Zhao, Jiliang Tang, and Qing Li. Recommender Systems in the Era of Large Language Mode"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           66,
           "doi:10.48550/arXiv.2312.06677. [20] Muhammad Usman Hadi, Qasem Al Tashi, Rizwan Qureshi, Abbas Shah,"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           67,
           "Grundy, and Haoyu Wang. Large Language Models for Software Engineering: A Systematic Literature Revi"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           68,
           "Cox, Wibe A. de Jong, Matthew L. Evans, Nico- las Gastellu, Jerome Genzling, María Victoria Gil, Ank"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           69,
           "2023. doi:10.1039/D3DD00113J. [25] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Ro"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           70,
           "[28] Varun Kumar, Leonard Gleyzer, Adar Ka- hana, Khemraj Shukla, and George Em Karni- 22 adakis. My"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           71,
           "ACL 2023, pages 4536–4554, Toronto, Canada, July 2023. Association for Computational Linguistics. do"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           72,
           "April 2024. doi:10.48550/arXiv.2404.03275. [34] Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, and "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           73,
           "Burgard. Grounding Language with Visual Af- fordances over Unstructured Data. In 2023 IEEE Internati"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           74,
           "Software-Based Dialogue Systems: Survey, Taxonomy, and Challenges. ACM Comput- ing Surveys, 55(5):91"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           75,
           "365 overview. https://learn.microsoft. com/en-us/copilot/microsoft-365/ microsoft-365-copilot-overvi"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           76,
           "cember 2007. ISSN 0742-1222, 1557-928X. doi:10.2753/MIS0742-1222240302. [47] Mohaimenul Azam Khan Ra"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           77,
           "ture Switzerland. doi:10.1007/978-3-031-43126- 5_11. [49] Shubhra Kanti Karmaker Santu and Dongji Fe"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           78,
           "IEEE. doi:10.1109/ICRA48891.2023.10161317. [52] Gero Strobel, Leonardo Banh, Frederik Möller, and Th"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           79,
           "[55] Daniel Szopinski, Thorsten Schoormann, and Dennis Kundisch. Visualize different: To- wards rese"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           80,
           "40(1): e13098, 2023. doi:10.1111/exsy.13098. [59] Bryan Wang, Gang Li, and Yang Li. En- abling Conve"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           81,
           "Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhew"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           82,
           "[65] Hao Wen, Yuanchun Li, and Sean KiteFly- Kid. MobileLLM/AutoDroid. Mobile LLM, Jan- uary 2024. h"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           83,
           "Yang, Yihao Feng, Ran Xu, Wenpeng Yin, and Caiming Xiong. FOFO: A Benchmark to Evaluate LLMs’ Format"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           84,
           "Zhou, and Helen Meng. SGP-TOD: Build- ing Task Bots Effortlessly via Schema-Guided LLM Prompting. (a"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           85,
           "Instruction- Following Evaluation for Large Language Mod- els. (arXiv:2311.07911), November 2023. do"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           0,
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable Chaofan Lin1∗, Zhenhua Ha"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           1,
           "able in the prompt of a request, and creates the data pipeline when connecting multiple LLM requests"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           2,
           "∗This work is partially done while Chaofan Lin’s internship and Dr. Chen Chen’s visting scholar in M"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           3,
           "the same application, how different requests are connected, or whether there are any similarities. T"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           4,
           "(d) Multi-agent Coding Figure 1: The workflow of popular LLM-based applications. The final result re"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           5,
           "difference between the two types of tasks. As a result, the current practice is to blindly optimize "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           6,
           "three opportunities, leading to high end-to-end service latency and reduced throughput. Based on the"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           7,
           "analysis to perform de-duplication. Parrot’s scheduling also takes different opportunities into acco"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           8,
           "workflow to generate the final results. It splits the long tran- script into chunks, uses multiple r"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           9,
           "intention, enrich the query with supplementary information, retrieve related data, undergo a safety "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           10,
           "shows our empirical study of the latency breakdown of the LLM calls from a popular LLM application i"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           11,
           "Maximize Throughput Minimize Latency Latency=2700 ms Latency=1100 ms Reduce Stage Map Stage Reduce S"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           12,
           "two steps of the same application are scheduled together, thus allowing the output of Step A to be f"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           13,
           "user is located in London, UK.  [user](#message) Explain AI agent for a kid. Task Role (static) Few-"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           14,
           "ference. Increasing the batch size can bring up to 8.2× higher throughput but lead to 95% higher lat"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           15,
           "tomize a ChatGPT for a specific purpose whose prompt tem- plate is the same across users. The common"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           16,
           "MetaGPT [22] and AutoGen [54] recurrently incorporate con- versation history into the prompt over se"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           17,
           "Centering on this abstraction, Parrot Manager is designed to schedule LLM requests at a cluster-leve"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           18,
           "Your test code: {{output:test}} \"\"\" def WriteSnakeGame(): task = P.SemanticVariable(\"a snake game\") "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           19,
           "a structure. Instead, Parrot relies on Semantic Variables to preserve the prompt structure for furth"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           20,
           "Hash( ) Hash( ) ① PrefixHash() ④ GetPerfObj()  Latency    ③ GetConsumers()  [Request( )] ② GetProduc"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           21,
           "scheduler (§5.4). 4.2 Primitives of Inter-Request Analysis In general, Parrot perform inter-request "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           22,
           "final output Semantic Variables, Parrot can deduct the request- level scheduling preference by analy"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           23,
           "(i.e. producer requests are all finished), which allows instant execution and maximizes batching opp"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           24,
           "When an application annotates a Semantic Variable to pre- fer higher throughput, all requests genera"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           25,
           "LLM requests at the same stage are grouped into a task group (Task Groups 0 and 1). The scheduler sh"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           26,
           "where each entry maps a (hashed) prefix of tokens to a list of requests, thus the scheduler can quic"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           27,
           "7 r∗= FindEngine(SharedReqsInQueue); 8 else if CtxInEngine ̸= ∅then 9 r∗= FindEngine(r, filter=CtxIn"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           28,
           "is determined by the LLM request with the most strict la- tency constraint. Therefore, Parrot’s sche"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           29,
           "how Parrot chooses LLM engines (i.e., FindEngine). Briefly, Parrot finds the engine that satisfies t"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           30,
           "the potential of Parrot’s design when facing new types of applications. We leave these extensions as"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           31,
           "is to “glue” different LLM calls to accomplish a complex task (aka. LLM orchestration). Parrot can b"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           32,
           "ment of LLM requests, including Semantic Variables, com- munication, and scheduling. We also build a"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           33,
           "\"transforms\": str}, ...], \"session_id\": str} ,→ ,→ (get) {\"semantic_var_id\": str, \"criteria\": str, \""
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           34,
           "ate attention decoding computation involving shared prefixes. This kernel retains PagedAttention’s a"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           35,
           "ferent requests. Hence we propose a universal abstraction to describe the minimal capability require"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           36,
           "text (i.e. free its KV cache in GPU memory). Separating Fill and Generate not only fits Semantic Var"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           37,
           "Arxiv dataset [27], executing chain and map-reduce summa- rizations on an extensive collection of ac"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           38,
           "quests. The majority of LLM applications used in our baseline Workload Serving Dependent Requests. P"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           39,
           "chat completion APIs as provided by FastChat [62]. FastChat is a widely recognized open-source LLM s"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           40,
           "token) for vLLM, with continuous batching enabled, experi- ences a notable uptick when the engine’s "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           41,
           "512 1024 1536 2048 Chunk Size (# tokens) 0 50 100 150 200 250 Average Latency (s) 1.21x 1.21x 1.20x "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           42,
           "duction in end-to-end latency by as much as 1.38× and 1.88× compared to the baselines employing vLLM"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           43,
           "ground LLM requests at varying rates to examine the capa- bility of Parrot in mitigating additional "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           44,
           "Figure 13: The difference in E2E latency of the 25 chain- summary application between Baseline and P"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           45,
           "Map-Reduce Applications. An alternative implementation of the document summarization application fol"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           46,
           "objective that identifies the mapping tasks as a task group. By recognizing this relationship, Parro"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           47,
           "We synthesized 64 requests from the length distribution we measured using Bing Copilot. The system p"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           48,
           "0.10 0.12 Latency per token (s) 1.44x 1.53x 1.56x 1.58x Parrot Baseline w/ Sharing (a) Batch Size = "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           49,
           "of the shared prompt once, when computing the attention from the diverged tokens of different users."
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           50,
           "0 1 2 3 4 5 6 7 8 9 10111213141516 Request rate (req/s) 0 100 200 300 Normalized latency  (ms/token)"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           51,
           "menting on a single file. The Coders then revise their code based on these comments. This review-and"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           52,
           "16 Number of Files 0 500 1000 1500 Average Latency (s) 1.00x 1.04x 1.14x 1.16x 1.22x 1.61x 1.88x 2.3"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           53,
           "Variables that could also be shared during runtime. As de- picted in Figure 18b, Parrot without this"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           54,
           "implementations: one tailored for latency, limiting engine ca- pacity to reduce decoding time, and a"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           55,
           "efficient than the throughput-centric baseline. Parrot excels by providing both low latency for chat"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           56,
           "level, which is also known as continuous batching. vLLM proposes PagedAttention [25] allows the batc"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           57,
           "cally generate plans based on the needs of the users. Prompt- Flow [35] supports chains of native an"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           58,
           "such dependency information. Moreover, it is unique to LLM applications to understand the prompt str"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           59,
           "Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, San- jay Ghemawat, Geoffrey Irving, Michael Isard, M"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           60,
           "using mantri. In 9th USENIX Symposium on Operating Systems Design and Implementation (OSDI 10), Van-"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           61,
           "langchain-ai/langchain, October 2022. [9] Lequn Chen. Dissecting batching effects in gpt infer- ence"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           62,
           "A. Oh, editors, Advances in Neural Information Process- ing Systems, volume 35, pages 16344–16359. C"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           63,
           "ciation. [17] Juncheng Gu, Mosharaf Chowdhury, Kang G. Shin, Yibo Zhu, Myeongjae Jeon, Junjie Qian, "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           64,
           "GPU-accelerated DNN inferences. In 16th USENIX Symposium on Operating Systems Design and Imple- ment"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           65,
           "and Dennis Fetterly. Dryad: Distributed data-parallel programs from sequential building blocks. In P"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           66,
           "New York, NY, USA, 2023. Association for Computing Machinery. [26] Benjamin Lefaudeux, Francisco Mas"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           67,
           "USENIX Association. [29] Jerry Liu. LlamaIndex, November 2022. [30] Ashraf Mahgoub, Karthick Shankar"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           68,
           "[34] Microsoft. Microsoft 365 copilot. https: //www.microsoft.com/en-us/microsoft-365/ enterprise/mi"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           69,
           "Flexible, high-performance ml serving. In Workshop on ML Systems at NIPS 2017, 2017. [40] OpenAI. Ch"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           70,
           "chini. Splitwise: Efficient generative llm inference using phase splitting. arXiv preprint arXiv:231"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           71,
           "Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient fo"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           72,
           "[55] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming langua"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           73,
           "USENIX Symposium on Operating Systems Design and Implementation (OSDI 08), San Diego, CA, 2008. [59]"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           74,
           "a GPU cluster for deep learning with guarantees. In 14th USENIX Symposium on Operating Systems Desig"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           0,
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends ZIBIN ZHENG, "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           1,
           "from four databases and categorize Code LLMs based on their publishers. Next, we investigate the per"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           2,
           "∗Yanlin Wang is the corresponding author (wangylin36@mail.sysu.edu.cn). Authors’ addresses: Zibin Zh"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           3,
           "provided that copies are not made or distributed for profit or commercial advantage and that copies "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           4,
           "mining [114], and more [15, 117]. However, many studies adopt a negative stance on the current perfo"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           5,
           "as well as the remarkable performance of state-of-the-art general LLMs like GPT-4 in various softwar"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           6,
           "obtained 149 relevant and valid papers. Through this research, we aim to address the following three"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           7,
           "Furthermore, when the number of parameters is comparable, Code LLMs tend to outperform general LLMs."
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           8,
           "selected 149 relevant works from a large number of articles in four major open-source communities or"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           9,
           "Finally, in Section 7, we summarize the entire paper. 2 METHODOLOGY In this section, we introduce th"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           10,
           "SE LLM Software Engineer- ing Large Language Model Software Engineer- ing LLM SE Large Language Mode"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           11,
           "but do not include LLM/Large Language Model keywords in their abstracts. Additionally, Google Schola"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           12,
           "• Evaluation of LLMs on Software Engineering Tasks To improve the accuracy of the literature screeni"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           13,
           "papers can be found at https://github.com/. 2.3 Data Analysis We used an open card sorting approach "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           14,
           "RQs Type of Data We Collected RQ1 What LLMs are specifically designed for software engineering tasks"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           15,
           "designed specifically for software engineering tasks and show the relationships between them. Accord"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           16,
           "multilayer generative Transformer models. GPT-C underwent retraining on a large-scale unsuper- vised"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           17,
           "involves retraining the model with randomly initialized parameters. The second variant, CodeGPT- ada"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           18,
           "is a 1.3 billion parameter LLM trained primarily on a specially curated “textbook-quality\" synthetic"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           19,
           "in 100 sampling tests. PyCodeGPT [109] is a library-oriented code generation model developed by Micr"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           20,
           "propose addressing this challenge by fine-tuning models using annotated data and enabling models to "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           21,
           "achieves excellent performance on multiple code generation benchmarks and the DeepFix code repair te"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           22,
           "regions. It is trained on a large corpus of code files with permissive licenses, where random code r"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           23,
           "Huawei: PanGu-Coder [19] is a pre-trained decoder language model proposed by Huawei Noah’s Ark Lab. "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           24,
           "Cloud Intelligent Programming Assistant CodeArts Snap to enhance developers’ programming efficiency."
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           25,
           "on cross-lingual NL-PL or NL-NL pairs to enable the model to learn cross-lingual/modal align- ment a"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           26,
           "2021 2022 2023 2020 MultiPL-T Phi-1 GPT-C PyMT5 CodeGPT PLBART CodeParrot CodeT5 Codex GPT-Neo JuPyT"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           27,
           "770M 2B-137B 1.3B-6.7B 220M-16B 350M-16.1B 1.1B 7B-13B 1B-16B 13B 2.7B 7B 15.5B 15B 1.3B 16B Unknown"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           28,
           "denoising autoencoding on a large corpus of Java and Python functions along with their associated na"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           29,
           "CodeGeeX [116] is an open-source multilingual LLM led by Tsinghua University. CodeGeeX adopts a deco"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           30,
           "Due to inheriting the characteristics of the ChatGLM2-6B model, CodeGeeX2-6B provides better support"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           31,
           "six programming languages. Cassano et al. [10] also focus on low-resource languages and propose an e"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           32,
           "code, LLMs lack the capability in this aspect. This is because, during pre-training, LLMs have limit"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           33,
           "functionality in two programming languages: Java and C#. The results of the study show that Codeedit"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           34,
           "text generation model with 6 billion parameters trained on the Pile using Mesh Transformer JAX. Alth"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           35,
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 13 evaluation"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           36,
           "(FIM) capability within the paradigm of LLMs [7]. The FIM framework allows for simple modifications "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           37,
           "which tokens are identifiers and recover them when they are masked. CodeT5 was trained on the CodeSe"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           38,
           "enabling LLMs to understand long contexts and generate coherent responses. Therefore, utilizing this"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           39,
           "MULTI. Finally, developers employ the parameters of CodeGen-MULTI as initial parameters for training"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           40,
           "and natural language distributions on model performance. The experimental results showed the followi"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           41,
           "component, RoBERTa, and two decoder components, GPT-2 and CodeGPT. By combining these components (or"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           42,
           "comparison with other LLMs in terms of performance on software engineering tasks, nor did it further"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           43,
           "as usernames while generating it. In addition, the BigCode community has also released an open-sourc"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           44,
           "coding tasks (code fixing, code interpretation, and code synthesis) across six languages (Python, Ja"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           45,
           "OpenChat: OpenCoderPlus [96]is a code-oriented large language model in the OpenChat series, fine-tun"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           46,
           "and CodeExercise-Python-27k. They have also introduced two models: CodeFuse-13B and CodeFuse- CodeLl"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           47,
           "3.4 Individuals & Anonymously-led LLMs CodeAlpaca [13] is an instruction-guided LLaMA [94] model tra"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           48,
           "ENGINEERING TASKS? In this section, we primarily focus on addressing and summarizing RQ2. First, we "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           49,
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 17 Phi-1.5 GP"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           50,
           "natural language generation data visualization. The article evaluates them through five case studies"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           51,
           "speaking, ChatGPT is slightly better. However, they still fall far short of human performance. Yang "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           52,
           "Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen problem"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           53,
           "CSN-Python dataset for code summarization. The results show that ChatGPT performs relatively poorly,"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           54,
           "experiments in the article are conducted on three Public library benchmarks and two Private library "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           55,
           "multiple models on two different aspects (human preferences and execution success) and four programm"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           56,
           "J-6B, and GPT-NeoX-20B in almost all metrics. The article also provides a comparison between PaLM-54"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           57,
           "stronger performance. Fu et al. [27] introduce CodeApex, a bilingual benchmark dataset focused on te"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           58,
           "as GPT-2. CodeGPT performs better than GPT-2 in tasks such as code completion and code search. Simil"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           59,
           "descriptions) and outputs (solution code), along with example tests for effective filtering. Therefo"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           60,
           "CodeGen-Mono, Codex, and other Code LLMs. Wang et al. [99] extensively evaluated CodeT5Mix on seven "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           61,
           "was outperformed by general LLMs on Language Understanding and Knowledge Benchmarks. The article pre"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           62,
           "some work has compared the performance differences between Code LLMs and general LLMs. Tang et al. ["
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           63,
           "21 more attention to different types of keywords, while programmers are more sensitive to adjectives"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           64,
           "Secondly, due to the varying publication dates of each work, the choice of Code LLMs and general LLM"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           65,
           "• Currently, the state-of-the-art Code LLMs (such as CodeFuse-CodeLlama-34B and Open- CoderPlus) out"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           66,
           "with competitive programming settings. In competitive programming, there is typically a well-defined"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           67,
           "✓ Information extraction Zhang et al. [113] ✓ API-related Tasks Siddiq et al. [84] ✓ Unit test Gener"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           68,
           "✓ Code Generation Total Number 11 20 7 enables them to achieve stronger results in the HumanEval set"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           69,
           "code generation tasks. Some commonly used benchmarks include HumanEval [16], DS-1000 [44], and MBPP "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           70,
           "from the collected papers on HumanEval and obtained Table 4. In the case of *, this result is source"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           71,
           "data is limited, making it difficult to showcase performance differences among different LLMs. There"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           72,
           "18.16 27.81 44.85 CodeFuse-CodeLlama-34B 74.4 - - SantaCoder-1.1B 18 29 49 Phind-CodeLlama-34B-v2 73"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           73,
           "25.6 41.2 WizardCoder-15B 57.3 73.2 90.46 InCoder-6.7B 15.2 27.8 47.0 Code-LLaMA-Python-34B 53.7 82."
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           74,
           "CodeGen-Mono-350M 12.76 23.11 35.19 Code-LLaMA-Instruct-34B 41.5 77.2 93.5 CodeGen-mono-350M 12.76 2"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           75,
           "35.0 54.5 77.9 InCoder-1.3B 8.9 16.7 25.6 Code-LLaMA-Instruct-7B 34.8 64.3 88.1 PyCodeGPT-110M 8.33 "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           76,
           "5.59 9.84 17.68 MIM-2.7B 30.7 48.22 69.6 JuPyT5-300M 5.4 15.46 25.60 Replit-Finetuned-2.7B 30.5 - - "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           77,
           "17.9 PaLM-540B 26.2 - 76.2 CodeParrot-1.5B 3.99 8.69 17.88 CodeGen-Mono-6.1B 26.13 42.29 65.82 CodeP"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           78,
           "2.48 5.93 9.62 LLaMA2-34B 22.6 47.0 79.5 PolyCoder-160M 2.13 3.35 4.88 MIM-1.3B 22.4 41.7 53.8 CODEG"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           79,
           "8.9 16.7 25.6 InCoder-1.3B [110] 11.09 16.14 24.20 CodeGen-multi-16.1B [116] 19.22 34.64 55.17 CodeG"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           80,
           "2B multi PolyCoder 160M PolyCoder 0.4B PolyCoder 2.7B GPT 3.5 Java - - 0.0979 0.2330 - - 0.3019 0.13"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           81,
           "best performer is code-davinci-002 (Codex), followed by CodeGen-6B. Siddiq et al. [83] introduced a "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           82,
           "the performance of CodeGenAPI, which is CodeGen fine-tuned for this task, did not surpass that of Co"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           83,
           "6.00 24.00 CERT-numpy-220M 31.47 46.42 16.03 27.72 2.20 14.00 CERT-pandas-220M 18.81 33.66 28.42 48."
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           84,
           "11.88 GPT-3.5 2.47 8.91 6.68 17.82 CodeGenAPI-350M 1.19 4.68 4.44 8.24 CodeGenAPI-retrieval-475M 3.4"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           85,
           "7.34% 9.02% 0.66% 61.36% 80.13% 49.17% 64.47% 72.93% 47.02% Vicuna- 1.5 45.66% 37.17% 16.97% 57.85% "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           86,
           "Method buggy-HumanEval buggy-FixEval CodeGen InCoder CodeGen InCoder 350M 2 B 1 B 6 B 350M 2 B 1 B 6"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           87,
           "34.9% Instruct-CodeGen 8.2% 12.3% 13.0% 24.9% 34.3% 37.1% CodeGeeX 7.2% 9.4% 10.0% 21.2% 27.1% 29.5%"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           88,
           "understanding and code generation capabilities. In terms of programming understanding, GPT-3.5- turb"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           89,
           "performance in practical code generation. The article does not conduct large-scale evaluation ex- pe"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           90,
           "Fully Markdown-formatted 9.9 39.9 74.5 89.0 Average 10.3 36.0 73.9 89.1 Variance 1.01 13.61 2.04 0.3"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           91,
           "of generated tests and the percentage of passed generated tests. The experimental results in the art"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           92,
           "performance is better than that of GPT-3.5. In Siddiq et al. [84], ChatGPT, codegen-350M-multi, and "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           93,
           "74.5% 1,406 222 SF110 Codex (4K) 3.4% 83.5% 1,039 152 Table 16. Performance comparison on Test Suite"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           94,
           "18.45 23.58 18.32 CodeT5-220M 15.24 16.16 19.56 20.01 20.31 26.03 19.55 CodeT5+-220M 15.51 16.27 19."
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           95,
           "The details of their performance can be found in Table 17. Furthermore, Wanget al. [99] demonstrates"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           96,
           "CodeGeeX-13B - - - 26.54 43.56 56.48 25.84 41.52 59.72 23.22 47.33 65.87 9.56 23.83 33.56 CodeGeeX-1"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           97,
           "Java InCoder-6.7B 42.76 65.55 80.43 40.01 55.17 70.39 - - - 43.20 68.24 84.39 21.58 35.20 54.97 Code"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           98,
           "- - 33.38 49.08 64.14 CodeGeeX-13B 31.15 54.02 72.36 30.32 51.63 69.37 24.68 48.35 69.03 - - - 11.91"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           99,
           "- - Based on the available information, it can be concluded that in the task of code summarization, "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           100,
           "points out a strong correlation between the average number of test attempts per translation sample a"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           101,
           "82.5% 92.7% 66.7% 20.1% 89.0% 93.5% 99.0% Total/Average (CodeNet) 81.9% 91.6% 62.7% 18.0% 86.8% 90.7"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           102,
           "ties of LLMs. MBPP, which stands for Massively Bugs and Performance Problems, is a benchmark that co"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           103,
           "attempted to search using a snowballing approach but did not make significant progress. Based on the"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           104,
           "Pass@1Pass@10Pass@80Pass@100 WizardCoder-16B 51.8 - - - CodeParrot-110M 0.48 3.89 - 15.93 Unnatural-"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           105,
           "- 84.1 PyCodeGPT-110M 9.39 28.37 - 48.71 Code-LLaMA-7B 41.4 66.7 - 82.5 PolyCoder-400M 1.31 7.98 - 2"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           106,
           "- 67.92 LLaMA-7B 17.7 - - - CodeGen-Multi-350M 7.46 24.18 - 46.37 LLaMA-65B 37.7 - - - CodeGen-Multi"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           107,
           "- 80.09 JuPuT5-300M - - 52.2 - CodeGen-Mono-16.1B 35.3 - - - InstructCodeT5+-16B - - - - CODEGEN-Mon"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           108,
           "29.51 code-davinci-001 51.80 72.80 - 84.10 GPT-J-6B 11.30 35.62 - 53.63 code-cushman-001 45.90 66.90"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           109,
           "46.5 - 66.2 CodeGen-Mono-16.1B [67] 35.28 67.32 - 80.09 CodeGen-Mono-16.1B [50] 35.3 - - - prompts. "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           110,
           "20 10 - 13 GPT-J 28 14 - 16 GPT-NeoX 34 18 - 21 CodeT5 6 - 6 - INCODER 1.3B 32 - 32 - INCODER 6.7B 3"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           111,
           "achieve similar (or even better) performance through carefully designed APR tools. Additionally, in "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           112,
           "23.4 0.0 56.2 0.0 13.0 ChatGLM-6B 7.1 54.2 17.5 12.8 1.7 46.2 45.0 54.0 Vicuna-7B 54.0 54.1 13.2 - 1"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           113,
           "50.0 Instruct-CodeGen-16B 47.8 54.6 14.2 20.7 8.4 55.0 9.0 41.0 5.6 Other Evaluation or New Benchmar"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           114,
           "Zan et al. [110] conducted a comprehensive investigation of Code LLMs on 27 existing LLMs and review"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           115,
           "surpass the best monolingual models. (2) LLMs have the potential to learn from their uncurated progr"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           116,
           "165 English Multi. 203.7 1989.2 66.4 2239.3 92.1 Competitions DS-1000 1,000 English Python 1.6 879.1"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           117,
           "Private Library MTPB 115 English Python - 72.7 1.0 - - Multi-Turn ODEX 2022 c 945 Multi. Python 1.8 "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           118,
           "worse. The authors attribute this to InstructCodeT5+, CodeGen, and CodeGen2 being trained on single-"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           119,
           "Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen Table 2"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           120,
           "3.013 4.459 Necessary Only 0 0 0 0 0.032 0.159 0.318 0.637 StarCoder-15.5B Summary at Top 0 0 0 0 3."
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           121,
           "1.911 Summary Only 1.300 5.031 8.042 12.000 2.548 8.279 12.864 18.057 Summary at Bottom - - - - 4.17"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           122,
           "6.369 Necessary Only 0 0 0 0 0 0 0 0 CodeGen2-7B Summary at Top 0 0 0 0 0.637 0.637 0.637 0.637 Unco"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           123,
           "44.029 47.771 A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           124,
           "lack relevant benchmarks to measure the differences in capabilities among various models in other so"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           125,
           "article organizes and categorizes 123 selected works and literature on the intersection of software "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           126,
           "syntax trees and ELMo-enhanced variational autoencoders to train multiple pre-trained source code la"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           127,
           "LLMs for various software engineering tasks. Karmakar et al. [39] evaluated the code synthesis abili"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           128,
           "LLMs. Next, we organized Code LLMs based on the types of institutions to which their main developers"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           129,
           "[2] Toufique Ahmed and Premkumar T. Devanbu. 2022. Few-shot training LLMs for project-specific code-"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           130,
           "2023. SantaCoder: don’t reach for the stars! CoRR abs/2301.03988 (2023). https://doi.org/10.48550/ar"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           131,
           "Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022. Association for Comput"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           132,
           "Language Models. https://arxiv.org/abs/2204.06745 [9] Nghi D. Q. Bui, Hung Le, Yue Wang, Junnan Li, "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           133,
           "Zero-shot, Few-shot, or Fine-tuning? arXiv:2306.01754 [cs.CR] [12] Shubham Chandel, Colin B. Clement"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           134,
           "preprint arXiv:2107.03374 (2021). [16] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Po"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           135,
           "[19] Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           136,
           "Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen [22] De"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           137,
           "Large Language Models of Code Fail at Completing Code with Potential Bugs. arXiv:2306.03438 [cs.LG] "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           138,
           "Jianghao Lin, Yuchen Fang, Yifan Liu, Jingkuan Wang, Siyuan Qi, Kangning Zhang, Weinan Zhang, and Yo"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           139,
           "Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Ma"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           140,
           "https://doi.org/10.1162/neco.1997.9.8.1735 [34] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           141,
           "Neural Attention Model. In Proceedings of the 54th Annual Meeting of the Association for Computation"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           142,
           "and Retrieval. arXiv:2303.03004 [cs.CL] [41] B. A. Kitchenham. 2007. Kitchenham, B.: Guidelines for "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           143,
           "Fried, Sida Wang, and Tao Yu. 2022. DS-1000: A Natural and Reliable Benchmark for Data Science Code "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           144,
           "Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin C. C. Chang, Fei Huang, Reynold Cheng, and Yong"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           145,
           "Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nichol"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           146,
           "arXiv:2305.06161 [cs.CL] [51] Tsz-On Li, Wenxi Zong, Yibo Wang, Haoye Tian, Ying Wang, Shing-Chi Che"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           147,
           "and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. CoRR abs/1907.1"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           148,
           "Daxin Jiang. 2023. WizardCoder: Empowering Code Large Language Models with Evol-Instruct. CoRR abs/2"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           149,
           "ChatGPT, Codex and GPT-3 Large Language Models. arXiv:2302.02094 [cs.HC] [61] Pablo Antonio Martínez"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           150,
           "arXiv:2308.07124 [cs.CL] [65] Artashes Arutiunian Nathan Coooper et al. 2022. Code Clippy Data: A la"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           151,
           "ating and Explaining Large Language Models for Code Using Syntactic Structures. arXiv:2308.03873 [cs"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           152,
           "[72] Phind. 2023. Phind-CodeLlama. https://huggingface.co/Phind/Phind-CodeLlama-34B-v1 [73] Colin Ra"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           153,
           "[75] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Amb"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           154,
           "Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis M"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           155,
           "Zhao, Yuenan Guo, and Qianxiang Wang. 2023. PanGu-Coder2: Boosting Large Language Models for Code wi"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           156,
           "43 arXiv:2305.00418 [cs.SE] [85] Giriprasad Sridhara, Ranjani H. G., and Sourav Mazumdar. 2023. Chat"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           157,
           "using transformer. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Con"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           158,
           "Applications. CoRR abs/2201.08239 (2022). arXiv:2201.08239 https://arxiv.org/abs/2201.08239 [93] THU"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           159,
           "Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA. 5"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           160,
           "of Encoder-decoder Transformers for Code Understanding and Generation. https://openreview.net/forum?"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           161,
           "Language Models of Code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Pr"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           162,
           "ang Wang. 2023. CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Mode"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           163,
           "oriented Code Generation. In Proceedings of the Thirty-First International Joint Conference on Artif"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           164,
           "Language Models. arXiv:2307.14991 [cs.SE] [113] Kechi Zhang, Huangzhao Zhang, Ge Li, Jia Li, Zhuo Li"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           165,
           "[116] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           166,
           "arXiv:2308.10335 [cs.CL] [120] Maosheng Zhong, Gen Liu, Hongwei Li, Jiangling Kuang, Jinshan Zeng, a"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           167,
           "arXiv:2304.14317 [cs.AI] [125] Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri, Weiqing Wang, "
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           0,
           "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1 LLM Online Spatial-temporal Signal Recon"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           1,
           "to breakthroughs in many fields such as healthcare diagnostics [1] and investment portfolio construc"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           2,
           "healthcare decisions can be made by LLMs which can lead to physical or psychological harm [12]. Desp"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           3,
           "quantum computing [18]. By exploiting the graph topology, interactions among multivariate data are c"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           4,
           "stantial advantages. Firstly, this integration potentially expands the application of LLMs by enabli"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           5,
           "time-varying GSP method and LLMs for efficient and accurate reconstruction of missing signals in dyn"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           6,
           "knowledge is presented in Section II. Section III provides a detailed discussion of the LLM-OSR. The"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           7,
           "where D is the degree matrix, defined as D = diag(1T A) and 1 is an all ones vector. The spectral op"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           8,
           "also known as the graph convolution. By implementing various filters, such as high-pass and low-pass"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           9,
           "ments that address some of these limitations of GPT-3. GPT-4 offers improved performance across a wi"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           10,
           "The LLM-OSR algorithm reconstructs missing graph sig- nals by combining GSP-based processing and LLM"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           11,
           "(6) where the membership of a node vi in the observation set is defined as: O(vi) = ( 1, if vi ∈O, 0"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           12,
           "2: Initialize and train the GSP-based spatial-temporal signal handler as seen in Algorithm 2 3: Test"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           13,
           "the graph convolution (4) to the training data, calculating the loss of the estimation, and updating"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           14,
           "IGFT GSP filter learning Training set: noise signal 𝒙1 …  𝒙T GSP-based  Spatiotemporal  Signal Handl"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           15,
           "spatial-temporal signal handler is applied to unseen test sam- ples o[t] as they are received in rea"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           16,
           "the graph is typically transformed into a feature space using methods such as GCN or Node2Vec, simil"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           17,
           "h(Λ) = h(Λ) −η · ∇hMAE as seen in (9) 10: end while 11: Export the parameters after training is comp"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           18,
           "from the observed node neighbors. These expressions are then further put into the prompt as the task"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           19,
           "(11) This approach shifts the role of the LLM from performing logical reasoning or mathematical calc"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           20,
           "logical reasoning of GPT-4, demonstrating better performance compared to GPT-3 at understanding prom"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           21,
           "constraints to shape the behavior of the LLM. For our spa- tiotemporal task, this role defines the o"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           22,
           "These prompts include precise temporal and spatial context, such as the time index, the node index, "
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           23,
           "checking function ensures that exceptions and invalid LLM outputs are handled robustly, even in case"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           24,
           "the LLM Let LLM predict ˆxi[t] 9: end for (Retry if output invalid) 10: Collect all node reconstruct"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           25,
           "testing set. An illustration of 4 different time instances of this time-varying dataset can be found"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           26,
           "from the observation and past p estimations. 2) Considered Algorithms: We consider 2 distinct settin"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           27,
           "activation function. • GVARMA [45]: A time-series analysis method that ex- tends the classical VARMA"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           28,
           "robs\":null,\"finish_reason\":\"stop\"}],\"usage\":{\"prompt_tokens\":106,\"completion_tokens\":3,\"tot al_token"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           29,
           "not output text. Do not recall memories.  Time index:  1439, Entity index: 322. Previous: 61.5, Neig"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           30,
           "I and II, demonstrate the significant performance improve- ments achieved by the LLM-OSR models in t"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           31,
           "level signals and adapt to noisy inputs compared to GPT-3.5. It also shows that earlier LLM versions"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           32,
           "TABLE II EXPERIMENT MAE FOR SEATTLE LOOP DATASET Model 1.0 1.5 LLM-OSR-3.5 3.52 ± 3.8e-01 4.23 ± 3.3"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           33,
           "their capability in handling spatiotemporal graph data under varying noise conditions. Gaussian nois"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           34,
           "and with temporal attention for temporal embeddings. These embeddings are then integrated using tran"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           35,
           "are expected to perform worse as noise increases, but LLM- OSR-4 appears more sensitive to this degr"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           36,
           "1.91 ± 1.1e-02 GLMS 2.19 ± 1.8e-03 2.21 ± 2.9e-03 2.22 ± 5.4e-03 GNLMS 2.18 ± 1.5e-03 2.20 ± 3.3e-03"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           37,
           "2.77 ± 8.8e-01 3.09 ± 8.4e-01 5.06 ± 4.9e-01 LLM-OSR-4 1.01 ± 6.3e-03 1.35 ± 6.3e-03 1.54 ± 1.0e-02 "
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           38,
           "3.10 ± 1.9e-03 3.12 ± 3.9e-03 3.14 ± 7.7e-03 V. LIMITATIONS AND FUTURE WORK The LLM-OSR demonstrated"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           39,
           "GLMS 4.41 ± 5.1e-03 4.43 ± 7.1e-03 4.45 ± 9.1e-03 GNLMS 4.40 ± 5.7e-03 4.41 ± 7.0e-03 4.42 ± 9.1e-03"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           40,
           "1.97 ± 8.0e-02 2.02 ± 8.2e-02 LLM-OSR-4 0.90 ± 4.1e-03 1.19 ± 3.2e-03 1.35 ± 3.6e-03 GLMS 3.03 ± 1.3"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           41,
           "2.47 ± 4.5e-03 us a numeric output. This limitation can be addressed in the future when more powerfu"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           42,
           "prompt that demonstrate how numerical outputs are expected; this can transform the LLM predictors fr"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           43,
           "complexity. Currently, the speed of completing each LLM call is constrained by the speed at which LL"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           44,
           "tailed noise, such as communication systems [53] and medical imaging [54]. A potential approach invo"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           45,
           "OSR-4 in capturing spatial-temporal dependencies and it achieves high accuracy in signal reconstruct"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           46,
           "D. Doreswamy, and S. K. Bhat, “Forecasting stock market prices using machine learning and deep learn"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           47,
           "A. G. Lee, and A. Tavakkoli, “Gpt-4: a new era of artificial intelligence in medicine,” Irish Journa"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           48,
           "research, current challenges, and possible future directions,” Journal of King Saud University-Compu"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           49,
           "F. Ferreri, and P. M. Rossini, “Brain connectivity and graph theory analysis in alzheimer’s and park"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           50,
           "[18] S. Xu, F. Wilhelm-Mauch, and W. Maass, “Quantum feature embeddings for graph neural networks.,”"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           51,
           "convolutional networks,” ICLR, 2017. [23] B. Yu, H. Yin, and Z. Zhu, “Spatio-temporal graph convolut"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           52,
           "[28] R. Ye, C. Zhang, R. Wang, S. Xu, Y. Zhang, et al., “Natural language is all a graph needs,” arX"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           53,
           "C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCan"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           54,
           "general intelligence: Early experiments with gpt-4,” arXiv preprint arXiv:2303.12712, 2023. [34] J. "
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           55,
           "bedding technique using wikipedia,” Journal of Intelligent Information Systems, vol. 53, pp. 137–165"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           56,
           "Signal and Information Processing over Networks., vol. 2, no. 4, pp. 555 – 568, 2016. [43] M. J. M. "
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           57,
           "“Simac: simulating agile collaboration to generate acceptance criteria in user story elaboration,” A"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           58,
           "Y. Fang, “Evaluating very long-term conversational memory of llm agents,” in Proceedings of the Annu"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           59,
           "exploiting interference fringe,” Communications Biology, vol. 6, no. 1, pp. 464, 2023. [55] D. Herra"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           60,
           "“Insights on features’ contribution to desalination dynamics and capacity of capacitive deionization"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           0,
           "What Limits LLM-based Human Simulation: LLMs or Our Design? Qian Wang 1 Jiaying Wu 1 Zhenheng Tang 2"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           1,
           "man behavior by replicating their actions and characteristics (Ofoegbu, 2023; Winsberg, 2003). It ha"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           2,
           "2023b; Yang et al., 2024b). Moreover, reliable LLM sim- ulations can generate high-quality data for "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           3,
           "diverse and randomized personas in response to psychologi- cal questionnaires, they exhibit consiste"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           4,
           "comprehensive and unbiased human behavioral data, and (2) developing systematic validation framework"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           5,
           "tion 2 provides a comprehensive review of LLM-based sim- ulation categories and their key components"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           6,
           "providing a comprehensive understanding of their architec- tures, capabilities, and limitations. 2.1"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           7,
           "in-depth social science analysis (Bail, 2024; Ziems et al., 2024). These simulations often explore e"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           8,
           "2: Generate agent responses: ai ←fi(mi, E) for each fi ∈Fs // Agents process inputs and generate act"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           9,
           "mentation impacts. LLMs are utilized to simulate policy- making processes through virtual government"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           10,
           "lations (Gao et al., 2024) enables both the exploration of diverse scenarios and the study of emerge"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           11,
           "et al. (2024e) pointed out that LLM agents exhibited incon- What Limits LLM-based Human Simulation: "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           12,
           "update their internal states including memories and relation- ships (Si ←update(ai, mi)), and plan f"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           13,
           "lateral dictator games (Kahneman et al., 1986) and hiring decisions (Horton et al., 2011). At the sy"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           14,
           "strategies with gaming history. Fontana (Fontana et al., 2024) found that in the Iterated Prisoner’s"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           15,
           "2.5. Policy Simulations Policy simulations require modeling complex human in- teractions and trust d"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           16,
           "2021), such as feedback, nonlinearity, and time lags. Lem- pert (Lempert, 2002) highlights ABMs’ pot"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           17,
           "components into LLM-generated behaviors and human-defined controls. LLM Actions Human Participation "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           18,
           "2.6. Psychology Simulation LLMs have demonstrated significant potential in psycho- logical simulatio"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           19,
           "(Qiu & Lan, 2024) have shown promise in generating high- quality therapeutic dialogues. In psycholog"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           20,
           "ensuring psychological validity and experimental rigor. To better understand psychology simulations "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           21,
           "authentic cross-cultural interactions and decision-making processes. This bias particularly affects "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           22,
           "tain consistent cognitive patterns when simulating human What Limits LLM-based Human Simulation: LLM"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           23,
           "tent personas across multiple interactions (Han et al., 2022), often fail to accurately simulate com"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           24,
           "many frameworks reduce complex emotional states to basic categories or numerical scales, failing to "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           25,
           "1987). This limitation stems partly from LLMs’ inherent constraints - they lack embodied experience "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           26,
           "incorporate expert knowledge and domain-specific insights (Si et al., 2024). Framework designers fac"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           27,
           "data from multiple cultural contexts (Santurkar et al., 2023), for example, using multilingual datas"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           28,
           "Comprehensive framework design with validation. To enhance framework reliability, we recommend: (1) "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           29,
           "benchmarks (Si et al., 2024). Expert knowledge integration. To effectively incorporate domain expert"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           30,
           "What Limits LLM-based Human Simulation: LLMs or Our Design? processes. Behavioral pattern monitoring"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           31,
           "verse behavioral patterns by systematically varying key pa- rameters such as personality traits, emo"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           32,
           "data evaluation. Iterative improvement through feedback loops. By im- plementing automated LLM evalu"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           33,
           "L., and Wang, R. Is cognition and action consistent or not: Investigating large language model’s per"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           34,
           "What Limits LLM-based Human Simulation: LLMs or Our Design? ceedings of the National Academy of Scie"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           35,
           "14th Annual Computing and Communication Workshop and Conference (CCWC), pp. 0375–0379. IEEE, 2024. C"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           36,
           "URL https://aclanthology. org/2024.emnlp-main.474/. Chen, Y., Liu, T. X., Shan, Y., and Zhong, S. Th"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           37,
           "lations. Journal of Artificial Societies and Social Simula- tion, 13(1):7, 2010. De La Torre, F., Fa"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           38,
           "guage models replace human participants? Trends in Cognitive Sciences, 27(7):597–600, 2023. Downing,"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           39,
           "guage models. arXiv preprint arXiv:2406.19283, 2024. Ferrara, E. Should chatgpt be biased? challenge"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           40,
           "Shen, Y., Ma, S., Liu, H., et al. A survey on llm-as-a- judge. arXiv preprint arXiv:2411.15594, 2024"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           41,
           "E., and Chang, B. Meet your favorite character: Open- domain chatbot mimicking fictional characters "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           42,
           "The online laboratory: Conducting experiments in a real labor market. Experimental economics, 14:399"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           43,
           "Language Models: Design and Development of a Com- putational Tool. PhD thesis, Massachusetts Institu"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           44,
           "academy of sciences, 99(suppl 3):7195–7196, 2002. Li, H., Yang, C., Zhang, A., Deng, Y., Wang, X., a"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           45,
           "empowered agents for simulating macroeconomic activi- ties. Available at SSRN 4606937, 2023a. Li, N."
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           46,
           "Cryptotrade: A reflective llm-based agent to guide zero- shot cryptocurrency trading. In Proceedings"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           47,
           "project life cycle perspective. ACM Computing Surveys, 57(4):1–38, 2024. Mao, H., Chen, Z., Tang, W."
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           48,
           "Orcutt, G. H., Caldwell, S., and Wertheimer, R. F. Policy exploration through microanalytic simulati"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           49,
           "P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior. In In the 36th "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           50,
           "Simu- lating counselor-client psychological counseling via role-playing llm-to-llm interactions. arX"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           51,
           "and Moor, M. Agentclinic: a multimodal agent bench- mark to evaluate ai in simulated clinical enviro"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           52,
           "USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp."
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           53,
           "for llms through multi-agent simulation. arXiv preprint arXiv:2410.14251, 2024. Tjuatja, L., Chen, V"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           54,
           "Peng, N. ” kelly is a warm person, joseph is a role model”: Gender biases in llm-generated reference"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           55,
           "simulate patients for training mental health professionals. arXiv preprint arXiv:2405.19660, 2024c. "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           56,
           "M., Tang, S., Peng, R., and Xiao, C. Shall we talk: Exploring spontaneous collaborations of competin"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           57,
           "for agents in embodied city environment. arXiv preprint arXiv:2312.11813, 2023a. Xu, Z., Shi, S., Hu"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           58,
           "Yang, Q., Wang, Z., Chen, H., Wang, S., Pu, Y., Gao, X., Huang, W., Song, S., and Huang, G. Psychoga"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           59,
           "mind: A survey of strategic reasoning with large language models. arXiv preprint arXiv:2404.01230, 2"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           60,
           "llm-as-a-judge with mt-bench and chatbot arena. Ad- vances in Neural Information Processing Systems,"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           61,
           "user simulators for conversational recommendation. In Companion Proceedings of the ACM on Web Confer"
          ],
          [
           "Asynchronous LLM Function Calling",
           0,
           "ASYNCHRONOUS LLM FUNCTION CALLING In Gim 1 Seung-seob Lee 1 Lin Zhong 1 ABSTRACT Large language mode"
          ],
          [
           "Asynchronous LLM Function Calling",
           1,
           "1 INTRODUCTION Function-calling capabilities enable large language mod- els (LLMs) to access externa"
          ],
          [
           "Asynchronous LLM Function Calling",
           2,
           "ing using compilers to parallelize function calls (Kim et al., 2023), fusing sequential calls to red"
          ],
          [
           "Asynchronous LLM Function Calling",
           3,
           "e.g., the LLM waiting for the function call executor to finish. We propose AsyncLM, a system that en"
          ],
          [
           "Asynchronous LLM Function Calling",
           4,
           "ing an interrupt halfway through an argument for another function call could disrupt both the ongoin"
          ],
          [
           "Asynchronous LLM Function Calling",
           5,
           "most efficient strategy to manage the blocking state, such as dropping, swapping, or retaining the K"
          ],
          [
           "Asynchronous LLM Function Calling",
           6,
           "prompting—where the model is guided through CML exam- ples and instructions in the input prompt (§5)"
          ],
          [
           "Asynchronous LLM Function Calling",
           7,
           "et al., 2022; Yao et al., 2023), autonomous agents (Huang et al., 2024; Wang et al., 2024), and neur"
          ],
          [
           "Asynchronous LLM Function Calling",
           8,
           "lization and reduce latency. Various studies have explored different optimization strategies to impr"
          ],
          [
           "Asynchronous LLM Function Calling",
           9,
           "Fetch Read html Read xls Read txt Send Send [EOS] [EOS] Send Read html Read xls Read txt (1) Synchro"
          ],
          [
           "Asynchronous LLM Function Calling",
           10,
           "which require a new session per function call. This has moti- vated research on efficient management"
          ],
          [
           "Asynchronous LLM Function Calling",
           11,
           "pendent tasks alongside ongoing function calls. To support asynchronous calls, AsyncLM introduces me"
          ],
          [
           "Asynchronous LLM Function Calling",
           12,
           "between the LLM and the executor, with its syntax ensuring that each component provides the necessar"
          ],
          [
           "Asynchronous LLM Function Calling",
           13,
           "text(‘533-3313’, ‘msg.txt’) Find local florists nearby and text  them to ask about the prices for  M"
          ],
          [
           "Asynchronous LLM Function Calling",
           14,
           "blocking the token stream, enabling implicit parallelism. For example, as shown in Figure 3, if the "
          ],
          [
           "Asynchronous LLM Function Calling",
           15,
           "3.2 Trigerring Interrupts When the executor completes a function call with a reg- istered identifier"
          ],
          [
           "Asynchronous LLM Function Calling",
           16,
           "critical section, implemented within the interrupt manager (§5.3), determines whether the executor c"
          ],
          [
           "Asynchronous LLM Function Calling",
           17,
           "helping generate training samples to fine-tune LLMs for asynchronous function handling (§4). They al"
          ],
          [
           "Asynchronous LLM Function Calling",
           18,
           "estimated function execution times. Specifically, the LLM must make decisions in the following areas"
          ],
          [
           "Asynchronous LLM Function Calling",
           19,
           "ing time among the options, the LLM is trained to call it next. For instance, in the task scheduling"
          ],
          [
           "Asynchronous LLM Function Calling",
           20,
           "branches to represent independent function calls. Multi-turn scenarios consist of multiple DAGs. Sim"
          ],
          [
           "Asynchronous LLM Function Calling",
           21,
           "inference. While some aspects of these components can be implemented on cloud-based LLM APIs without"
          ],
          [
           "Asynchronous LLM Function Calling",
           22,
           "call or trap is detected in the token stream, and (ii) to en- force CML syntax compliance. Implement"
          ],
          [
           "Asynchronous LLM Function Calling",
           23,
           "terrupts. Each function call runs on a dedicated worker, allowing multiple calls to execute concurre"
          ],
          [
           "Asynchronous LLM Function Calling",
           24,
           "Trap Handler The trap handler’s goal is to minimize idle KV cache usage in GPU memory without adding"
          ],
          [
           "Asynchronous LLM Function Calling",
           25,
           "ment it using OpenAI’s (streaming) chat completion API without modifying the serving system. The exe"
          ],
          [
           "Asynchronous LLM Function Calling",
           26,
           "function calls. Workloads. To evaluate task completion latency and func- tion calling accuracy, we u"
          ],
          [
           "Asynchronous LLM Function Calling",
           27,
           "AsyncLM setup. We consider two LLM deployment set- tings: local and cloud. In the local deployment, "
          ],
          [
           "Asynchronous LLM Function Calling",
           28,
           "ation and execution are interleaved. • Sync-Parallel: Parallel function calling (OpenAI, 2023; Kim e"
          ],
          [
           "Asynchronous LLM Function Calling",
           29,
           "“cheat sheet” with ground truth answers in the prompt. This ensures consistency in the generated fun"
          ],
          [
           "Asynchronous LLM Function Calling",
           30,
           "For evaluations on more complex setup under function call dependencies, we evaluate AsyncLM using th"
          ],
          [
           "Asynchronous LLM Function Calling",
           31,
           "Figure 6. Multi-step parallel function calling latencies. End-to- end task completion latency for si"
          ],
          [
           "Asynchronous LLM Function Calling",
           32,
           "calling. The total latency for Sync in simple parallel function calling can be modeled as: LSync(F) "
          ],
          [
           "Asynchronous LLM Function Calling",
           33,
           "worst case, assuming the number of generated tokens is the same and there are no additional overhead"
          ],
          [
           "Asynchronous LLM Function Calling",
           34,
           "parallel function calling. We prove that the LPT heuristic minimizes total latency in asynchronous p"
          ],
          [
           "Asynchronous LLM Function Calling",
           35,
           "Figure 7. The diagrams of the trap handling strategy that mini- mizes idle KV cache in memory withou"
          ],
          [
           "Asynchronous LLM Function Calling",
           36,
           "(TTFT) latency. On GPT-4o, Async-Naive suffers from 1.5× higher latency than Sync-Parallel (Figure 6"
          ],
          [
           "Asynchronous LLM Function Calling",
           37,
           "the optimal order is a – c – b. This challenge falls under Table 1. Average function composition acc"
          ],
          [
           "Asynchronous LLM Function Calling",
           38,
           "swapping and recomputation based on the context length. Figure 7 analyzes the optimal trap handling "
          ],
          [
           "Asynchronous LLM Function Calling",
           39,
           "Table 1 presents the results for GPT-4o and Llama models under these adaptation strategies. Impact o"
          ],
          [
           "Asynchronous LLM Function Calling",
           40,
           "a significant drop in accuracy when switching from Sync to Async. We attribute this to the limited i"
          ],
          [
           "Asynchronous LLM Function Calling",
           41,
           "interactions, where users can interrupt an ongoing LLM in- ference to add or adjust tasks without wa"
          ],
          [
           "Asynchronous LLM Function Calling",
           42,
           "these interrupts every 200 ms and measured the end-to-end task completion latency of Async, as shown"
          ],
          [
           "Asynchronous LLM Function Calling",
           43,
           "may introduce more complexity and require LLMs to learn when to chime in just like human conversatio"
          ],
          [
           "Asynchronous LLM Function Calling",
           44,
           "Y. Infercept: Efficient intercept support for augmented large language model inference. In Proc. ICM"
          ],
          [
           "Asynchronous LLM Function Calling",
           45,
           "et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. Gao, S., Chen, Y., and Shu, J. Fast s"
          ],
          [
           "Asynchronous LLM Function Calling",
           46,
           "ning of LLM agents: A survey. CoRR, abs/2402.02716, 2024. Hugging-Face. Text generation inference. h"
          ],
          [
           "Asynchronous LLM Function Calling",
           47,
           "tasks by connecting foundation models with millions of apis. Intelligent Computing, 2024. Lu, J., Ho"
          ],
          [
           "Asynchronous LLM Function Calling",
           48,
           "//platform.openai.com/docs/guides/fu nction-calling. Accessed: 2024-10-26. Pan, L., Albalak, A., Wan"
          ],
          [
           "Asynchronous LLM Function Calling",
           49,
           "Scialom, T. Toolformer: Language models can teach themselves to use tools. In Proc. NeurIPS, 2024. S"
          ],
          [
           "Asynchronous LLM Function Calling",
           50,
           "ing olympiad geometry without human demonstrations. Nature, 2024. 11 Wang, L., Ma, C., Feng, X., Zha"
          ],
          [
           "Asynchronous LLM Function Calling",
           51,
           "Xu, D. Rewoo: Decoupling reasoning from observa- tions for efficient augmented language models. CoRR"
          ],
          [
           "Asynchronous LLM Function Calling",
           52,
           "A benchmark for tool-agent-user interaction in real-world domains. CoRR, abs/2406.12045, 2024. Yu, L"
          ],
          [
           "Asynchronous LLM Function Calling",
           53,
           "Morency, L., Bisk, Y., Fried, D., Neubig, G., and Sap, M. SOTOPIA: interactive evaluation for social"
          ],
          [
           "Asynchronous LLM Function Calling",
           54,
           "Tokens are generated sequentially, functions are executed concurrently: LSync-Parallel(F) = X f∈F G("
          ],
          [
           "Asynchronous LLM Function Calling",
           55,
           "2. Establishing LAsync(F) ≤LSync-Parallel(F): Under the LPT heuristic, functions are ordered such th"
          ],
          [
           "Asynchronous LLM Function Calling",
           56,
           "order statistics, assuming E(f) follows a normal distribu- tion: X f∈F G(f) ≈nG, max f∈F E(f) ≈E + σ"
          ],
          [
           "Asynchronous LLM Function Calling",
           57,
           "Assume, for contradiction, that there exists a schedule σ′ that deviates from the LPT order and yiel"
          ],
          [
           "Asynchronous LLM Function Calling",
           58,
           "Thus, swapping fi and fj always reduces the maximum completion time. By repeatedly applying such swa"
          ],
          [
           "Multi-LLM Text Summarization",
           0,
           "Multi-LLM Text Summarization Jiangnan Fang1, Cheng-Tse Liu1, Jieun Kim1, Yash Bhedaru1, Ethan Liu1, "
          ],
          [
           "Multi-LLM Text Summarization",
           1,
           "for decentralized multi-LLM summarization. Overall, we find that our multi-LLM summa- rization appro"
          ],
          [
           "Multi-LLM Text Summarization",
           2,
           "and structures. Instead of relying solely on a single model or simple prompt-engineering methods, we"
          ],
          [
           "Multi-LLM Text Summarization",
           3,
           "with higher coherence, relevance, and factual ac- curacy, often rivaling or surpassing human-written"
          ],
          [
           "Multi-LLM Text Summarization",
           4,
           "et al. (2024), employed semantic clustering and multi-stage summarization with LLaMA2 to man- age le"
          ],
          [
           "Multi-LLM Text Summarization",
           5,
           "refine answers and explanations, achieving signifi- cant improvements over single-agent systems. Li "
          ],
          [
           "Multi-LLM Text Summarization",
           6,
           "els. We present two interaction topologies, cen- tralized and decentralized, to guide the collab- or"
          ],
          [
           "Multi-LLM Text Summarization",
           7,
           "evaluation step to select the best final summary. This is the initial process before we extend it to"
          ],
          [
           "Multi-LLM Text Summarization",
           8,
           "list of participating models M = {M1, . . . , Mk} independently generates a summary of the same inpu"
          ],
          [
           "Multi-LLM Text Summarization",
           9,
           "the central LLM’s evaluation of all candidate sum- maries. This includes the choice for the best sum"
          ],
          [
           "Multi-LLM Text Summarization",
           10,
           "Each LLM Mj generates an initial summary S(1) j from the original input text S using the prompt P: S"
          ],
          [
           "Multi-LLM Text Summarization",
           11,
           "E(i) = C(Pec, Si), If the confidence level meets the threshold, the process terminates, and the summ"
          ],
          [
           "Multi-LLM Text Summarization",
           12,
           "Algorithm 1 Centralized Multi-LLM Summary Require: ordered set S = {S1, . . . , Sm} of summaries, se"
          ],
          [
           "Multi-LLM Text Summarization",
           13,
           "around 160 words. Output the summary text only and nothing else. [text] Figure 2: Prompt for generat"
          ],
          [
           "Multi-LLM Text Summarization",
           14,
           "please evaluate the summaries and output the name of the agent that has the best summary. Output the"
          ],
          [
           "Multi-LLM Text Summarization",
           15,
           "marization approach that in addition to providing the best summary, it also outputs the confidence l"
          ],
          [
           "Multi-LLM Text Summarization",
           16,
           "k . These preferences are ag- gregated into a result vector r ∈1, . . . , kk, where each element rj "
          ],
          [
           "Multi-LLM Text Summarization",
           17,
           "experiment Appendix C.1. 5.2 Conversational The conversational approach extends the decentral- ized "
          ],
          [
           "Multi-LLM Text Summarization",
           18,
           "(e.g., Figure 2), evaluation prompt Pe (e.g., Figure 4) Ensure: summary S∗of the text 1: S = CREATES"
          ],
          [
           "Multi-LLM Text Summarization",
           19,
           "rounds with new generation prompts. Formally, let E(i) j represent model Mj’s choice in round i. In "
          ],
          [
           "Multi-LLM Text Summarization",
           20,
           "(Huang et al., 2021) to evaluate our summarization methods. We assess the quality of LLM-generated s"
          ],
          [
           "Multi-LLM Text Summarization",
           21,
           "performs baselines by 70%. In our theoretical cost analysis (Section B.1 and B.2) we show that the i"
          ],
          [
           "Multi-LLM Text Summarization",
           22,
           "work outperformed single-LLM baselines, averag- ing 64% improvement for the decentralized vari- ant "
          ],
          [
           "Multi-LLM Text Summarization",
           23,
           "0.043 0.468 0.190 0.477 0.112 Centralized Multi-LLM 3 round max 0.329 0.168 0.217 0.031 0.468 0.189 "
          ],
          [
           "Multi-LLM Text Summarization",
           24,
           "els contributing to the summarization and eval- uation, and with models receiving fine-grained promp"
          ],
          [
           "Multi-LLM Text Summarization",
           25,
           "average over single-LLM summaries, and for indi- vidual scores we see improvements of up to 2.9×. Mo"
          ],
          [
           "Multi-LLM Text Summarization",
           26,
           "ously. The results in Table 8 shows that the central- ized approach provides the most performance ga"
          ],
          [
           "Multi-LLM Text Summarization",
           27,
           "0.339 0.180 0.224 0.043 GPT-4o & GPT-3.5 0.328 0.170 0.212 0.033 GPT-4o & GPT-4o mini 0.305 0.153 0."
          ],
          [
           "Multi-LLM Text Summarization",
           28,
           "Underlined numbers are overall best scores for each metric in this table. Furthermore, the central L"
          ],
          [
           "Multi-LLM Text Summarization",
           29,
           "eration phase (Section 4.1.1), and are instructed to evaluate these two sets of summaries for Cohere"
          ],
          [
           "Multi-LLM Text Summarization",
           30,
           "marization. Future work should continue to investi- gate multi-LLM approaches for summarization. 8 L"
          ],
          [
           "Multi-LLM Text Summarization",
           31,
           "2020. Etc: Encoding long and structured inputs in transformers. Lochan Basyal and Mihir Sanghvi. 202"
          ],
          [
           "Multi-LLM Text Summarization",
           32,
           "odkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, "
          ],
          [
           "Multi-LLM Text Summarization",
           33,
           "and Mausam. 2014. Hierarchical summarization: Scaling up multi-document summarization. In Pro- ceedi"
          ],
          [
           "Multi-LLM Text Summarization",
           34,
           "ciation for Computational Linguistics. Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, a"
          ],
          [
           "Multi-LLM Text Summarization",
           35,
           "and Phil Blunsom. 2015. Teaching machines to read and comprehend. Luyang Huang, Shuyang Cao, Nikolau"
          ],
          [
           "Multi-LLM Text Summarization",
           36,
           "Veselin Stoyanov, and Luke Zettlemoyer. 2020. Bart: Denoising sequence-to-sequence pre-training for "
          ],
          [
           "Multi-LLM Text Summarization",
           37,
           "jape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language mod"
          ],
          [
           "Multi-LLM Text Summarization",
           38,
           "Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don’t give me the details, just the summary"
          ],
          [
           "Multi-LLM Text Summarization",
           39,
           "Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, De- bajy"
          ],
          [
           "Multi-LLM Text Summarization",
           40,
           "janowski, and Armand Joulin. 2019. Adaptive at- tention span in transformers. In Proceedings of the "
          ],
          [
           "Multi-LLM Text Summarization",
           41,
           "2023. Benchmarking large language models for news summarization. A Detailed Experimental Setup Datas"
          ],
          [
           "Multi-LLM Text Summarization",
           42,
           "ROUGE scores emphasizes recall while BLEU scores emphasize precision. Baselines: For comparison with"
          ],
          [
           "Multi-LLM Text Summarization",
           43,
           "that round (e.g., references to previously generated summaries). Each LLM then produces up to Omax o"
          ],
          [
           "Multi-LLM Text Summarization",
           44,
           "160-word summary). Moreover, careful prompt engineering can curtail δi growth, ensuring that the num"
          ],
          [
           "Multi-LLM Text Summarization",
           45,
           "Thus, for tmax rounds, the overall complexity be- comes: O(tmax · (k · I + k · Ie + k · Omax + k2 · "
          ],
          [
           "Multi-LLM Text Summarization",
           46,
           "emerges quickly, the number of rounds tmax can be effectively reduced, thereby decreasing the total "
          ],
          [
           "Multi-LLM Text Summarization",
           47,
           "consistent with previous methodology, we modify the process here so that GPT-4o receives the final- "
          ],
          [
           "Multi-LLM Text Summarization",
           48,
           "choice. Furthermore, GPT-3.5 as a centralized evaluator and tie-breaking choice separately out- perf"
          ],
          [
           "Multi-LLM Text Summarization",
           49,
           "a member and the evaluator/default choice offer larger improvements compared to those without GPT-3."
          ],
          [
           "Multi-LLM Text Summarization",
           50,
           "propagation of noise or redundancy in intermediate summaries. This added complexity could dilute the"
          ],
          [
           "Multi-LLM Text Summarization",
           51,
           "sential information from each chunk. With this, we propose a form of specialized prompting as a way "
          ],
          [
           "Multi-LLM Text Summarization",
           52,
           "summaries, as well as their inclusion as input, mir- ror the procedures used to obtain decentralized"
          ],
          [
           "Multi-LLM Text Summarization",
           53,
           "0.201 0.027 0.441 0.176 0.447 0.092 Multi-LLM 1 round max 0.330 0.165 0.222 0.028 0.439 0.175 0.446 "
          ],
          [
           "Multi-LLM Text Summarization",
           54,
           "0.456 0.183 0.461 0.100 Centralized Multi-LLM 3 round max 0.318 0.162 0.206 0.027 0.449 0.181 0.452 "
          ],
          [
           "Multi-LLM Text Summarization",
           55,
           "0.043 0.468 0.190 0.477 0.112 Centralized 3 rounds 0.329 0.168 0.217 0.031 0.468 0.189 0.470 0.109 1"
          ],
          [
           "Multi-LLM Text Summarization",
           56,
           "0.027 0.451 0.182 0.459 0.099 Centralized 3 rounds 0.294 0.151 0.177 0.023 0.451 0.181 0.440 0.095 1"
          ],
          [
           "Multi-LLM Text Summarization",
           57,
           "0.109 1 round max 0.333 0.173 0.219 0.036 0.479 0.197 0.485 0.121 Specialized Prompts Decentralized "
          ],
          [
           "Multi-LLM Text Summarization",
           58,
           "experimental variables, and we underline the best results overall. Generate a summary that maximizes"
          ],
          [
           "Multi-LLM Text Summarization",
           59,
           "contains little to no explicit markers for section identification. Thus, we present a simple heurist"
          ],
          [
           "Multi-LLM Text Summarization",
           60,
           "plague LLMs in terms of capturing and summariz- ing relevant content. The similar performance on met"
          ],
          [
           "Multi-LLM Text Summarization",
           61,
           "framework (the best-performing setup for ArXiv; see Table 2), and prompt human raters to rate each s"
          ],
          [
           "Multi-LLM Text Summarization",
           62,
           "0.313 0.163 0.200 0.029 Multi-LLM 1 round max 0.338 0.180 0.224 0.043 Short Text Decentralized Multi"
          ],
          [
           "Multi-LLM Text Summarization",
           63,
           "erage the rating for each model and each summary in each evaluation criterion (Table 9). Thus, for e"
          ],
          [
           "Multi-LLM Text Summarization",
           64,
           "3.57 4.42 4.28 3.95 3.80 2 4.28 4.00 4.42 3.85 4.85 4.85 4.52 4.23 3 3.42 4.57 3.57 4.42 3.85 4.57 3"
          ],
          [
           "Multi-LLM Text Summarization",
           65,
           "5-point Likert scale. We additionally show the score averaged from the scores for the three criteria"
          ],
          [
           "Multi-LLM Text Summarization",
           66,
           "GPT-4o mini 6 GPT-4o mini GPT-4o mini GPT-3.5 GPT-4o mini GPT-4o mini 7 GPT-4o mini GPT-3.5 GPT-4o m"
          ],
          [
           "Multi-LLM Text Summarization",
           67,
           "bottom row we show the inter-rater agreement (as measured by Cohen’s kappa) between the human choice"
          ],
          [
           "Multi-LLM Text Summarization",
           68,
           "3: Main ideas connect logically with each other. A few arguments/points are out of order. 4: Most id"
          ],
          [
           "Multi-LLM Text Summarization",
           69,
           "3: The text is generally grammatically correct. Some sentences may have incorrect grammar. 4: The te"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           0,
           "arXiv:2505.20921v2  [cs.CL]  29 May 2025 Automatic Transmission for LLM Tiers: Optimizing Cost and A"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           1,
           "lowing the selection of a suitable initial tier without training. Given an input, accuracy esti- mat"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           2,
           "Generator, and Judge, respectively. are solved by dividing them into multiple sub- tasks, each requi"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           3,
           "from the training data distribution, limiting their generalizability in real-world applications. To "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           4,
           "mulated inference records, referred to as History, without requiring correctness-labeled data. This "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           5,
           "et al. (2024); Ong et al. (2024) formulate routing as a binary classification problem, directing sim"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           6,
           "pose an iterative verify-then-correct framework that enhances model reliability by using external to"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           7,
           "2023) and DeepSeek (DeepSeek, 2025) etc. 3.1 Overview Figure 2 shows an overview of the proposed LLM"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           8,
           "and GPT-4o-mini. 3.2 Generator Rather than focusing on enhancing the performance of a single model, "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           9,
           "the generator and receives the question and the an- swer of the generator, evaluates its validity an"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           10,
           "starter, which starts from an appropriate tier for a given question rather than always beginning fro"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           11,
           "Pseudo-labeling of Correctness To estimate the accuracy of each tier, we rely on pre- vious inferenc"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           12,
           "would require significant labeled data, as well as additional training and usage costs. To avoid the"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           13,
           "αT = λ·AccBench, αF = λ·(1−AccBench) (2) where λ is a hyperparameter to adjust the impor- tance of t"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           14,
           "question to evaluate whether LLM-AT effectively selects the LLM tier while optimizing trade-offs bet"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           15,
           "Model Tier Price ($/1M tokens) Input Output o1 1 15.00 60.00 o1-mini 2 3.00 12.00 GPT-4o 3 2.50 10.0"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           16,
           "which is one tier higher than GPT-4o-mini, to avoid overestimation and ensure evaluation accuracy. 4"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           17,
           "that comprise MATH and MCQA. 4.5 Implementation Details of LLM-AT We apply a few-shot PoT prompt to "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           18,
           "5 Experimental Results 5.1 Main Results Figure 3 shows the trade-offs between accuracy and total exe"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           19,
           "reduces cost by 59.37% ($41.56 →$16.89) with 2https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B- ins"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           20,
           "o1 0.5 0.6 0.7 0.8 0.9 Accuracy MATH Threshold 0.7 Median Line Actual Accuracy GPT-4o-mini GPT-4o o1"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           21,
           "mator, we compare the estimated accuracy with the actual accuracy obtained through single inferences"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           22,
           "0.71 4.41 25.28 Q2 (q101– q200) 0.79 4.81 23.70 Q3 (q201– q300) 0.75 3.73 20.44 Q4 (q301– q400) 0.86"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           23,
           "sult demonstrates that, even without a correctness label, it is possible to estimate the accuracy of"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           24,
           "tion of the judge. Based on the reliable performance of the judge, we consider that the pseudo-label"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           25,
           "question. Even when historical data changes with every input, LLM-AT maintains a performance com- pa"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           26,
           "0.125 0.117 0.088 0.088 -0.032 Proportion GPT-4o 0.333 0.482 0.281 0.549 0.750 0.328 0.526 o1-mini 0"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           27,
           "accuracy difference narrows, and in the Number Theory, GPT-4o even outperforms o1-mini. Reflect- ing"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           28,
           "if LLM-AT requires more time and cost than using a single LLM, the benefits of the hierarchical ap- "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           29,
           "more cost-effective performance compared to us- ing a single LLM. This suggests that the starter eff"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           30,
           "6 Conclusion We introduce LLM-AT, a novel framework that auto- matically selects LLM tiers to maximi"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           31,
           "defined answers. As a future direction, it would be important to extend the LLM-AT system to open- e"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           32,
           "idya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappagan-"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           33,
           "Machine Learning Research. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, C"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           34,
           "pers), pages 14664–14690, Bangkok, Thailand. As- sociation for Computational Linguistics. Dayuan Fu,"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           35,
           "Jacob Steinhardt. 2021b. Measuring mathematical problem solving with the math dataset. NeurIPS. Jie "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           36,
           "semble of large language models. In Proceedings of the 2024 Conference of the North American Chap- t"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           37,
           "tricity? a new dataset for open book question an- swering. In Proceedings of the 2018 Conference on "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           38,
           "Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. arXiv preprint "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           39,
           "2024. Triad: A framework leveraging a multi-role LLM-based agent to solve knowledge base question an"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           40,
           "MCQA LLM-AT Single Iteration Accuracy Figure 6: Results when using benchmark performance specialized"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           41,
           "approach compared to RouteLLM (Ong et al., 2024), a training-based routing method. RouteLLM uses a b"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           42,
           "iments, the top-tier model o1 is designated as the strong model and each lower-tier model is set as "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           43,
           "achieving lower performance compared to single- model inference. These results demonstrate that the "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           44,
           "the right plot, when the abstention option is en- abled(w/o Starter), we observe that the cost in- c"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           45,
           "OpenBook ARC Easy ARC Challenge MMLU GPQA 0 100 200 300 400 500 Counts Tier 3: GPT-4o OpenBook ARC E"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           46,
           "ficult questions rather than doing so at random. To analyze this, we had the model re-answer the pre"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           47,
           "D Qualitative Examples We provide examples from MATH and MCQA to illustrate how LLM-AT works in Tabl"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           48,
           "If there is even the slightest doubt about solving the question correctly, output only “Abstain” and"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           49,
           "x1, y1, z1 = 2, 1, -4 x2, y2, z2 = 5, 8, -3 distance_squared = (x2 - x1)**2 + (y2 - y1)**2 + (z2 - z"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           50,
           "by the answer. The final items you need to output are Code: code should be here Please output only t"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           51,
           "count_zeros = 0 while product % 10 == 0: count_zeros += 1 product /= 10 return count_zeros num_zeros"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           52,
           "the process of deriving the correct answer to the question. 2. Assess whether the answer generated b"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           53,
           "male_leads = math.comb(4, 1) objects_roles = math.comb(6, 3) total_ways = female_leads * male_leads "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           54,
           "First, we use the formula for the interplanar distance in a rhombohedral lattice, given by dhkl = a "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           55,
           "10 √ 1+1+1+2(1+1+1) cos(30◦) d111 = 10 √3+6×0.8660 d111 ≈ 10 2.863 ≈3.49 Å. However, considering cry"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           0,
           "Published at Building Trust Workshop at ICLR 2025 SYSTEMATIC EVALUATION OF LLM-AS-A-JUDGE IN LLM ALI"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           1,
           "rithms. In this work, we systematically evaluate LLM-as-a-Judge on alignment tasks by defining more "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           2,
           "pairwise evaluation on numerous LLM alignment tasks, such as summarization and multiturn con- versat"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           3,
           "Wang et al., 2023c; Li et al., 2023; Zheng et al., 2024; Li et al., 2024c; Chiang et al., 2024; Dubo"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           4,
           "position and length bias, by (1) defining them within a unified accuracy-based framework, (2) explic"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           5,
           "ated by separate LLMs, the human judge is asked to select the better response based on predefined cr"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           6,
           "top-k, which generate non-deterministic outputs. The non-deterministic level is controlled by the te"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           7,
           "shorter versions. Saito et al. (2023) observed a discrepancy between LLMs and human preferences rega"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           8,
           "y(n) r is the less preferred response, both by human evaluators. We assume each case is drawn from t"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           9,
           "result sn = (y(n), y′(n)) represents the selection outcome from both response orders across all the "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           10,
           "the same decision (e.g., selecting yc with X =1). However, if we consider self-inconsistency, the LL"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           11,
           "where q is the probability that the LLM judge’s decision is flipped. For a completely deterministic "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           12,
           "This implies that accuracy should be invariant regarding response positions: p [X = 1|(yc, yr)] − p "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           13,
           "Finally, we address the general case in which the LLM-judge makes non-deterministic decisions and ex"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           14,
           "the position bias measurement based on de-noised accuracies, and provide a practical method for thei"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           15,
           "the same accuracy framework, analogous to the definition of position bias. Following the same ration"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           16,
           "1 −2 · q∆l≤0 , q∆l≤0 = p [1 −X|X, ∆l≤0] where q∆l>0 and q∆l≤0 are the probabilities that the LLM jud"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           17,
           "fined or user-customized LLM judges for alignment tasks based on aforementioned evaluation metrics a"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           18,
           "5 Published at Building Trust Workshop at ICLR 2025 outcome for metrics computation. This module all"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           19,
           "Also, human preference labels are available to indicate which response is more aligned with human pr"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           20,
           "without replacement, resulting in five non-overlapping splits. Since measuring length bias requires "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           21,
           "mini is the most cost-efficient model, while GPT-3.5-turbo is from the last OpenAI model generation "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           22,
           "the templates in this study are derived. Temperature Parameter Selection Temperature parameter deter"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           23,
           "templates from DPO paper (Rafailov et al., 2024b) are utilized for both datasets. Metrics Computatio"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           24,
           "temperatures. The self-consistent rate, given by 1 −q as defined in Eq. 1, measures the probability "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           25,
           "SCR (yr, yc) Acc (Accboth) SCR (yc, yr) SCR (yr, yc) Acc (Accboth) 0.0 0.977 0.971 0.665 (0.003) 0.9"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           26,
           "summarization and HH-RLHF-Helpfulness datasets. Results are demonstrated using GPT-4o and prompt tem"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           27,
           "which prompt template is used. It demonstrates that the superior internal capacities of recent LLMs,"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           28,
           "all the result figures throughout this section. Position Bias Position biases of all the LLM judges "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           29,
           "same context. (a) TL;DR (b) HH-RLHF (c) TL;DR (d) HH-RLHF Figure 3: Position bias (top two) and leng"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           30,
           "which is consistent with previous studies (Zheng et al., 2024; Saito et al., 2023). Furthermore, com"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           31,
           "Second, our evaluation studies concentrate on LLM-as-a-Judge methods, although open-source re- ward "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           32,
           "bias. We developed a framework to evaluate, compare, and visualize the reliability of LLM judges and"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           33,
           "arXiv preprint arXiv:2305.10403, 2023. 10 Published at Building Trust Workshop at ICLR 2025 Yuntao B"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           34,
           "arXiv preprint arXiv:2311.08045, 2023. Cheng-Han Chiang and Hung-yi Lee. Can large language models b"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           35,
           "Yann Dubois, Bal´azs Galambosi, Percy Liang, and Tatsunori B Hashimoto. Length-controlled al- pacaev"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           36,
           "Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online ai fe"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           37,
           "Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feed"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           38,
           "for evaluating alignment. arXiv preprint arXiv:2310.05470, 2023. Tianle Li, Wei-Lin Chiang, Evan Fri"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           39,
           "Neiswanger. Sample efficient reinforcement learning from human feedback via active exploration. 2023"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           40,
           "arXiv:2303.16755, 2023. Wei Shen, Xiaoying Zhang, Yuanshun Yao, Rui Zheng, Hongyi Guo, and Yang Liu."
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           41,
           "Dieuwke Hupkes. Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges. arXi"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           42,
           "arXiv preprint arXiv:2401.06080, 2024a. Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxia"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           43,
           "Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer2: Open-source dataset for training top-pe"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           44,
           "Zenglin Xu. On diversified preferences of large language model alignment, 2024. Lianmin Zheng, Wei-L"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           45,
           "02/2024 wang (Wang et al., 2024a) https://arxiv.org/pdf/2401.06080 01/2024 zheng (Zheng et al., 2024"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           46,
           "https://arxiv.org/pdf/2403.07708v2 02/2024 guo (Guo et al., 2024) https://arxiv.org/pdf/2402.04792 0"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           47,
           "Examples of Prompt Templates (TL;DR Summarization Dataset) Template from Rafailov et al. Rafailov et"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           48,
           "of the responses influence your evaluation. Do not favor certain names of the assistants. Strive to "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           49,
           "the user’s questions. A helpful response should directly address the human questions without going o"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           50,
           "--The End of Assistant A’s Answer-- --The Start of Assistant B’s Answer-- {response 2} --The End of "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           51,
           "insight or heard of anything that could cause them to do this, and it’s very uncharacteristic of the"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           52,
           "and the school administration\" Rejected Summary by Human Evaluators: \"My parents found out my girlfr"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           53,
           "Do you know the name of the singer? Assistant: Oh yeah, I know who sang \"Smooth\". He’s a great singe"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           54,
           "+ template) for each dataset. We display top five templates or LLM-judges and report their Accboth, "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           55,
           "0.5 0.954 0.951 0.655 (0.003) 0.942 0.926 0.579 (0.009) 0.7 0.939 0.941 0.650 (0.004) 0.924 0.916 0."
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           56,
           "SCR (yr, yc) Acc (Accboth) 0.0 0.989 0.991 0.631 (0.001) 0.987 0.990 0.589 (0.003) 0.1 0.986 0.985 0"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           57,
           "TL;DR Summarization HH-RLHF-Helpfulness Temperature SCR (yc, yr) SCR (yr, yc) Acc (Accboth) SCR (yc,"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           58,
           "Table 6: Self-consistent rate (SCR) and accuracy (Acc) related to the tested temperatures for the TL"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           59,
           "-0.047 (0.017) 0.174 (0.039) rafailov 0.547 (0.022) 0.668 (0.040) 0.049 (0.018) 0.152 (0.045) chen 0"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           60,
           "0.197 (0.031) chen 0.658 (0.028) 0.734 (0.029) -0.081 (0.023) 0.117 (0.055) guo 0.655 (0.011) 0.733 "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           61,
           "0.619 (0.032) 0.715 (0.010) 0.090 (0.036) 0.257 (0.068) chen 0.615 (0.021) 0.692 (0.031) 0.010 (0.01"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           62,
           "To prove this, we analyze two separate conditions: (1) the LLM judge prefers the first position, (2)"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           63,
           "0.197 (0.031) chen / gpt-4o 0.658 (0.028) 0.734 (0.029) -0.081 (0.023) 0.117 (0.055) guo / gpt-4o 0."
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           64,
           "0.654 (0.023) 0.013 (0.036) 0.531 (0.044) guo 0.506 (0.025) 0.651 (0.016) 0.029 (0.060) 0.280 (0.062"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           65,
           "-0.005 (0.013) 0.135 (0.075) xu 0.610 (0.025) 0.702 (0.019) 0.086 (0.010) 0.029 (0.057) bai 0.603 (0"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           66,
           "0.294 (0.059) rafailov 0.594 (0.014) 0.657 (0.019) 0.047 (0.020) 0.463 (0.039) zeng 0.587 (0.031) 0."
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           67,
           "0.135 (0.075) xu / gpt-4o 0.610 (0.025) 0.702 (0.019) 0.086 (0.010) 0.029 (0.057) bai / gpt-4o 0.603"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           68,
           "15 presents all possible combinations of outcomes resulting from the LLM-judge’s decisions, where ✓a"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           69,
           "alone would only result in the LLM selecting a consistent response (either yc or yr, not both) acros"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           70,
           "1 N N X n=1 [1(✓✗✗✓)+1 (✓✗✓✗)]−lim N→∞ 1 N N X n=1 1(✓✗✗✓) = lim N→∞ 1 N N X n=1 1(✓✗✓✗) = lim N→∞ 1"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           71,
           "the second position. In this context, we can employ the same analytical approach as in the first cas"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           72,
           "the (a) first and (b) second responses, respectively. Here, ✓and ✗indicate whether a response (yc or"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           73,
           "{ p [X|X, (yc, yr)] · p [X = 1|(yc, yr)] + p [1 −X|X, (yc, yr)] · p [X = 0|(yc, yr)] | {z } X is fli"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           74,
           "1 −2 · qrc 3) Position bias computation procedure Given a dataset D = {hn|n = 1. . .N}, a practical "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           75,
           "ˆp [Z =1|(yc, yr)]= 1 N N X n=1 1 \u0010 y(n) =y(n) c \u0011 , ˆp [Z =1|(yr, yc)]= 1 N N X n=1 1 \u0010 y′(n) =y(n)"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           76,
           "X n=1 ( k(n) rc K · k(n) rc −1 K−1 + K−k(n) rc K · K−k(n) rc −1 K−1 ) . where k(n) cr = PK k=1 1(y(n"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           77,
           "0. Let LBcr and LBrc be length biases measured for response order (yc, yr) and (yr, yc) in all the d"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           78,
           "with PB in its measurement. In the next part, we introduce a method to approximate accuracies p [X ="
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           79,
           "When Accrandom is used for accuracy, it depends on the proportion of both the first and the third ca"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           80,
           "of J : J∆l>0 = {s|∆l > 0, s ∈J } and J∆l≤0 = {s|∆l ≤0, s ∈J }. The accuracy based on Z can 24 Publis"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           81,
           "∆l>0 = {s′|∆l > 0, s′ ∈J ′} and J ′ ∆l≤0 = {s′|∆l ≤0, s′ ∈J ′}. The flipping probabilities q∆l>0 and"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           0,
           "Published as a conference paper at ICLR 2024 TEST: TEXT PROTOTYPE ALIGNED EMBEDDING TO ACTIVATE LLM’"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           1,
           "enable the pre-trained LLM to handle TS data. Given the lack of data, limited resources, semantic co"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           2,
           "this study will serve as a foundation for future work to support TS+LLM progress. 1 INTRODUCTION Imp"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           3,
           "∗Corresponding authors 1This categorization focuses on the requirement for changing the model. But f"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           4,
           "than that for NLP Zhou et al. (2023); TS-for-LLM methods can use a relatively small dataset as its o"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           5,
           "signs, and laboratory values, such as heart rate, lactic acid, etc., need to be included when diagno"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           6,
           "image embedding through text descriptions of the image Wang et al. (2023). However, TS lacks visual "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           7,
           "prototypes to constrain TS’ embedding space and highlights feature-wise patterns. We show that TEST "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           8,
           "GPT4TSZhou et al. (2023) accurate lose language ability LLM4TSChang et al. (2023) Tool augmented Par"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           9,
           "TS data, but it can be difficult to construct a large well-labeled dataset due to data acquisition a"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           10,
           "embedding space. Some efforts have been made to implement instance-level contrast Woo et al. (2022b)"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           11,
           "make the LLM can accept TS embeddings as input. 3 Published as a conference paper at ICLR 2024 Encod"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           12,
           "k=1 = fe(s) = fe(fs(x)). We first tokenize TS into some segmentation/subsequences/tokens/instances t"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           13,
           "built without harming the future process. 3.2 INSTANCE-WISE AND FEATURE-WISE CONTRAST The basic inst"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           14,
           "4 Published as a conference paper at ICLR 2024 We also propose a feature-wise contrast method to bre"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           15,
           "the positive and negative. However, this may cause the representation space to shrink within a small"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           16,
           "Naively, we can align the token embedding of TS and text using the similarity estimation. Although T"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           17,
           "text prototype can be relaxed, not necessarily the description related to TS. In this work, we choos"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           18,
           "still has to be instructed on how to do subsequent TS tasks. Token Encoder Language model TS embeddi"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           19,
           "Consider a conditional generation task where the input x is a context and the output y is a sequence"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           20,
           "| {z } Frozen LLM ) · X i∈peidx log p∆(δzi|h<i) | {z } Prompt peθ (5) Equation 5 also suggests that "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           21,
           "channel whose size is the same as the LLM’s embedding size. 6 Published as a conference paper at ICL"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           22,
           "Table 2: The Used Language Model The used LLMs are as listed in Table 2. Each encoder and soft promp"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           23,
           "Franceschi et al. (2019b), TS2Vec Yue et al. (2022), and CoST Woo et al. (2022a). The overall result"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           24,
           "univariate TS and 25% for multivariate TS. TEST makes most LLMs comparable to, if not better than, t"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           25,
           "4.2 FORECASTING We present short-forecasting MSE scores for all 19 kinds of varied time series datas"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           26,
           "𝑎 𝑖 𝑑 𝑔 LLM+TEST (ours) Classical SOTA models QA CL models UCR classification accuracy  using repres"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           27,
           "White important change loop happy actively limit finally Figure 4: Matching TS Embedding to Words Re"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           28,
           "representing TS through a series of patterns. Instead of regarding TS as a sequence of numbers, we s"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           29,
           "impression of a forcibly aligning operations between TS and text, it dose convert TS into an under- "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           30,
           "ACKNOWLEDGMENTS This work is supported by National Natural Science Foundation of China (No.62172018,"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           31,
           "2018. Eoin Brophy, Zhengwei Wang, Qi She, and Tom´as Ward. Generative adversarial networks in time s"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           32,
           "forecasting with pre-trained llms. CoRR, abs/2308.08469, 2023. Daoyuan Chen, Yilun Huang, and et al."
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           33,
           "Angus Dempster, Daniel F. Schmidt, and Geoffrey I. Webb. Minirocket: A very fast (almost) de- termin"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           34,
           "Cuntai Guan. Time-series representation learning via temporal and contextual contrasting. In Proceed"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           35,
           "tern mining framework for multivariate time series classification. In Proceedings of International J"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           36,
           "unsupervised visual representation learning. In Computer Vision and Pattern Recognition, pp. 9726–97"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           37,
           "2310.01728. Fazle Karim, Somshubra Majumdar, Houshang Darabi, and Samuel Harford. Multivariate lstm-"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           38,
           "Hung Wong. Shapenet: A shapelet-neural network approach for multivariate time series classification."
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           39,
           "few-shot health learners. CoRR, abs/2305.15525, 2023. doi: 10.48550/arXiv.2305.15525. Yong Liu, Haix"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           40,
           "masked hierarchical cluster-wise contrastive learning for multivariate time series. In AAAI Con- fer"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           41,
           "machines. In Conference on Robot Learning, volume 229 of Proceedings of Machine Learning Research, p"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           42,
           "contrastive learning for improving face representations. In IEEE International Conference on Au- tom"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           43,
           "time series with temporal neighborhood coding. In International Conference on Learning Repre- sentat"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           44,
           "Wetterstation. Weather. 2017. doi: https://www.bgc-jena.mpg.de/wetter/. Kristoffer Wickstrøm, Michae"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           45,
           "nential smoothing transformers for time-series forecasting. CoRR, abs/2202.01381, 2022c. Haixu Wu, J"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           46,
           "framework for univariate time series representation. Knowl. Based Syst., 245:108606, 2022. Jinsung Y"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           47,
           "A transformer-based framework for multivariate time series representation learning. In ACM SIGKDD Co"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           48,
           "Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji- Rong W"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           49,
           "on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 27268– 27286, 2022."
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           50,
           "data, without the requirement of annotating every sample. Enabling URL is extremely crucial for time"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           51,
           "the implicit semantics shared by samples in the same cluster. They can address the limitation that i"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           52,
           "Unable to measure feature relations SimMTM Dong et al. (2023) Adversarial Eliminate the need for exp"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           53,
           "SLIC Khorasgani et al. (2022) MHCCL Meng et al. (2022) Temporal-level TS2Vec Yue et al. (2022) TS-TC"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           54,
           "Text2ECGChung et al. (2023) External Encoder Parameter-efficient, Weak robust TEST multiple abilitie"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           55,
           "ENCODER The core of TEST is to train an encoder and a soft prompt. The encoder must can extract rele"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           56,
           "Figure 6: Illustration of Three Stacked Dilated Causal Convolutions and Composition of the i-th Laye"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           57,
           "shot learning, mean absolute percentage error (MAPE) is used for TOURISM; symmetric MAPE (sMAPE) is "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           58,
           "ILI is not used for few-shot learning for the limited quantity that is hard to follow the definition"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           59,
           "The details of zero-shot forecasting datasets are: M4 is a large and diverse dataset that contains t"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           60,
           "For few-shot forecasting, we refor to the SOTA methods reported in Zhou et al. (2023): DLinear Zeng "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           61,
           "Methods TEST GPT4TS TimesNet ETSformer DLinear FEDformer Informer TCN LSTM ETTm1 96 0.293 0.346 0.29"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           62,
           "1.166 0.823 1.153 0.820 1.324 0.858 Avg 0.353 0.382 0.352 0.383 0.350 0.406 0.429 0.425 0.403 0.407 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           63,
           "0.459 0.465 1.107 0.809 1.238 0.932 1.259 0.841 720 0.447 0.467 0.477 0.456 0.521 0.500 0.562 0.535 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           64,
           "5.602 1.931 4.315 1.635 3.312 1.384 336 0.329 0.381 0.373 0.407 0.452 0.452 0.485 0.559 0.594 0.541 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           65,
           "0.375 0.437 192 0.158 0.241 0.153 0.251 0.184 0.239 0.199 0.196 0.285 0.201 0.315 0.296 0.386 0.266 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           66,
           "0.650 0.396 0.587 0.366 0.719 0.391 0.684 0.384 0.843 0.453 192 0.423 0.287 0.407 0.290 0.617 0.336 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           67,
           "0.625 0.383 0.610 0.376 0.764 0.416 0.705 0.395 1.011 0.541 Weather 96 0.150 0.202 0.162 0.212 0.152"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           68,
           "0.535 0.520 Avg 0.229 0.271 0.237 0.270 0.236 0.287 0.271 0.334 0.265 0.317 0.309 0.360 0.634 0.548 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           69,
           "4.800 1.468 6.736 1.857 60 2.425 1.203 1.979 0.957 2.027 0.928 2.487 1.016 2.804 1.146 2.857 1.15 5."
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           70,
           "(ICLR 2023). The results are shown in Table 9. Overall, TEST achieves comparable performance to SOTA"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           71,
           "0.230 0.263 0.210 0.254 0.215 0.263 0.210 0.257 0.245 0.283 0.250 0.304 0.270 0.322 0.269 0.295 0.27"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           72,
           "0.284 0.324 0.300 0.342 0.318 0.323 0.318 0.360 0.289 0.322 0.597 0.495 0.546 0.469 ETTh1 96 0.455 0"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           73,
           "0.691 0.574 0.750 0.619 0.939 0.644 1.179 0.832 1.347 0.870 1.202 0.811 1.294 0.854 720 0.723 0.594 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           74,
           "0.378 0.409 0.382 0.416 0.413 0.451 0.389 0.411 0.678 0.619 2.022 1.006 3.837 1.508 3.788 1.533 192 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           75,
           "0.477 0.480 0.510 0.491 0.499 0.509 0.516 0.523 0.477 0.472 1.273 0.874 3.816 1.407 3.842 1.503 3.20"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           76,
           "0.382 0.412 0.437 0.434 0.630 0.528 0.617 0.546 0.754 0.592 0.781 0.574 0.955 0.703 0.957 0.701 1.17"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           77,
           "0.574 0.443 0.464 0.441 0.411 0.429 0.501 0.466 0.677 0.537 0.722 0.605 0.802 0.628 0.797 0.578 0.98"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           78,
           "0.338 0.385 0.306 0.353 0.323 0.353 0.543 0.559 2.408 1.407 0.348 0.376 0.469 0.498 1.031 0.775 3.25"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           79,
           "0.140 0.238 0.299 0.373 0.231 0.323 0.261 0.348 0.420 0.466 0.599 0.587 0.350 0.425 1.259 0.919 0.99"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           80,
           "0.715 0.685 0.510 0.521 0.757 0.664 0.611 0.597 1.203 0.898 1.004 0.790 Avg 0.176 0.269 0.176 0.269 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           81,
           "1.641 0.854 1.207 0.661 1.454 0.765 1.538 0.817 336 0.436 0.310 0.434 0.303 0.449 0.313 0.426 0.304 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           82,
           "1.248 0.684 1.534 0.811 1.551 0.821 1st count 5 5 4 0 0 0 0 0 0 0 0 0 Table 10: Few-shot Forecasting"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           83,
           "33.9 27.54 0 Informer 19.04 15.82 35.82 21.2 22.97 0 Reformer 14.09 13.37 25.48 21.6 18.63 0 GPT2(6)"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           84,
           "GPUs. We use Area Under Curve of Receiver Operating Characteristic (AUC-ROC) as metrics. Meanwhile, "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           85,
           "850 3 152 26 Heartbeat 204 105 61 495 2 JapaneseVowels 270 370 12 29 9 Libras 180 280 2 45 15 LSST 2"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           86,
           "is a bag-of-pattern based approach which extracts and represents features to words. Scalable Rep- re"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           87,
           "et al. (2018). The results are shown in Table 13. Overall, TEST achieves comparable performance to S"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           88,
           "CK 0.944 0.986 1.000 0.917 1.000 0.986 0.958 0.986 1.000 0.986 1.000 0.958 1.000 1.000 DDG 0.275 0.5"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           89,
           "0.373 0.430 0.236 0.323 0.312 0.447 0.468 0.369 0.337 0.331 0.373 FD 0.519 0.000 0.529 0.545 0.545 0"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           90,
           "0.756 0.718 0.771 0.779 0.712 0.790 0.791 IW 0.128 N/A N/A 0.167 N/A 0.160 0.208 0.250 N/A 0.595 0.3"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           91,
           "0.531 0.550 0.610 N/A 0.650 0.650 NT 0.850 0.850 0.883 0.889 0.870 0.944 0.939 0.883 0.885 0.928 0.9"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           92,
           "0.829 0.842 0.851 SCP1 0.771 0.765 0.775 0.874 0.710 0.846 0.652 0.782 0.866 0.925 0.802 0.925 0.884"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           93,
           "0.944 0.903 0.941 0.933 Avg.Rank 10.933 9.480 8.821 8.756 6.890 7.120 6.956 5.523 5.423 5.013 5.059 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           94,
           "of Receiver Operating Characteristic (AUC-ROC) as metrics. A.5.1 DATASET DETAILS We represent the re"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           95,
           "set. We then train an SVM with radial basis function kernel on top of the learned features using the"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           96,
           "0.664 0.684 CricketX 0.802 0.787 0.805 0.713 0.623 CricketY 0.754 0.749 0.769 0.728 0.597 CricketZ 0"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           97,
           "0.714 0.721 0.719 0.707 0.700 FaceAll 0.789 0.771 0.805 0.786 0.766 FaceFour 0.834 0.932 0.932 0.920"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           98,
           "0.594 0.594 InlineSkate 0.389 0.418 0.407 0.371 0.378 InsectWingbeatSound 0.620 0.630 0.624 0.597 0."
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           99,
           "0.591 0.584 0.578 0.591 0.571 MoteStrain 0.857 0.861 0.863 0.851 0.825 NonInvasiveFetalECGThorax1 0."
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           100,
           "ProximalPhalanxTW 0.785 0.824 0.805 0.771 0.810 RefrigerationDevices 0.587 0.586 0.589 0.515 0.565 S"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           101,
           "0.977 0.972 0.963 0.885 SyntheticControl 0.997 0.997 0.993 0.987 1.000 23 Published as a conference "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           102,
           "0.998 0.998 0.992 0.994 Wine 0.788 0.880 0.889 0.815 0.759 WordSynonyms 0.699 0.679 0.704 0.691 0.63"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           103,
           "0.754 0.758 0.722 0.738 EOGHorizontalSignal 0.544 0.569 0.522 0.605 0.442 EOGVerticalSignal 0.467 0."
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           104,
           "0.848 0.899 0.316 GunPointAgeSpan 0.994 0.987 0.968 0.994 0.984 GunPointMaleVersusFemale 1.000 1.000"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           105,
           "PigArtPressure 0.962 0.966 0.966 0.928 0.808 PigCVP 0.803 0.815 0.870 0.788 0.649 PLAID 0.551 0.561 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           106,
           "A.6 ABLATION TEST contains two contrastive learning strategies: instance-wise contrast and feature-w"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           107,
           "0.286 0.390 0.821 0.629 0.453 0.388 3.139 5.931 TEST 0.353 0.382 0.293 0.334 0.414 0.431 0.331 0.380"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           108,
           "4.223 OWA 0.861 1.051 1.480 0.851 0.855 1.172 1.051 0.918 0.930 0.939 1.230 1.775 Table 16: Short-te"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           109,
           "prototypes. 1 2 3 4 5 6 7 8 9 10 Avg. Std. SMAPE 11.907 11.920 11.927 11.926 11.925 11.925 11.950 11"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           110,
           "represented space size and expressed number of features are almost the same. Therefore, the key is t"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           0,
           "JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 1 From LLMs to LLM-based Agents for Sof"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           1,
           "topics: requirement engineering, code generation, autonomous decision-making, software design, test "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           2,
           "Haolin Jin, Linghan Huang and Huaming Chen are with the School of Electrical and Computer Engineerin"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           3,
           "tions is another main concern, where the model generates code that appears plausible but is actually"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           4,
           "integrated into GitHub Copilot [12], enabling real-time code suggestions and completion within devel"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           5,
           "generative capabilities needed for more sophisticated agent behaviors. [10] shows that LLM-based age"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           6,
           "4) Software Design and Evaluation: Contributing to design processes, architecture validation, perfor"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           7,
           "IX and Section X) • RQ4: What are the predominant experimental models employed when utilizing LLMs i"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           8,
           "More importantly, a focus of LLMs is the main contributions of these works, but there is no distingu"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           9,
           "Improving Code Generation Quality (3) 35 Autonomous Learning and Decision Making Multi-LLMs Decision"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           10,
           "Code Generation and Debugging (4) Penetration Testing and Security Assessment (2) Program Analysis a"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           11,
           "between LLM and LLM-based agent contributions, offering a comparative overview and addressing the li"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           12,
           "✓ ✓ ✗ ✗ Ours 2024 LLM & LLM-based Agent in SE ✓ ✓ ✓ ✓ DBLP offers comprehensive, curated coverage of"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           13,
           "tering process to ensure the relevance and quality of the papers included in this study. Specificall"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           14,
           "Exclusion criteria 1) Papers with fewer than 7 pages. 2) Papers that focus on general Artificial Int"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           15,
           "mentioning LLMs without substantial relevance to software engineering tasks were excluded. Furthermo"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           16,
           "JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 5 40.3% 10.1% 6.5% 3.6% 2.9% 2.9% 2.9% "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           17,
           "1.4%) illustrates LLMs’ expanding role across the SE life- cycle. Second, NLP venues (ACL/EMNLP: 5.0"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           18,
           "III. PRELIMINARIES In this section, we introduce the foundational concepts of large language models,"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           19,
           "In 2017 the new framework called ”Transformer” introduced by Google’s research team [31]. The transf"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           20,
           "texts and special contexts. Compared to other contemporary models like Google’s PaLM or Meta’s OPT, "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           21,
           "umentation maintenance, Requirements modeling, Requirements elicitation Software Design and Evaluati"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           22,
           "construct the correct translated text. A recent example of this architecture is the CodeT5+ model, l"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           23,
           "primarily involves the decoder receiving processed word vec- tors and generating output. Utilizing t"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           24,
           "based on specified reward/punishment rules. Notable mile- stones include AlphaGo [39], which leverag"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           25,
           "agents has also become a hot research area in recent years. LLM-based Embodied Agents are intelligen"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           26,
           "fox jumps over the lazy dog The swift brown fox leaps over the sluggish dog FIG. 4: ILLUSTRATION OF "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           27,
           "models were prompted to create new examples similar to the original data, thereby increasing the div"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           28,
           "advantages (RAG could be 99 percent cheaper than utilizing all tokens). Additionally, long texts can"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           29,
           "8 to the user. Most research papers on LLMs have stated this problem, while prompt engineering or to"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           30,
           "relevant responses over long interactions. • Specialization and Division of Labor: Different agents "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           31,
           "tating tasks such as code generation, defect prediction, and automated documentation. Integrating th"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           32,
           "and emerging fields such as software security and mainte- nance. Currently, there is no comprehensiv"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           33,
           "Criteria 1 to 4 (LLM as central reasoning core, decision- making and planning, autonomous tool usage"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           34,
           "engineering and plays an essential role in the software devel- opment process, its primary task is t"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           35,
           "encounter situations where clients present multiple require- ments at once, necessitating manual cla"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           36,
           "specific Fβ scoring method to emphasize recall, the results demonstrated that GPT-3.5 Few-Shot promp"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           37,
           "pipeline [65] pushes this boundary by unifying user story and test case generation through a three-s"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           38,
           "this stuy demonstrated the potential of using fine-tuned LLMs to generate SRS and increase the overa"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           39,
           "tation and system design. The paper provides a catalog of 13 prompt patterns, each aimed at addressi"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           40,
           "in machine learning at that time. This study provides an excellent paradigm for the subsequent appli"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           41,
           "standability and correctness, but scored lower in feasibility and unambiguity, especially when domai"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           42,
           "useful researches to help us to see the potential possibility. JOURNAL OF LATEX CLASS FILES, VOL. 18"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           43,
           "LLM-based agent will improve the requirement case according to this information, and then design the"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           44,
           "by completing within a day compared to months manually. In agile software development, the quality o"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           45,
           "ation, quality assessment, feasibility review, and prioritization, achieving coherence and efficienc"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           46,
           "tasks can no longer be accomplished by simple LLMs alone, especially for high-level software design."
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           47,
           "Final refined version Final refined version Default User Story Prompt Generated Response Further imp"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           48,
           "code artifacts through practical usage and feedback. In [62], four datasets are primarily used, char"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           49,
           "and non-functional requirements to aid and assist models in learning this domain, the dataset utiliz"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           50,
           "Methodology to design and evaluate the LLM prototype but does not mention a specific dataset, focusi"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           51,
           "13 formance. While this approach is flexible and targeted, it also highlights the field’s shortcomin"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           52,
           "and standards. Evaluation expands beyond accuracy to in- clude reasoning traceability, compliance vi"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           53,
           "For traditional LLM applications, especially in classifica- tion and extraction tasks (e.g., functio"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           54,
           "ers semantic similarity (via sentence embeddings), response time, and alignment with standards like "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           55,
           "by behavioral robustness, human satisfaction, and contextual traceability. While LLMs excel in task "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           56,
           "Clarity, Consistency, and Compliance Completeness and accuracy of acceptance criteria No [62] NFR, S"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           57,
           "PTC-B F β, Mean Average Precision (MAP) No [61] PROMISE NFR Precision (P), Recall (R), F1-score (F1)"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           58,
           "Smallness, Testability. Survey among professionals Yes is more extensive and in-depth. Using natural"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           59,
           "language descriptions, where models utilize previously learnt code snippets or apply few-shot learni"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           60,
           "LLMs help developers quickly generate efficient database query code. In [90], proposed the SQL-PaLM "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           61,
           "user’s prompts. If the prompts are too vague or general, the LLM typically struggles to understand t"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           62,
           "INCODER model which capable of both program synthesis and editing. By leveraging bidirectional conte"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           63,
           "performance that was comparable to or better than the best Codex model on certain metrics, further d"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           64,
           "A user study with 15 programmers showed that TICODER significantly improved correctness (84% vs. 40%"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           65,
           "semantic bullet-point analysis, modular code generation, and structured YAML outputs. A key insight "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           66,
           "with algorithm synthesis tasks that require not just implemen- tation but also the inference of the "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           67,
           "LLM-based agents have shown significant potential and ad- vantages by substantially improving task e"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           68,
           "external tool integration and dynamic adjustment capabilities, this framework exhibits common charac"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           69,
           "Overcoming Context Limitations for Large Codebases. The limitations of context windows were not disc"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           70,
           "oration. Unlike traditional single-agent models constrained by limited context length, SoA employs m"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           71,
           "development, [27] proposed a multi-GPT agent framework that automates tasks such as project planning"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           72,
           "erator (DCGG), which tracks cross-file dependencies to en- hance context awareness for code generati"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           73,
           "Agile development methodologies. Enhancing Feedback, Tool Use, and Open-Source Model Performance. Th"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           74,
           "things and improve themselves. Similarly, [113] enhanced LLMs’ interaction with external APIs throug"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           75,
           "ACI provides simplified, LM-friendly commands, immediate feedback, and built-in guardrails for commo"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           76,
           "designed to address the ambiguity in user-provided natu- ral language requirements during code gener"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           77,
           "generation is conceptualized, orchestrated, and evaluated. Tra- ditional LLM methods typically cente"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           78,
           "velopment pipelines through distributed roles or agents for planning, coding, testing, and debugging"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           79,
           "multi-turn queries, such as iterative code completion or user- guided debugging, but they do not gen"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           80,
           "19 Role Assignment and Scrum Planning Tester Requirement Engineer Software Developer Scrum Master So"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           81,
           "processes. D. Benchmarks In the field of code generation and software development, there are notable"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           82,
           "more complex problems for a comprehensive evaluation of model performance [116]. The Spider and BIRD"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           83,
           "domain datasets, such as HumanEval-X and CodeSearchNet, to evaluate model performance across differe"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           84,
           "tion of generated code that passes all test cases within the JOURNAL OF LATEX CLASS FILES, VOL. 18, "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           85,
           "Win Rate and Agreement Rate are important metrics for evalu- ating the effectiveness of multi-agent "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           86,
           "Autonomous Learning and Decision Making is a critical and evolving field in modern software engineer"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           87,
           "their adaptability in dynamic environments through continuous learning and optimization. In this con"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           88,
           "searchers hope LLMs can continuously learn to fix bugs and eventually identify human oversights or c"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           89,
           "Execution Accuracy, Confidence Calibration Execution Rate No [50] Alpaca Data Win-Rate, Agreement Ra"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           90,
           "n@k (1@k, 2@k, 10@k, 100@k) No [101] MBPP/-ET, HumanEval/-ET Pass@k Yes [104] HumanEval Pass@1 Yes ["
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           91,
           "Yes [108] HumanEval, MBPP, ProjectDev Pass@1, Executability Rate, #Errors, Runtime Stats Yes [110] R"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           92,
           "makes it easier for developers to understand and accept au- tomatically generated patches. Although "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           93,
           "from the perspective of creativity theory, exploring their ability to generate creative content, the"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           94,
           "Multi-Agent Discussion and Reasoning Enhancement. Multi-agent collaboration and dialogue frameworks "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           95,
           "The framework uses self-reflection and language feedback to help language agents learn from mistakes"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           96,
           "Reflexion even further enhance the performance in iterative task attempts, highlighting the importan"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           97,
           "signed to improve task completion efficiency and effectiveness through collaboration. The framework "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           98,
           "and proposes a new multi-agent coordination strategy for solving complex tasks through efficient com"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           99,
           "to meet the human alignment. The framework enhances model capabilities through iterative processes o"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           100,
           "In terms of simulating trust behaviors, LLM-based agents demonstrate human-like behavior patterns th"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           101,
           "mark with an improvement margin of up to 90.2% compared to the best existing methods. [134] presents"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           102,
           "blast-radius control. Additionally, agents have shown strong capabilities in autonomous learning and"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           103,
           "tion planning. Its modular code-centric design demonstrates strong capabilities in both software-ori"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           104,
           "visual environment. We can conclude that, the application of LLM-based agents in the topic of autono"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           105,
           "to perform more complex and context-based decision-making tasks. Trajectory (Actions & Observations)"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           106,
           "thereby aiding subsequent decision-making. This autonomous decision-making capability is something t"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           107,
           "Chemistry, challenging the models’ diverse knowledge base and reasoning capabilities. The TransCoder"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           108,
           "making, evaluating the model’s performance in dynamic and interactive tasks. The FEVER (Fact Extract"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           109,
           "decision-making, it can showcase their advantages in handling complex interactions and multitasking "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           110,
           "In research on LLMs, evaluation metrics primarily focus on model accuracy and task completion. In [1"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           111,
           "value, novelty and surprise are used as creativity dimensions. Quality, social acceptability, and si"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           112,
           "(SR) is often used as a primary metric, evaluated through tasks such as HotpotQA and FEVER to assess"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           113,
           "task optimization and LLM-based agents’ potential in col- laborative handling of complex tasks with "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           114,
           "and design, demonstrating an overlap with the topic of code generation and software development. LLM"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           115,
           "Almost-Right HumanEval Plausible Patches, Correct Patches, Precision, Accuracy No [123] No Specific "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           116,
           "Yes [58] FED, Commongen Challenge, MGSM, Logic Grid Puzzles, HumanEval Pass@1, Task completion rate "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           117,
           "[142]. This section reviews the main research achievements of LLMs in software design and evaluation"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           118,
           "28 application prospects of LLMs in generative tasks. Similarly, in the LLMs evaluation domain, LLM-"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           119,
           "HLS debugging assistants. The optimized LLM significantly improves inference performance, providing "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           120,
           "55.6% of the time. However, the correctness of its explanations was only 53.0%, indicating the need "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           121,
           "These limitations can lead to incomplete workflows, inaccurate outputs, and unstable performance in "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           122,
           "show that this framework can test LLMs’ spatial designing, strategic planning, and teamwork abilitie"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           123,
           "This approach is inherently concurrency-friendly and supports the development of complex nested AI i"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           124,
           "understanding the integration of LLM components in software systems, laying a theoretical foundation"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           125,
           "in software design is commonly included in the software development, like previously discussed, the "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           126,
           "primarily used to evaluate the effectiveness of LLM debugging tools in detecting and correcting inje"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           127,
           "is usually because the research needs to evaluate the perfor- mance from multiple angles, and it is "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           128,
           "employed different evaluation metrics to measure the perfor- mance of LLMs and LLM-based agents acro"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           129,
           "such as accuracy and completion time, LLM-based agents use flexible evaluation metrics, including me"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           130,
           "the precision and scope of test case generation. Online tools like Sofy5, which use machine learning"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           131,
           "eration. These tasks are achieved through various models and techniques, significantly improving sof"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           132,
           "31 TABLE IX: EVALUATION METRICS IN SOFTWARE DESIGN AND EVALUATION Reference Paper Benchmarks Evaluat"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           133,
           "Extensibility. Yes [154] Sample Applications. BERTScore, BLEU Yes [151] CodexGLUE BLEU, METEOR, ROUG"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           134,
           "prompt engineering, few-shot learning, and chain-of-thought reasoning, AdbGPT demonstrates the power"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           135,
           "in improving software quality and reducing the burden on developers. Through various models and tech"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           136,
           "multiple languages and systems under test (SUTs) through comprehensive evaluations. Coverage-Driven "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           137,
           "correct” software. By comparing the effectiveness of AID in generating fault-revealing test inputs a"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           138,
           "tools and potential complements to SBST. B. LLM-based Agents Tasks Collaborative Multi-Agent Systems"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           139,
           "multiple agents based system. The study [168] evaluates the effectiveness of LLMs in generating high"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           140,
           "utilize middleware to facilitate interactions between the LLM and various testing tools, achieving m"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           141,
           "system and a variant without the reflection component. Exper- imental results show that XUAT-Copilot"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           142,
           "niques like prompt engineering and few-shot learning. The number of related studies is increasing as"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           143,
           "the future generation. The overall framework lacks autonomy, the LLM-based agent [169] framework on "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           144,
           "datasets containing complex bug scenarios, used to evaluate the precision and recall of generated te"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           145,
           "34 platform and fuzz testing in multi-language environments. This because the LLM-Based agents not o"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           146,
           "processing, such as generating test sets and considering the coverage of generated test sets. Howeve"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           147,
           "application of LLMs in software security and maintenance encompasses several aspects, including vuln"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           148,
           "In the field of software security and maintenance, research on LLMs can be categorized into three ma"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           149,
           "detection, uncovering current challenges. [175] evaluated only the performance of ChatGPT and GPT-3 "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           150,
           "Go Toolchain. Java Compiler (javac). Qiskit. Code Coverage. Validity Rate. Hit Rate. Bugs Detected. "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           151,
           "Yes were only 0.51, indicating performance equivalent to random guessing. In multi-label classificat"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           152,
           "the-art LLMs such as CodeT5 [177] and PLBART by 27.5% 8https://atcoder.jp/contests/ahc030 and 62.4% "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           153,
           "through comparisons on balanced and unbalanced datasets and developing an efficient batch packing st"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           154,
           "gap between current code language models’ performance and actual requirements for vulnerability dete"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           155,
           "sively used for automating program repair. One study proposed using Round-Trip Translation (RTT) for"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           156,
           "ular LLMs (ChatGPT, DeepSeek Coder, and Magicoder) to demonstrate its effectiveness in improving cod"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           157,
           "alpha, and Mistral-7B) using two benchmarks, evalrepair-C++ and EvalRepair-Java. The results indicat"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           158,
           "duced a new PowerShell command repair dataset, providing valuable resources for the research communi"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           159,
           "of penetration testing, where they are used to enhance the efficiency and effectiveness of automated"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           160,
           "AI tools (specifically ChatGPT 3.5) in penetration testing. Through practical application experiment"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           161,
           "of LLM software and reveals its vulnerabilities in handling complex natural language inputs. This re"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           162,
           "domain of vulnerability detection, researchers have enhanced detection accuracy by combining Role-Ba"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           163,
           "the powerful capabilities of LLMs in automated debugging, improving the performance of existing APR "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           164,
           "ingredients, and validate the repairs, similar to how human developers fix bugs. RepairAgent achieve"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           165,
           "cialized agents—Manager, Repository Custodian, Developer, and Quality Assurance (QA) Engineer. These"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           166,
           "GPTLENS achieved a 76.9% success rate in detecting smart contract vulnerabilities, better than the 3"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           167,
           "level vulnerability discovery, thereby enhancing penetration testing capabilities. The experiments d"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           168,
           "code structures to improve the accuracy and efficiency of detecting vulnerabilities, traditional LLM"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           169,
           "making and dynamic adjustment capabilities during the repair process. In terms of software security,"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           170,
           "demonstrating their strong potential in proactive defense, com- plex task handling, and meeting high"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           171,
           "consists of defective specifications and tests the ability of LLMs to understand and repair formal s"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           172,
           "emerge. Both approaches frequently use datasets like De- fects4J, CVE, and HumanEval, highlighting t"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           173,
           "in addressing emerging challenges in software security and maintenance. This distinction reflects th"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           174,
           "which helps determine if the model’s capabilities and perfor- mance are comparable to human performa"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           175,
           "to emphasize the success rate and accuracy during both the generation and validation phases, providi"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           176,
           "fields at the time of the study. In summary, across the 139 papers, we identified a total of 82 uniq"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           177,
           "into larger systems and combining with other machine learning models and tools, these models can be "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           178,
           "No [179] CVEfixes Manually-Curated Dataset VCMatch Precision, Recall No [171] HackTheBox VulnHub Ove"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           179,
           "[176] EvalGPTFix Number of Correctly Fixed Bugs, Fix Rate per Prompt Type, Success Rate over All Tes"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           180,
           "Yes [137] 263 real smart contract vulnerabilities F1 score, Accuracy, Consistency,Precision, Recall "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           181,
           "LLaMA-2 for research and evaluation. During our analysis, we found that many studies on LLM-based ag"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           182,
           "by LLM-based agents is relatively limited. For instance, in the requirement engineering and document"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           183,
           "performance and potential compared to other models like Codex. Another potential reason is the previ"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           184,
           "followed closely by software security and maintenance, which together account for nearly half of all"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           185,
           "10 2 2 12 16 11 2 4 3 3 7 7 2 2 3 7 2 8 10 LLM BERT LLM GPT-3.5 LLM GPT-4 LLM Alpaca LLM CodeGen LLM"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           186,
           "REQ (6) 7.8% Code (20) 26.0% Autonomous (23) 29.9% Design (9) 11.7% Test (6) 7.8% Security (13) 16.9"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           187,
           "dynamically generating code or performing real-time security assessments. Similarly, enhancing softw"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           188,
           "Therefore, the benchmark datasets shown in the figure pri- marily represent commonly-used public dat"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           189,
           "automated program repair. However, it is observed to be rarely used by LLM-based agent studies, pote"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           190,
           "(6.0%). These metrics are particularly common in structured tasks such as classification [94], code "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           191,
           "both LLMs and agents too access many software task, it is often interpreted differently static class"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           192,
           "SWEbench CodeContests SVAMP ALFWorld FEVER GSM8K ChatPHP PROMISE Spider WebShop EvalPlus APIBank CAA"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           193,
           "20.0% 5.7% 8.6% Top 10 Metrics for LLM and LLM-based Agent (Percentage) LLM Agent FIG. 14: TOP 10 EV"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           194,
           "2) Workflow complexity and error propagation in multi- agent systems. While multi-agent collaboratio"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           195,
           "transfer capabilities. Although some agents exhibit expertise in specific tasks, such as program rep"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           196,
           "E. Opportunities for Future Research 1) Towards a unified agent taxonomy and evaluation framework to"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           197,
           "and embodied agents with multimodal capabilities in software engineering. Several studies [152], [15"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           198,
           "(DCGG) module [108] continuously models cross-file depen- dencies, class structures, and component v"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           199,
           "software test generation, and software security and mainte- nance. For each topic, we analyzed the t"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           200,
           "[3] “Chatgpt: Optimizing language models for dialogue,” 11 2022. [Online; accessed 17-July-2024]. [4"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           201,
           "ford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish,"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           202,
           "and X. Hu, “Harnessing the power of llms in practice: A survey on chatgpt and beyond,” ACM Trans. Kn"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           203,
           "S. Jin, E. Zhou, R. Zheng, X. Fan, X. Wang, L. Xiong, Y. Zhou, W. Wang, C. Jiang, Y. Zou, X. Liu, Z."
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           204,
           "com/features/copilot, 2024. [Online; accessed 17-July-2024]. [13] S. Russell and P. Norvig, Artifici"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           205,
           "J. Grundy, and H. Wang, “Large language models for software engi- neering: A systematic literature r"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           206,
           "rity, privacy, explainability, efficiency, and usability of large language models for code,” 2024. ["
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           207,
           "ion Proceedings of the 29th International Conference on Intelligent User Interfaces, pp. 30–32, 2024"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           208,
           "Advances in neural information processing systems, vol. 30, 2017. [32] L. Floridi and M. Chiriatti, "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           209,
           "moyer, “Opt: Open pre-trained transformer language models,” 2022. [35] Y. Wang, H. Le, A. D. Gotmare"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           210,
           "T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar, et al., “Llama: Open and efficient foundatio"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           211,
           "as zero-shot planners: Extracting actionable knowledge for embodied agents,” in International confer"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           212,
           "Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al., “Gemini 1.5: Unlocking multim"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           213,
           "need,” 2024. [50] Y. Dubois, C. X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin, P. S. L"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           214,
           "and O. Press, “Swe-agent: Agent-computer interfaces enable automated software engineering,” 2024. [5"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           215,
           "Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, C. Ran, L. Xiao, C. Wu, and J. Schmidhuber, “Metagpt: Meta p"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           216,
           "els,” in Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           217,
           "[65] T. Rahman and Y. Zhu, “Automated user story generation with test case specification using large"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           218,
           "p. 108–118, Association for Computing Machinery, 2024. [70] J. White, S. Hays, Q. Fu, J. Spencer-Smi"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           219,
           "for user story quality evaluation: Trustworthy out of the box?,” in Agile Processes in Software Engi"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           220,
           "with llms,” in 2024 IEEE 32nd International Requirements Engineering Conference (RE), pp. 416–422, 2"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           221,
           "“Llm-based agents for automating the enhancement of user story quality: An early report,” in Agile P"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           222,
           "P. Abrahamsson, “Ai based multiagent approach for requirements elicitation and analysis,” 2024. [85]"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           223,
           "S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D."
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           224,
           "“L2ceval: Evaluating language-to-code generation capabilities of large language models,” 2023. [90] "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           225,
           "[93] S. Peng, E. Kalliamvakou, P. Cihon, and M. Demirer, “The impact of ai on developer productivity"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           226,
           "“Llm-based test-driven interactive code generation: User study and empirical evaluation,” IEEE Trans"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           227,
           "[102] F. Lin, D. J. Kim, et al., “When llm-based code generation meets the software development proc"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           228,
           "by using multi-agents,” arXiv preprint arXiv:2402.01411, 2024. [107] D. Huang, Q. Bu, J. M. Zhang, M"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           229,
           "Research and Development, vol. 8, no. 5, pp. 892–898, 2024. [111] T. Zheng, G. Zhang, T. Shen, X. Li"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           230,
           "2023. JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 49 [114] X. Wang, Y. Chen, L. Yua"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           231,
           "human behaviors for llm-based task-oriented coordination via collabo- rative generative agents,” arX"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           232,
           "[121] X. Chen, M. Lin, N. Sch¨arli, and D. Zhou, “Teaching large language models to self-debug,” arX"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           233,
           "bench and chatbot arena,” Advances in Neural Information Processing Systems, vol. 36, 2024. [126] Q."
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           234,
           "Z. Chen, J. C. Niebles, D. Arpit, et al., “Bolaa: Benchmarking and orchestrating llm-augmented auton"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           235,
           "arXiv:2402.15538, 2024. [133] M. Zhuge, W. Wang, L. Kirsch, F. Faccio, D. Khizbullin, and J. Schmidh"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           236,
           "European Software Engineering Conference and Symposium on the Foundations of Software Engineering, p"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           237,
           "K. Narasimhan, “Tree of thoughts: Deliberate problem solving with large language models,” Advances i"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           238,
           "Current status and challenges,” arXiv preprint arXiv:2402.01383, 2024. [145] L. J. Wan, Y. Huang, Y."
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           239,
           "software testing education: promises & perils (2023),” arXiv preprint arXiv:2302.03287, 2023. [149] "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           240,
           "Y. Yao, J. Wei, D. Paul, and R. West, “Flows: Building blocks of reasoning and collaborating ai,” ar"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           241,
           "evaluation of llm outputs with human preferences,” arXiv preprint arXiv:2404.12272, 2024. [157] D. R"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           242,
           "“Test mimicry to assess the exploitability of library vulnerabilities,” in Proceedings of the 31st A"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           243,
           "B. Ray, “Code-aware prompting: A study of coverage guided test gener- ation in regression setting us"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           244,
           "erators: Performance evaluation and enhancement,” arXiv preprint arXiv:2404.13340, 2024. [169] Z. Wa"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           245,
           "automatic penetration testing tool,” arXiv preprint arXiv:2308.06782, 2023. [172] M. Xiao, Y. Xiao, "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           246,
           "model for vulnerability detection,” arXiv preprint arXiv:2304.07232, 2023. [176] Q. Zhang, T. Zhang,"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           247,
           "preprint arXiv:2308.00245, 2023. [180] Y. Ding, Y. Fu, O. Ibrahim, C. Sitawarin, X. Chen, B. Alomair"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           248,
           "Goues, and S. Jin, “Multi-objective fine-tuning for enhanced program repair with llms,” arXiv prepri"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           249,
           "Languages, vol. 8, no. OOPSLA1, pp. 1100–1124, 2024. [187] H. Joshi, J. C. Sanchez, S. Gulwani, V. L"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           250,
           "arXiv:2402.16906, 2024. [191] M. Alhanahnah, M. R. Hasan, and H. Bagheri, “An empirical evaluation o"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           251,
           "model-powered smart contract vulnerability detection: New perspec- tives,” 2023. [196] F. Geissler, "
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           0,
           "Large Language Model Supply Chain: Open Problems From the Security Perspective Qiang Hu1, Xiaofei Xi"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           1,
           "promising guidance to help build safer LLM systems. We hope our work can facilitate the evolution of"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           2,
           "extensively studied over the past decade, research specifically focused on the supply chain of AI sy"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           3,
           "along the supply chain are still unclear. To fill this gap, we take the first step in analyzing pote"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           4,
           "App Store Data Provider User Data Collection Data Data Selection Data Cleaning Data Labeling Pre-Tra"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           5,
           "the Internet. The App store then becomes a potential upstream component. The LLM application in the "
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           6,
           "open-source model hubs and directly use it, and 3) download it and then fine-tune it for specific do"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           7,
           "application preparation phase, risks are potentially hidden in other software and the model optimiza"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           8,
           "filtering high-quality data for training or testing and will affect all the remaining components in "
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           9,
           "domain task, a useful data sample requires the raw data and the corresponding ground truth. Labeling"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           10,
           "software implementation and the AI models. Even worse, some vulnerabilities could arise in later dow"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           11,
           "active learning methods to force models to select vulnerable data for training. Risk7: Distribution "
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           12,
           "different security issues and need to be carefully checked before the fine-tuning process. No matter"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           13,
           "the following components in the LLM system. Therefore, it is necessary to analyze the influence and "
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           14,
           "Then, the upstream supplier is recommended to provide some data distribution information to the down"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           15,
           "rics (especially security risk evaluation) to guide its reliable usage. Besides, as mentioned in Sec"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           16,
           "risks in the large language model supply chain. Different from existing works that mainly focused on"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           17,
           "Linguistics, Aug. 2024, pp. 13 256–13 274. [Online]. Available: https://aclanthology.org/2024.findin"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           18,
           "large language models through the lens of verification and validation,” Artificial Intelligence Revi"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           19,
           "Y. Liu, “Jailbreaker: Automated jailbreak across multiple large language model chatbots,” arXiv prep"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           20,
           "“A unified framework for adversarial attack and defense in constrained feature space,” arXiv preprin"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           21,
           "modelscan [19] Q. Hu, Y. Guo, M. Cordy, X. Xie, W. Ma, M. Papadakis, and Y. Le Traon, “Towards explo"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           22,
           "C. Wei, and H. Wang, “Models are codes: Towards measuring malicious code poisoning attacks on pre-tr"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           0,
           "arXiv:2501.10970v3  [cs.CL]  17 Jun 2025 The Alternative Annotator Test for LLM-as-a-Judge: How to S"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           1,
           "modest subset of annotated examples to justify using LLM annotations. Additionally, we in- troduce a"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           2,
           "1Code for the procedure and datasets are available at: https://github.com/nitaytech/AltTest One key "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           3,
           "2024), and commonly serve as evaluators for bench- marking models and methods (Ahmed et al., 2024; G"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           4,
           "evaluating outputs of other LLMs. It can be viewed as a special case of the broader “LLM-as-an-annot"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           5,
           "we aim to compare the LLM to the group. Other measures frequently rely on majority vote labels, over"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           6,
           "annotators (at least three) on a modest subset of examples (between 50 and 100). Our procedure is de"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           7,
           "tasks, including two vision-language tasks. Our results indicate that in many cases, LLMs can serve "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           8,
           "introduce a versatile and interpretable measure, the average advantage probability, for comparing LL"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           9,
           "Dong et al. (2024) investigated personalized LLM judges, Verga et al. (2024) proposed using a panel "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           10,
           "ten focusing on specific LLM limitations or biases (Wu and Aji, 2023; Ashktorab et al., 2024; Jung e"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           11,
           "multiple annotators. This is the exact setup we use in this paper: a modest subset of randomly sampl"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           12,
           "tage over human annotators, which justifies using it. examples, each annotated by multiple annotator"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           13,
           "3 annotation predicted by the LLM is denoted as f(xi). In addition, [−j] represents the set of in- d"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           14,
           "LLM for instance xi. We use S(f, xi, j) to denote the alignment scoring function between f(xi) and t"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           15,
           "1 |Hi| −1 X k∈Hi[−j] sim(f(xi), hk(xi)) Note that −RMSE(hj, xi, j), ACC(hj, xi, j), and SIM(hj, xi, "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           16,
           "ing the inequality (to ≤) in the definition above, representing that the annotation of hj for xi is "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           17,
           "decision. Notice, however, that employing an LLM is a cheaper and less labor-intensive alternative. "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           18,
           "distribution table. When n < 30, the normality assumption may not hold, and a non-parametric test (e"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           19,
           "particularly when the hypotheses are dependent (Dror et al., 2017). In our case, the dependency aris"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           20,
           "where the null hypotheses are dependent. In our experiments, we use the standard target FDR level of"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           21,
           "of the wins. For example, ρf j = 0.9 and ρf j = 0.6 contribute equally to ω if their respective null"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           22,
           "eliminates the need to switch between measures. 5 Discrete Annotation Tasks Dataset m n Cats I.p.A A"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           23,
           "CEBaB-S 10 C 711 1–5 219 3.08 0.67 0.67 Identify the star rating (1-5) given in restaurant reviews. "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           24,
           "correlation. For the text generation task, we compute the average embedding cosine similarity (see T"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           25,
           "|Hi| , predicting the mean annotation for xi. In both cases, the optimal LLM-as-a-judge achieves an "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           26,
           "each item be annotated by multiple annotators. 4.2 LLMs The six models that were used as candidate L"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           27,
           "LGBTeen (ε = 0.2) MT-Bench (ε = 0.2) Framing (ε = 0.15) CEBaB-A (ε = 0.1) Acc WR ω AP ρ Acc WR ω AP "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           28,
           "0.24 0.0 0.57 0.54 0.0 0.72 0.54 0.0 0.69 0.66 0.5 0.80 0.87 0.6 0.89 Mistral-v3 0.17 0.0 0.50 0.58 "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           29,
           "0.77 0.08 0.43 GPT-4o 0.54 0.0 0.48 0.47 0.69 0.76 0.80 0.9 0.90 0.67 0.0 0.62 0.78 0.2 0.53 GPT-4o-"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           30,
           "rate (WR ω, the ε value is stated next to the dataset name) and the average advantage probability (A"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           31,
           "tion, and similarity), the winning rate (WR, denoted as ω), and the average advantage probability (A"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           32,
           "success of LLMs is nuanced and aspect-dependent. In Table 5 in the Appendix, we analyze three datase"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           33,
           "WAX (ε = 0.1) LGBTeen (ε = 0.2) MT-Bench (ε = 0.2) SummEval (ε = 0.2) 10K Prompts (ε = 0.15) Acc WR "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           34,
           "0.39 0.17 0.69 0.55 0.04 0.73 0.63 0.03 0.77 0.57 0.59 0.77 0.24 0.0 0.60 + CoT 0.36 0.09 0.68 0.48 "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           35,
           "0.49 0.0 0.53 0.36 0.48 0.76 + 4-shots 0.30 0.01 0.62 0.60 0.12 0.77 0.61 0.0 0.74 0.60 0.77 0.79 0."
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           36,
           "0.41 0.74 Table 3: Results – Advanced LLM Judges: Each data point is calculated using a bootstrap of"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           37,
           "Notably, in almost all datasets, the top-ranked LLM is the same based on ρ values and the tradi- tio"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           38,
           "to minimize computational costs11 and primarily to reflect practical constraints better, as research"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           39,
           "50 100 150 200 LGBTeen - GPT-4o 50 100 150 200 MT-Bench - GPT-4o 50 100 150 200 Framing - GPT-4o 50 "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           40,
           "This naturally leads to the question: how many annotated instances are needed for a reliable test? T"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           41,
           "and in half even before 50 instances. With ε = 0.1 the alt-test requires more instances, typically d"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           42,
           "examples, depending on the complexity of the task. Appendix A provides a list of frequently asked qu"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           43,
           "are publicly available and might have been included in the training data of some LLMs. Notice that m"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           44,
           "lines, unqualified annotators, or the inherent sub- jectivity of the task. Traditional measures woul"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           45,
           "experts, and both are tested for alignment with a single expert. If the non-experts are particularly"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           46,
           "Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report."
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           47,
           "inference. CoRR, abs/2311.01453. Zahra Ashktorab, Michael Desmond, Qian Pan, James M. Johnson, Marti"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           48,
           "Hoelscher-Obermaier, and Jacob Pfau. 2023. Self- consistency of large language models under ambigu- "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           49,
           "Yoav Benjamini and Daniel Yekutieli. 2001. The con- trol of the false discovery rate in multiple tes"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           50,
           "Zhou, Yao Wan, and Lichao Sun. 2024b. Mllm- as-a-judge: Assessing multimodal llm-as-a-judge with vis"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           51,
           "Canada. Association for Computational Linguistics. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Z"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           52,
           "2024. Can llm be a personalized judge? arXiv preprint arXiv:2406.11657. Rotem Dror, Gili Baumer, Mar"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           53,
           "Cann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. Summeval: Re-evaluating summariza- ti"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           54,
           "ing factual consistency evaluation with large lan- guage models. In Proceedings of the 2023 Confer- "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           55,
           "Shengjie Ma, Honghao Liu, et al. 2024. A survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594. "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           56,
           "Methods in Natural Language Processing, pages 582– 601, Abu Dhabi, United Arab Emirates. Association"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           57,
           "Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo."
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           58,
           "Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Huang. 2023. A systematic study and 12 compr"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           59,
           "Ophir, Eyal Fruchter, Anat Brunstein Klomek, and Roi Reichart. 2024. The colorful future of LLMs: Ev"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           60,
           "ton Pashkov, et al. 2024. Large language models surpass human experts in predicting neuroscience res"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           61,
           "llm-generated annotations in low-resource language NLP tasks. IEEE Access, 12:71876–71900. Maja Pavl"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           62,
           "els replace economic choice prediction labs? CoRR, abs/2401.17435. Mingyang Song, Mao Zheng, and Xua"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           63,
           "Liu. 2024b. Large language models for data anno- tation and synthesis: A survey. In Proceedings of t"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           64,
           "Minjie Xu, Naomi White, and Patrick Lewis. 2024. Replacing judges with juries: Evaluating llm genera"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           65,
           "Association for Computational Linguistics (Volume 1: Long Papers), pages 15474–15492, Bangkok, Thai-"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           66,
           "2023, Singapore, December 6-10, 2023, pages 4615– 4635. Association for Computational Linguistics. L"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           67,
           "Zhehao Zhang, and Diyi Yang. 2024. Can large lan- guage models transform computational social sci- e"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           68,
           "which assess agreement among a group of annota- tors, our goal is to compare the LLM to the group to"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           69,
           "man annotators and not two? A: While our procedure can be used with two an- notators, we believe it "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           70,
           "Q: How do I select the ε value? A: We discuss this topic in detail in §B.1. Note that ε is the cost-"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           71,
           "instead. Still, we strongly recommend having an- notators label additional instances. See the next q"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           72,
           "could lead to higher p-values, making the LLM fail. Q: What if I care about ranking rather than exac"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           73,
           "can be used across multiple environments or do- mains, it is important to evaluate it in each settin"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           74,
           "tions to our procedure to account for variations in annotator quality. B Discussion The goal of this"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           75,
           "0.1 0.0 0.1 0.2 0.3 0.4 0.0 0.2 0.4 0.6 0.8 1.0 SummEval (Experts) 0.1 0.0 0.1 0.2 0.3 0.4 10K Promp"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           76,
           "We wish to use LLMs instead of human annota- tors since they offer a much cheaper, faster, and less "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           77,
           "and 0.3. For ε > 0.3, all LLMs achieve ω ≥0.5 on every dataset (except SummEval, and Gemini- Pro in "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           78,
           "in the following subsection. In our experiments, we selected ε values based on the type of annota- t"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           79,
           "compares the LLM against non-experts to deter- mine whether the LLM aligns more closely with the sin"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           80,
           "because there is no need to penalize humans. In §C.5, we discuss adapting the alt-test to rigorously"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           81,
           "by Table 6, few-shot helps LLMs adjust and skew their distributions, improving their alignment. Note"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           82,
           "have substantial overlap with others, as illustrated in the code below: 1 from scipy.stats import pe"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           83,
           "where MVj(xi) is the majority vote label for xi based on all annotators except hj (ensuring the ex- "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           84,
           "vantage probability, ρf j,π, is: ρf,π j = P i∈Ij πyi,jWi,j P i∈Ij πyi,j This formulation ensures tha"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           85,
           "are expensive, hence most often we have only one expert to compare to. To address this scenario, we "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           86,
           "and the winning rate ω remain unchanged. C.3 Incorporating Annotator Quality A key principle of our "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           87,
           "in our procedure. The first is in the formula for the alignment score metric, S(f, xi, j), where we "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           88,
           "ing hate speech or offensive language, often lack a single ground truth and may reflect diverse per-"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           89,
           "to fit the researcher’s needs. For example, one might use a variant of accuracy suitable for hate sp"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           90,
           "ios. To apply the alt-test, the modification follows the approach outlined in the previous subsectio"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           91,
           "sented in Algorithm 1) is a statistical procedure de- signed to control the false discovery rate (FD"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           92,
           "5: Find the largest i such that p(i) ≤threshold(i) 6: Reject null hypotheses corresponding to p(1), "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           93,
           "ing function. The optimal LLM-as-a-judge, denoted as f∗(xi), is defined as follows: • If S = ACC, th"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           94,
           "{ k ∈Hi : hk(xi) = MV (xi)} \f\f ≥ \f\f{ k ∈Hi : hk(xi) = hj(xi)} \f\f Note that if there is a single majo"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           95,
           "ing W f i,j = 1. Otherwise, hj(xi) ̸= ¯h(xi). To show that RMSE(f, xi, j) < RMSE(hj, xi, j) (which i"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           96,
           "\u00012 = X k∈Hi[−j] \u0000hj(xi) −hk(xi) \u00012 The first inequality holds because \u0000¯h(xi) −hj(xi) \u00012 > 0 given h"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           97,
           "16 predefined relation types. We included only items that were annotated by at least five crowd work"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           98,
           "tated by at least two annotators. • MT-Bench (Zheng et al., 2024b) – Prompt provided in Box G.4. MT-"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           99,
           "number of colors present (1-6), presence of structures such as dots (0-2) and presence of a blueish "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           100,
           "five-point scale. We use two variants of this dataset: CEBaB-A, which includes an- notations for the"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           101,
           "of tangram images (see an example in Fig- ure 4), annotated by MTurk workers. Each annotator provide"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           102,
           "– – 0.78 – – Gemini-Flash 0.28 0.42 0.56 0.79 0.66 0.61 Gemini-Pro 0.26 0.14 0.49 0.77 0.08 0.43 GPT"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           103,
           "indicate the best-performing LLM according to ρ and a green background highlights a ω higher than 0."
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           104,
           "0.42 1.0 0.76 Llama-3.1 0.36 1.0 0.70 0.52 0.0 0.68 0.26 0.0 0.2 0.38 1.0 0.74 Mistral-v3 0.17 0.33 "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           105,
           "0.21 0.59 0.83 0.81 0.24 0.0 0.59 GPT-4o-mini 0.15 0.17 0.65 0.49 1.0 0.93 0.01 0.0 0.57 0.60 0.67 0"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           106,
           "0.25 0.76 GPT-4o-mini 0.80 0.0 0.80 0.81 0.75 0.93 0.67 0.25 0.73 0.50 0.00 0.69 0.47 0.0 0.75 Llama"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           107,
           "0.62 0.18 0.0 0.47 0.33 0.0 0.59 0.33 0.0 0.53 GPT-4o 0.57 0.5 0.78 0.58 0.0 0.65 0.69 0.25 0.87 0.6"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           108,
           "number of annotators and instances, respectively. Acc is the accuracy with the majority vote, and Pe"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           109,
           ".08 .00 + 4-shots .02 .16 .53 .25 .04 .00 .03 .08 .09 .80 .00 .01 .07 .24 .68 .02 .10 .53 .31 .04 Ge"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           110,
           ".01 + 4-shots .01 .11 .27 .57 .04 .00 .00 .05 .11 .84 .00 .01 .08 .27 .64 .00 .07 .21 .58 .14 Table "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           111,
           "* PartOf: A part or component of an entity or event. * Material-MadeOf: The material something is ma"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           112,
           "* Synonym: The cue and association are synonyms. * Antonym: The cue and association are antonyms. * "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           113,
           "25 ** Question ** [QUESTION] Box G.3: LGBTeen - Questions ’Q1’: LGBTQ+ Inclusiveness: Does the respo"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           114,
           "contains elements that could be perceived as antagonistic or judgemental. * Partially - The response"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           115,
           "’Q4’: Mental Status: Does the response recognize, fit, and support the writer’s mental status (e.g.,"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           116,
           "acknowledge them and should inquire for more information (e.g., by asking follow-up questions). * Pa"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           117,
           "* Ignore - If no response supplied (e.g., \"unable to help\", low quality or nonsensical response). ’Q"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           118,
           "uncomfortable situation. * Partially - The advice is generally safe, but may not fully consider the "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           119,
           "* Yes - The response thoroughly addresses every aspect of the situation described by the writer. * I"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           120,
           "Answer the following questions: [QUESTION_GROUP] Please follow the JSON format below when answering "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           121,
           "\"hi3\": \"Does the story emphasize how one or more entities are NEGATIVELY affected by the issue/prob-"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           122,
           "\"ec2\": \"Is there a mention of the costs/degree of the expense involved?\", \"ec3\": \"Is there a referen"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           123,
           "\"re6\": \"Mark ’yes’ if at least one entity in the story is described as actively alleviating or plann"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           124,
           "sue. Select the most positively affected entity.\", \"hi5\": \"Mark ’yes’ if the story explicitly refers"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           125,
           "a quote or metaphor.\", \"mo3\": \"Mark ’yes’ if the story explicitly mentions expectations around norms"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           126,
           "```json { \"food\": \"Positive/Negative/unknown\", \"service\": \"Positive/Negative/unknown\", \"ambiance\": \""
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           127,
           "```json { \"coherence\": int (1-5), \"consistency\": int (1-5), \"fluency\": int (1-5), \"relevance\": int ("
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           128,
           "* Border: irregularity of the border (scale 0-2, where 2 is high irregularity) * Color: number of co"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           0,
           "We’re Different, We’re the Same: Creative Homogeneity Across LLMs EMILY WENGER∗, Duke University YOE"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           1,
           "partners—regardless of the model used—may drive all users towards a limited set of “creative” output"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           2,
           "While concerning, these works typically only look at a single LLM and it’s effect on downstream crea"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           3,
           "in these models [26, 29, 32]. We postulate that such feature space alignment in LLMs may result in h"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           4,
           "of responses. While caution should be used in extrapolating human-centric psychological tests to non"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           5,
           "and Picasso. Our set of AI “creative” partners will instead collectively drive us towards a mean. 2 "
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           6,
           "findings further motivate our study of whether it is the use of specific models in these studies—oft"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           7,
           "that large models will inevitably become more similar over time. However, limited work has considere"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           8,
           "LLMs tested, and evaluation metrics. 3.1 How do we elicit creative responses from LLMs? The American"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           9,
           "the AUT evaluation process is onerous, so researchers have developed more lightweight divergent thin"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           10,
           "variability of LLM and human responses to creative prompts, which is an empirical rather than psycho"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           11,
           "Exact test wording is in Appendix A. Guilford’s Alternative Uses Test (AUT) [23] presents people wit"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           12,
           "are subject to certain constraints: only nouns, no proper nouns, only single words in English, and t"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           13,
           "instruct, Phi 3 mini 4k instruct, Phi 3 small 128k instruct,Phi 3 small 8k instruct, and Phi 3.5 min"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           14,
           "Llama 3.1 8B Instruct. We evaluate all models with the default system prompt of “You are a helpful a"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           15,
           "Non-Binary 3% Hispanic or Latino or Spanish Origin of any race 11% 45-54 19% White 53% 55+ 5% Other "
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           16,
           "4https://osf.io/7p5mt/ 5https://osf.io/kbeq6/ 6 Emily Wenger and Yoed Kenett 3.4 Evaluation Metrics "
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           17,
           "necessitates different originality scoring procedures, described in detail in Appendix §B. Originali"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           18,
           "semantically similar sets of answers, this indicates that the response variability of the population"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           19,
           "all-MiniLM-L6-v2 from the sentence_transformers Python library [42], a high-performing and widely us"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           20,
           "difference between the means of the two populations divided by their pooled standard deviation. Cohe"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           21,
           "on FF. Figure 1 shows the distributions of originality scores for humans and LLMs on these tests, an"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           22,
           "4.2 Population-level Response Variability—LLMs vs. Humans Now, we explore the main question: whether"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           23,
           "Test 𝜇(V𝑡(LLM)) 𝜇(V𝑡(Human)) Test statistic 𝑝-value Effect size Test power AUT 0.459 0.738 𝑡(10078) "
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           24,
           "and V𝑡(Human), e.g. cosine distances between responses in these respective populations. Both these v"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           25,
           "through analysis of lexical patterns in LLM and human responses. We remove stopwords from responses,"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           26,
           "AUT Response Overlaps LLMs Humans 2 3 4 5 6 7 8 9 10 11 12 13 Number of overlapping words 0.00 0.05 "
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           27,
           "creative homogeneity through the use of creative system prompts. Finally, we confirm that our human "
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           28,
           "0.1 0.2 0.3 0.4 0.5 Density Words in AUT Responses (Prompt Version 3) Human LLM Fig. 5. Effect of di"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           29,
           "(version 1) included the phrase: “Please list the uses as short sentences or phrases, separated by s"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           30,
           "(single word answers are ok), separated by semicolons.” The right graph of Figure 5 shows that promp"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           31,
           "resulting LLM response structure affect measurements of creativity and variability. We run the same "
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           32,
           "V𝑡(P) from the last row (v3, one word answers) are plotted in Figure 6 to visualize the shift in mea"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           33,
           "for prompt version 3, supporting that this is a reasonable setting for measuring population creativi"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           34,
           "otherwise unrelated models. To do this, we measure the population-level variability of AUT responses"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           35,
           "Test statistic 𝑝-value Effect size Test power 0.445 0.441 𝑡(248) = 0.2 0.41 0.02 0.01 Table 6. Model"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           36,
           "LLM system prompt to strictly request creative outputs will induce higher variability. We experiment"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           37,
           "0.459 𝑡(10078) = 19.1, 3.9𝑒−80 More creative 0.733 𝑡(5020) = −9.8, 1.0𝑒−22 0.503 𝑡(10078) = 16.1, 3."
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           38,
           "0.8 1.0 Originality Scores (0 = low, 1 = high) 0 2 4 6 Density Human DAT Scores Prior Study Our Stud"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           39,
           "Validation with preexisting survey data Finally, we compare responses in our user study to prior use"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           40,
           "14 Emily Wenger and Yoed Kenett 0.0 0.2 0.4 0.6 0.8 1.0 Cosine distance between responses (0 = low, "
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           41,
           "and alternative is that they are not. For AUT, the prior study has higher mean variability, 𝑡(1046) "
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           42,
           "homogenize creative outputs. Implications. These results have significant implications if LLMs are w"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           43,
           "Acknowledgments. We thank Austin Liu for helping us design the system prompts of §5.3. 7 ETHICAL CON"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           44,
           "[5] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Ba"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           45,
           "[9] Baptiste Barbot. 2018. The dynamics of creative ideation: Introducing a new assessment paradigm."
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           46,
           "13–13. [15] Ambra Demontis, Marco Melis, Maura Pintor, Matthew Jagielski, Battista Biggio, Alina Opr"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           47,
           "[19] Denis Dumas, Peter Organisciak, and Michael Doherty. 2021. Measuring divergent thinking origina"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           48,
           "the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP. 301–314. [25] Kent"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           49,
           "Language Models. In UniReps: the First Workshop on Unifying Representations in Neural Models. [30] J"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           50,
           "arXiv:1908.01581 (2020). http://arxiv.org/abs/1908.01581 [35] Kelsey Medieros, David H Cropley, Rebe"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           51,
           "Adobe (2024). https://blog.adobe.com/en/publish/2024/04/22/age-generative-ai-over-half-americans-hav"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           52,
           "arXiv preprint arXiv:2409.04109 (2024). [44] Massimo Stella, Thomas T Hills, and Yoed N Kenett. 2023"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           53,
           "[47] C Szegedy. 2014. Intriguing properties of neural networks. Proc. of ICLR (2014). [48] Gemini Te"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           54,
           "against transfer learning. In 27th USENIX security symposium (USENIX Security 18). 1281–1297. [52] F"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           55,
           "aligned language models. arXiv preprint arXiv:2307.15043 (2023). A DIVERGENT THINKING TEST WORDING H"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           56,
           "besides your proposed uses. A.2 Forward Flow prompts. We use the following start words for Forward F"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           57,
           "each other as possible, in all meanings and uses of the words. You must follow the following rules: "
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           58,
           "5.0 2.5 0.0 2.5 5.0 7.5 T-SNE Component 1 10 5 0 5 10 T-SNE Component 2 T-SNE of DAT Sentence Embedd"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           59,
           "cos(W(𝑟1), W(𝑟2)) to measure semantic distance between embedded responses. AUT scoring. Following [1"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           60,
           "in the sequence r as the average distance between the 𝑚𝑡ℎthought in the sequence 𝑟𝑚and all preceding"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           61,
           "lower population-level originality measurements. We perform k-means clustering of TNSE of FF sentenc"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           0,
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression Xin Wang Samiu"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           1,
           "V2 proposes loss-optimized weight truncation to ensure that the truncated singular values re- sult i"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           2,
           "intensive retraining, LLM compression is often con- ducted in a post-training manner. Techniques suc"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           3,
           "few SVD-based LLM compression methods have been proposed. At a high level, these methods all focus o"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           4,
           "trix being decomposed to be positive-definite, a condition that is challenging to fulfill in practic"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           5,
           "ing various language modeling, classification, and generation tasks as well as five LLMs with variou"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           6,
           "ular, LLMs compressed by SVD-LLM V2 are able to achieve a throughput speedup of up to 2.71× compared"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           7,
           "weight matrices for compression. However, it often fails to provide the desired inference speedups ("
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           8,
           "L = ||WX −W ′X||F (1) where W is the weight matrix of the original LLM, X is the activation of W, an"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           9,
           "(R1,R2,...,RN) Allocate Compression Ratio ... ... Figure 2: Overview of SVD-LLM V2. the impact of in"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           10,
           "the details of the two main components of SVD-LLM V2: (1) heterogeneous compression ratio allocation"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           11,
           "Figure 3: Comparison between SVD-LLM and SVD-LLM V2 on the truncation loss of the query weight matri"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           12,
           "However, existing SVD-based LLM compression methods either overlook this variation or require resour"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           13,
           "model compression ratio R, the compression ratio of each weight matrix within a group is determined "
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           14,
           "the next step is to truncate the weights accord- ing to their assigned compression ratios. To re- du"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           15,
           "1: procedure WEIGHT_TRUNCATION(W, X, R) 2: S ←XXT ▷Construct matrix S from X 3: Us, Ss, Vs ←SVD(S) ▷"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           16,
           "utilize pooled covariance matrices to precisely esti- mate the feature distribution to reduce trunca"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           17,
           "bypasses the Cholesky decomposition, resulting in a more straightforward process with improved numer"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           18,
           "theoretical minimum truncation loss. Proof. Since XXT is the symmetric matrix, sup- pose that the si"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           19,
           "F = ||SVD(W × Ux × Sx)||2 F = ||SVD(W × Ux × Sx × Vx)||2 F = ||SVD(WX)||2 F = L2 min (3) Therefore, "
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           20,
           "sion methods: LLM-Pruner (Ma et al., 2023), SliceGPT (Ashkboos et al., 2024), and Block- Pruner (Zho"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           21,
           "2018), WinoGrande (Sakaguchi et al., 2020), Hel- laSwag (Zellers et al., 2019), Arc_e (Clark et al.,"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           22,
           "four aspects: (1) performance on different LLMs, (2) performance on LLMs with larger scales, (3) per"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           23,
           "relative performance gain compared to the best-performing baseline is marked in green inside bracket"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           24,
           "Original 10.87 12.50 0.28 0.66 0.65 0.50 0.76 0.25 0.52 0.29 0.01 FWSVD 14559 17898 0.03 0.08 0.02 0"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           25,
           "0.02 0.00 0.00 ASVD 17.55 28.41 0.20 0.59 0.61 0.41 0.69 0.30 0.47 0.37 0.28 SVD-LLM 11.82 20.05 0.2"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           26,
           "SVD-LLM V2 consistently outperforms all the base- lines on both 13B and 30B model sizes. Table 3: Pe"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           27,
           "performs all baselines, and the performance gain compared to the best-performing baseline increases "
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           28,
           "directly from its truncation loss, making it signifi- cantly faster than ASVD. 4.2 Inference Speedup"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           29,
           "larger than 100, thus are not shown in the figure. 1.29x 1.63x 2.08x 2.71x Figure 5: Throughput (Tok"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           30,
           "SVD-LLM V2 (T) SVD-LLM V2 7.94 7.91 (↓1%) 7.43 (↓6%) 7.12 (↓10%) 4.3 Ablation Study SVD-LLM V2 has t"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           31,
           "(i.e., heterogeneous compression ratio allocation and loss-optimized weight truncation) of SVD-LLM V"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           32,
           "compressed by SVD-LLM V2 under 20% compres- sion ratio on WikiText-2 when using the default calibrat"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           33,
           "8.78 12.73 16.39 27.41 BlockPruner 9.40 12.76 19.78 43.05 SVD-LLM V2 7.84 (↓17%) 8.48 (↓34%) 10.17 ("
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           34,
           "SliceGPT (Ashkboos et al., 2024), and Block- Pruner (Zhong et al., 2024) under various weight memory"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           35,
           "to GPTQ-3-bit under the same memory budget. In other words, we find that under the same mem- ory bud"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           36,
           "training quantization method BiLLM. In particular, SVD-LLM V2 (2-bit) achieves 69% lower perplexity "
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           37,
           "14.73 (↓69%) 5 Conclusion In this paper, we present SVD-LLM V2, a SVD-based post-training LLM compre"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           38,
           "jishirzi. 2019. Mathqa: Towards interpretable math word problem solving with operation-based for- ma"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           39,
           "Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. "
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           40,
           "ish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluat"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           41,
           "training quantization for llms. In ICML. OpenRe- view.net. Yixin Ji, Yang Xiang, Juntao Li, Wei Chen"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           42,
           "Guangxuan Xiao, Chuang Gan, and Song Han. 2024b. Qserve: W4A8KV4 quantization and sys- tem co-design"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           43,
           "former. J. Mach. Learn. Res., 21:140:1–140:67. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat- u"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           44,
           "Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fulle"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           45,
           "driguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           46,
           "Zong, Samiul Alam, Mi Zhang, and Bhaskar Krish- namachari. 2024a. The internet of things in the era "
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           47,
           "Linguistics. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Chris"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           48,
           "Li, and Yong Li. 2024. ASER: activation smoothing and error reconstruction for large language model "
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           0,
           "MEGAnno+: A Human-LLM Collaborative Annotation System Hannah Kim, Kushan Mitra, Rafael Li Chen, Sajj"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           1,
           "in scenarios where repeated usage of LLMs for inference can be too costly (e.g., API calls) or time-"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           2,
           "that LLMs can achieve near-human or even better- than-human accuracy in some tasks. Furthermore, dow"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           3,
           "tion on how to onboard LLMs as annotators within a human-in-the-loop framework in labeling tools has"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           4,
           "2 Related Work LLMs as annotators There is growing interest in utilizing LLMs as general-purpose ann"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           5,
           "confidence scores (Lin et al., 2022) and calculating consistency over prompt perturbations (Wang et "
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           6,
           "annotation suggestions generated by ML models. HumanLoop (hum) and Autolabel (aut) support the annot"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           7,
           "lacks the confidence to train a downstream model without verifying the LLM annotations. However, wit"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           8,
           "prompt templates for reuse. (e) [Metadata] LLM artifacts are captured and stored as annotation metad"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           9,
           "tate it by interacting with the programmatic LLM controller. The LLM controller takes care of 1) age"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           10,
           "2MEGAnno+ only supports full LLM-integrated work- flows for record-level tasks. 3 Figure 2: UI for c"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           11,
           "APIs, in this work, we only demonstrate OpenAI Completion models for clarity and brevity. Prompt tem"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           12,
           "prompt template. Users can utilize MEGAnno’s sophisticated subset selection techniques, includ- ing "
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           13,
           "models, or other similar errors which require the user themselves to call to the LLM API again. On e"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           14,
           "violations, ensuring that the generated label is valid within the existing schema for the task. Meta"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           15,
           "valid or invalid, 3) API call progress such as the time taken to retrieve responses from the API cal"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           16,
           "“suspicious” outputs from LLMs. Our widget fa- cilitates this process by presenting metadata, such a"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           17,
           "default parameters and prompt template. To test this setting, she runs the model on 10 samples. 1 c "
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           18,
           "ates another agent, GPT-3 with temperature with zero and re-runs annotation on the same subset. 1 mo"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           19,
           "guage inference task is more effective than framing it as a binary classification of agreement and d"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           20,
           "different results than prompting yesterday (Chen et al., 2023). Therefore, LLM annotators and hu- ma"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           21,
           "learning. Additionally, we plan to incorporate di- verse annotation workflows such as Multi-agent LL"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           22,
           "References Autolabel. Github.com/refuel-ai/autolabel. Humanloop.com. Humanloop.com. Abubakar Abid, M"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           23,
           "for Computational Linguistics. Benjamin D Douglas, Patrick J Ewell, and Markus Brauer. 2023. Data qu"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           24,
           "Richard Eckart de Castilho, and Iryna Gurevych. 2018. The INCEpTION platform: Machine-assisted and k"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           25,
           "Jonathan Robinson. 2021. Reply to mturk, prolific or panels? choosing the right audience for online "
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           26,
           "generation: Progress and challenges. In Proceedings of the 59th Annual Meeting of the Association fo"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           27,
           "Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, and Michael Zeng. 2021. Want to reduce la- belin"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           28,
           "express their uncertainty? an empirical evaluation of confidence elicitation in llms. Dan Zhang, Han"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           0,
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Inte"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           1,
           "significant difference in LLM usage and user perceptions with or without prompt guidelines and the i"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           2,
           "IUI ’24, March 18–21, 2024, Greenville, SC, USA © 2024 Copyright held by the owner/author(s). Public"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           3,
           "articles [3, 22, 35, 36]. However, online software-help seeking is a complex endeavour, demanding pr"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           4,
           "LLMs with prompt-based interactions for seeking help for feature- rich applications. In particular, "
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           5,
           "help end users harness the full potential of LLM-based assistants for feature-rich software [31]. Th"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           6,
           "assistance for users’ prompts and help users finish their software tasks. However, our findings show"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           7,
           "ception of software features, regardless of implicit enhancements in the underlying model or explici"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           8,
           "access to help information, they often get lost in search results and forum posts and still face dif"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           9,
           "learnability and easing help-seeking processes [11, 14, 17, 25], it is unclear whether users’ learni"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           10,
           "[1, 43]. For example, some experimental work is being explored by integrating LLMs directly into fea"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           11,
           "2.3 Prompt-based interactions To leverage the potential of LLMs, a lot of the focus in HCI and AI re"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           12,
           "ing and non-deterministic output) inherent within prompt-based interactions of LLMs. Considering LLM"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           13,
           "Microsoft and others to enhance Generative AI tools [42, 44]; and, 2) directly integrating domain co"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           14,
           "than Baseline ChatGPT. • H3: Users will trust SoftAIBot more than Baseline ChatGPT. • H4: Users will"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           15,
           "ucation (2 Diploma, 4 Bachelor’s, 5 Master’s, 5 PhD). We recruited participants mainly from our univ"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           16,
           "particular software contexts. Automatic Prompt Guidelines: The SoftAIBot UI interface (See Figure 1)"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           17,
           "umentation by using Facebook AI Similarity Search (FAISS) index and vector search. Our custom API se"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           18,
           "plus based on GPT-4, where users can type in their query and LLM provide assistance to users for var"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           19,
           "project timeline in Microsoft PowerPoint that is visual and animated. 3.3 Choice of Application and "
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           20,
           "participant variability. To eliminate order effects, we used a Latin Square counterbalancing [38] wi"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           21,
           "ware application using given LLM intervention. After completing each of the 4 tasks, participants we"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           22,
           "recordings to evaluate two key aspects: how users sought help from LLM assistants (e.g., formulated "
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           23,
           "ing LLM assistance to complete the study tasks accurately, we evaluated how accurately users identif"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           24,
           "in applying the LLM assistance to the software task. 4 RESULTS 4.1 Task Completion, Accuracy and Rel"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           25,
           "icant differences (PowerPoint: t(24.5) =5.4, p<0.0001, two-tailed; Excel: t(24.8) =4.8, p<0.0001, tw"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           26,
           "Task Completion and Task Accuracy: None of the partic- ipants were able to completely finish either "
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           27,
           "ChatGPT (maximum= 50% and minimum= 0%). Although users Why and When LLM-Based Assistants Can Go Wron"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           28,
           "achieved better task accuracy scores with SoftAIBot in comparison to Baseline, paired-sample t-test "
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           29,
           "no significant difference in perceived accuracy (𝜒2(3, 𝑁= 28) = 3.96, p =0.27) and relevancy (𝜒2(2, "
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           30,
           "both LLMs. Furthermore, having more accurate and relevant LLM output did not impact task completion "
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           31,
           "participants were inconsistent and varied in how they constructed prompts, failing to leverage the i"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           32,
           "all of that data that it has in the back end..it’s so quick that it goes through it within nanosecon"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           33,
           "tasks that involved references to different visual and interactive elements and participants frequen"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           34,
           "it [prompt guidelines] but is not necessary. I knew what to search for. I do not think prompt guidel"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           35,
           "application (P07). complete the software tasks accurately. Below, we discuss some factors that shape"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           36,
           "ticipants who were infrequent users of Powerpoint and Excel also struggled because they lacked an ac"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           37,
           "but sometimes it gets difficult to use...If I get an image or graphical help along with screenshots "
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           38,
           "asking steps for each functionality at one time (P02). user interface. Instead of having awareness o"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           39,
           "4.4 Coherent LLM Output Leads to Blind Faith The presentation of LLM output fosters trust: The most "
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           40,
           "it [SoftAIBot] because I liked the way it gave these steps. SoftAIBot is more specific and gave me s"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           41,
           "biases and limitations. Participants attributed their inability to com- plete a task and locate sugg"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           42,
           "tions and misconceptions about these systems is imperative. By shedding light on the lack of awarene"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           43,
           "context, performed better and generated more relevant and accurate step-by-step software assistance."
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           44,
           "not be dismissed as instructional tools. Instead, to mitigate the issue of locating the exact instru"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           45,
           "it gibberish and still it works. I [will] doubt myself before doubting AI.” (P15) Such overtrust in "
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           46,
           "why the system did what it did and verify an AI’s recommendation [13] by demonstrating the similarit"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           47,
           "to be walk-up-and-use and support natural language interaction. However, similar to insights from re"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           48,
           "in documenting the challenges users encounter with LLMs for soft- ware help tasks, including their m"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           49,
           "to feature-rich applications. While our findings shed new light on users struggles in employing LLMs"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           50,
           "expectations and AI realities. By addressing these challenges head- on, we can foster a future where"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           51,
           "https://doi.org/10.1145/1621995.1622022 [4] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           52,
           "Example-Centric Programming: Integrating Web Search into the Development Environment. In Proceedings"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           53,
           "Inc., 1877–1901. https://proceedings.neurips.cc/paper_files/paper/2020/file/ 1457c0d6bfcb4967418bfb8"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           54,
           "(CHI ’12). Association for Computing Machinery, New York, NY, USA, 1549–1558. https://doi.org/10.114"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           55,
           "mer. 2020. ReMap: Lowering the Barrier to Help-Seeking with Multimodal Search. In Proceedings of the"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           56,
           "[18] Tovi Grossman, George Fitzmaurice, and Ramtin Attar. 2009. A Survey of Software Learnability: M"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           57,
           "signing Explainable Chatbot Interfaces for Enhancing Usefulness, Transparency, and Trust. In 2021 IE"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           58,
           "Association for Computing Machinery, New York, NY, USA, 4017–4026. https: //doi.org/10.1145/2556288."
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           59,
           "[26] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, "
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           60,
           "Wanling Ding, Tom M. Mitchell, and Brad A. Myers. 2018. APPINITE: A Multi-Modal Interface for Specif"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           61,
           "Inc., 1950–1965. https://proceedings.neurips.cc/paper_files/paper/2022/file/ 0cde695b83bd186c1fd4563"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           62,
           "Association for Computing Machinery, New York, NY, USA, 97–104. https: //doi.org/10.1145/1621995.162"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           63,
           "Technical Report MSR-TR-2022-12. Microsoft. https://www.microsoft.com/en- us/research/publication/ov"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           64,
           "[42] Jessica Shieh. 2023. Best practices for prompt engineering with openai API: Openai help center."
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           65,
           "for Computing Machinery, New York, NY, USA, 212–228. https://doi.org/10. 1145/3490099.3511119 [46] G"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           66,
           "[49] Helena Vasconcelos, Matthew Jörke, Madeleine Grunde-McLaughlin, Tobias Gerstenberg, Michael S. "
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           67,
           "[52] J.D. Zamfirescu-Pereira, Richmond Y. Wong, Bjoern Hartmann, and Qian Yang. 2023. Why Johnny Can"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           0,
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis Saranya Venkatraman, N"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           1,
           "and present baselines for LLM-LLM collabora- tion. We find that current baselines are not able to ha"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           2,
           "OLMO MISTRAL Part 2 Part 3 Part 4 Part 5 ORCA Figure 1: CollabStory contains over 32k creative sto- "
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           3,
           "ration with human authors or with a single LLM. Therefore, this study explores collaborative cre- at"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           4,
           "largest dataset to present multi-LLM or machine-machine collaborative generation. LLMs, i.e. LLM-LLM"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           5,
           "is a crucial setting to study in the context of writ- ing tasks since LLM-LLM collaboration could po"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           6,
           "to replicate a scenario in which commonly used LLMs from different organizations are being used in c"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           7,
           "(such as credit assignment, and legality of usage) arising in the generative AI landscape. As an exa"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           8,
           "gumentative stories (Lee et al., 2022). It is interesting to consider what it means for an entity to"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           9,
           "or “write a story as an angry politician”), the text being generated by LLMs seem to still have some"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           10,
           "tively. Beyond creative writing, (Zeng et al., 2024) developed the first machine-human academic essa"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           11,
           "ment, we refer to each of the LLMs as “authors\". For cases where we refer to the human author, we sp"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           12,
           "OLMo-7B-Instruct # Number # Words per Author / # Author Order # Prompts per # Stories Authors Huggin"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           13,
           "average of the dataset. The average length (num- ber of words) of articles in the test set is 675.75"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           14,
           "order, we then generate stories using each of the prompts from a unique set of prompts per N. Our go"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           15,
           "der 80 words, allowing LLMs to generate longer sequences. We also included the last sentence of the "
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           16,
           "Gemma Mistral Time and time again, you failed. Today, you succeeded. The golden trophy gleamed in yo"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           17,
           "limits. You had never believed in destiny or fate, but looking at the coach, you couldn’t help but t"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           18,
           "a sea of adoration and support, and felt a surge of determination wash over you. You would not let t"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           19,
           "nation. It was the culmination of countless sleepless nights, tire- less training sessions, and the "
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           20,
           "Today, you succeeded.” As the story progresses, new characters are introduced (such as the coach hig"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           21,
           "from our dataset can be read in detail in Table 4. 4 Dataset Analysis We compare the LLM-generated s"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           22,
           "7https://github.com/HLasse/TextDescriptives Figure 2: N on the X-axis denotes the number of authors,"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           23,
           "or introducing multiple LLMs in the generation setting does not disturb the quality of the stories g"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           24,
           "cabulary to maintain coherence, consistency, and fluency, which can restrict lexical variety compare"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           25,
           "to measure if the different story parts generated sequentially by different LLM authors followed a l"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           26,
           "ity scores to story parts that were not consecutive. This type of A/B testing is a means of demonstr"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           27,
           "a total of 600 pairs) and ask it the following ques- tion: “Does Part 2 serve as a good continuation"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           28,
           "As illustrated by this example, it is crucial to note that our prompt encourages the model to evalua"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           29,
           "in everyone. The villagers liked her for her pure heart, and her kindness was well-known throughout "
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           30,
           "light within. Drawing his weapon, he prepared for the unknown dangers that lay beyond. As he pushed "
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           31,
           "crease in both the Neither Wins and Incorrect Wins scenarios (see Table 5) where Incorrect Wins goes"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           32,
           "scenario. We then fine-tune and report performance using the following 5 baseline methods: Multino- "
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           33,
           "more distinct from single-authored ones. We con- jecture that introducing more authors in the articl"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           34,
           "this task, we used all the sentences at LLM-LLM boundaries, that is the last sentence of part i and "
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           35,
           "more detailed results for each LLM or class-wise F1 scores in the Appendix (Table 9). Thus, from the"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           36,
           "scenario where text is generated entirely by multi- ple LLMs. Our work addresses this extreme case, "
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           37,
           "dress incoming challenges brought by LLM-LLM interactions. Limitations Our work demonstrates one way"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           38,
           "is important to avoid deception or wrongful con- tent attribution. With creative writing tasks, it i"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           39,
           "Linguistics. Wissam Antoun, Djamé Seddah, and Benoît Sagot. 2024. From text to source: Results in de"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           40,
           "ings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6848–6863, Ab"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           41,
           "gia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           42,
           "Haijun Xia. 2023. Metaphorian: Leveraging large language models to support extended metaphor cre- at"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           43,
           "Low. 2024. Protecting text ip in the era of llms with robust and scalable watermarking. In ICML Work"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           44,
           "dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta:"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           45,
           "har, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from comple"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           46,
           "International Conference on Computational Linguis- tics, Language Resources and Evaluation (LREC- CO"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           47,
           "based on gemini research and technology. arXiv preprint arXiv:2403.08295. Hugo Touvron, Louis Martin"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           48,
           "Interaction strategies with generated text in human-ai collaborative fiction writing. In Joint Inter"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           49,
           "Yixin Huang, Zhenyang Sun, Shilin Zhang, Weiye Li, Zhengyan Fu, Yao Wan, et al. 2024a. Llm-as-a- coa"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           50,
           "input. This ensured that the generating LLM has ac- cess to the last sentence and the overall storyl"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           51,
           "parts often began with a short title for the section it was to generate surrounded by “###” for exam"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           52,
           "Task 1: Predict multi-author or not In the rapidly expanding and fiercely competitive market for LLM"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           53,
           "Task 3: Author Verification With LLMs increas- ingly paraphrasing and editing each other’s texts, it"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           54,
           "Plagiarism detection Style Change Detection and Attribution Finding all positions in the text where "
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           55,
           "177.89 ± 16.57 Lexical Diversity Gemma 0.67 ± 0.04 0.67 ± 0.05 0.70 ± 0.07 0.69 ± 0.07 0.68 ± 0.06 L"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           56,
           "Mistral 81.04 ± 8.58 83.99 ± 8.40 81.59 ± 10.07 82.02 ± 9.09 79.91 ± 9.44 Olmo 80.78 ± 9.01 83.31 ± "
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           57,
           "0.45 ± 0.08 0.45 ± 0.08 0.46 ± 0.08 0.47 ± 0.07 Table 8: Descriptive Statistics or Features for stor"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           58,
           "0.99 0.76 SVM - 0.61 0.68 0.58 0.97 0.71 BERT - 0.70 0.71 0.64 0.99 0.76 ALBERT - 0.78 0.73 0.70 0.9"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           59,
           "RoBERTa - 0.71 0.71 0.67 0.96 0.76 N=4 Method Orca Olmo Llama Mistral Gemma AVG MNB - 0.58 0.65 0.63"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           0,
           "1 On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models Sarah R Gao"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           1,
           "The dataset we created will be shared publicly on Github, under @andrewgcodes (https://github.com/an"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           2,
           "● Scipy [9] ● Plotly [10] ● Numpy [11] ● Scikit-learn [12] ● Radial Tree [13] ● NLTK [14] ● Matplotl"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           3,
           "\"falcon-7b\"). The number of parameters in millions was recorded in a column named 'params_millions' "
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           4,
           "chart. Word Clouds To understand the contents of our agglomerative clusters, we generate Word Clouds"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           5,
           "each other than to models in other groups. The detected communities were used for subsequent visual "
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           6,
           "6 model_name, link, downloads, likes, ReadMeLink, and params_millions. Rank is assigned in order of "
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           7,
           "benefit to the user to like a model on Hugging Face, while downloading the model is beneficial. We g"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           8,
           "method to detect communities. Figure 5. Graph of models with more than 10,000 downloads, with commun"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           9,
           "high-level view of prominent model families, while the graph-based visualization depicts the relatio"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           10,
           "References 1. Gao, A. (2023, July 8). Prompt Engineering for Large Language Models. SSRN; SSRN. http"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           11,
           "6. Beautiful Soup Documentation. (n.d.). Tedboy.github.io. Retrieved July 19, 2023, from https://ted"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           12,
           "https://scikit-learn.org/stable/user_guide.html 13. koonimaru. (2023, June 15). radialtree. GitHub. "
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           13,
           "18. Mueller, A. (2020, May 7). amueller/word_cloud. GitHub. https://github.com/amueller/word_cloud 1"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           14,
           "lm 108 tiny 106 13 1 106 4 105 8bit 103 headless 101 codegen 97 33b 97 deduped 95 sharded 89 airobor"
          ]
         ],
         "hovertemplate": "x=%{x}<br>y=%{y}<br>document=%{customdata[0]}<br>chunk=%{customdata[1]}<br>preview=%{customdata[2]}<br>color_id=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": {
           "bdata": "ERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIDg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDxISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDRAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQExMTExMTExMTExMTExMTExMTExMTExMTExMTExMTExMTExMTExMTExMTExMTExMTEwkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoBAQEBAQEBAQEBAQEBAQE=",
           "dtype": "i1"
          },
          "coloraxis": "coloraxis",
          "opacity": 0.6,
          "size": 6,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "showlegend": false,
         "type": "scattergl",
         "x": {
          "bdata": "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",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "color_id"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "legend": {
         "tracegroupgap": 0
        },
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Interactive PCA Projection of Chunks"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "y"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"color_id\",\n",
    "    hover_data=[\"document\", \"chunk\", \"preview\"],\n",
    "    title=\"Interactive PCA Projection of Chunks\"\n",
    ")\n",
    "fig.update_traces(marker=dict(size=6, opacity=0.6))\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-playground-ABXqpzBr-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
