{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "757f0f5a",
   "metadata": {},
   "source": [
    "# Processing: Load, Chunk, and Prepare for Vector DB\n",
    "\n",
    "This notebook demonstrates a step-by-step approach to loading, enriching,\n",
    "chunking, and storing documents into a vector database.\n",
    "It compares different outputs at each stage interactively with helpful visualizations.\n",
    "\n",
    "Create from scratch a chain that: \n",
    "- takes an input document. (In this case many Arxiv pdf documents.)\n",
    "- Chunks the document, keeping the metadata.\n",
    "- Embed the document chunks.\n",
    "- Saves the embeddings into a vector DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f71e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c404115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Supports all arguments of `ArxivAPIWrapper`\n",
    "loader = ArxivLoader(\n",
    "    query=\"LLM\",\n",
    "    load_max_docs=50,\n",
    "    top_k_results=20\n",
    "    # doc_content_chars_max=1000,\n",
    "    # load_all_available_meta=False,\n",
    "    # ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0df738bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cca765",
   "metadata": {},
   "source": [
    "# Splitter - (There are the variables that change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "860cd865",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1f7b754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': None,\n",
       " 'metadata': {'Published': '2024-12-23',\n",
       "  'Title': 'Trustworthy and Efficient LLMs Meet Databases',\n",
       "  'Authors': 'Kyoungmin Kim, Anastasia Ailamaki',\n",
       "  'Summary': 'In the rapidly evolving AI era with large language models (LLMs) at the core,\\nmaking LLMs more trustworthy and efficient, especially in output generation\\n(inference), has gained significant attention. This is to reduce plausible but\\nfaulty LLM outputs (a.k.a hallucinations) and meet the highly increased\\ninference demands. This tutorial explores such efforts and makes them\\ntransparent to the database community. Understanding these efforts is essential\\nin harnessing LLMs in database tasks and adapting database techniques to LLMs.\\nFurthermore, we delve into the synergy between LLMs and databases, highlighting\\nnew opportunities and challenges in their intersection. This tutorial aims to\\nshare with database researchers and practitioners essential concepts and\\nstrategies around LLMs, reduce the unfamiliarity of LLMs, and inspire joining\\nin the intersection between LLMs and databases.'},\n",
       " 'page_content': 'Trustworthy and Efficient LLMs Meet Databases\\nKyoungmin Kim\\nkyoung-min.kim@epfl.ch\\nEPFL\\nSwitzerland\\nAnastasia Ailamaki\\nanastasia.ailamaki@epfl.ch\\nEPFL\\nSwitzerland\\nAbstract\\nIn the rapidly evolving AI era with large language models (LLMs) at\\nthe core, making LLMs more trustworthy and efficient, especially in\\noutput generation (inference), has gained significant attention. This\\nis to reduce plausible but faulty LLM outputs (a.k.a hallucinations)\\nand meet the highly increased inference demands. This tutorial\\nexplores such efforts and makes them transparent to the database\\ncommunity. Understanding these efforts is essential in harness-\\ning LLMs in database tasks and adapting database techniques to\\nLLMs. Furthermore, we delve into the synergy between LLMs and\\ndatabases, highlighting new opportunities and challenges in their\\nintersection. This tutorial aims to share with database researchers\\nand practitioners essential concepts and strategies around LLMs,\\nreduce the unfamiliarity of LLMs, and inspire joining in the inter-\\nsection between LLMs and databases.\\n1\\nIntroduction\\nLarge language models (LLMs) have recently transformed various\\nfields with their ability to understand and generate human-like text.\\nIn the database domain, researchers are leveraging LLMs to tackle\\ncomplex data management tasks [55, 194]. LLMs can function not\\nonly as assistants for database administrators (DBAs) [271, 340] but\\nalso as internal components of database systems, optimizing query\\nplans [8, 168] and translating natural languages to SQLs [224].\\nBeyond these applications, key concepts and advancements from\\nthe LLM community remain underexplored by database researchers.\\nThis tutorial aims to bridge that gap by focusing on enhancing the\\ntrustworthiness and efficiency of LLMs. Improving trustworthiness\\ninvolves reducing hallucinations [124] to ensure LLMs generate\\naccurate, factual responses, thereby increasing their reliability in\\ndatabase tasks requiring precise answers and reasoning. Enhancing\\nefficiency focuses on decreasing inference latency and boosting\\nthroughput.\\nInference efficiency is particularly important because, while\\ntraining LLMs demands substantial resources and expertise, infer-\\nence occurs daily across numerous users, leading to significant oper-\\national costs. For instance, OpenAI handles millions of requests, in-\\ncurring substantial monthly expenses to run ChatGPT [73, 215]. In-\\ntegrating LLMs with external data sources, such as vector databases\\nand document retrieval systems in retrieval-augmented generation\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\\nConference’17, July 2017, Washington, DC, USA\\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM\\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\\n(RAG) [154], increases the number and complexity of LLM calls, es-\\npecially with longer inputs. Recent trends in chain-of-thought and\\nmulti-path reasoning, exemplified by models like OpenAI’s o1 [205],\\nfurther amplify inference demands, as generating final answers may\\nrequire multiple LLM calls to enhance trustworthiness.\\nFrom a systems perspective, improving LLM inference efficiency\\nparallels database management system (DBMS) development, pre-\\nsenting opportunities for database researchers to contribute to\\ncreating more efficient LLMs, promoting economic and environ-\\nmental sustainability by reducing the CO2 footprint associated with\\nextensive GPU usage.\\nAfter introducing the essential ideas in making LLMs more trust-\\nworthy and efficient, we will explain the intersection of LLMs and\\ndatabases with new challenges and opportunities.\\n1.1\\nTarget Audience and Prerequisites\\nOur tutorial is designed for conference attendees, focusing on three\\nkey areas to maximize engagement:\\nTrustworthy LLMs (Section 2.1): Aimed at individuals seeking to\\neffectively utilize large language models (LLMs) in database tasks\\nwith minimal errors. Prerequisites include experience with LLMs\\nlike ChatGPT and the distinction between training and inference\\nin machine learning. No in-depth knowledge of LLM internals is\\nrequired.\\nEfficient LLMs (Section 2.2): Targeted at those interested in en-\\nhancing LLM inference efficiency or contributing to the develop-\\nment of fast LLM inference systems by applying database tech-\\nniques. Prerequisites include basic database knowledge and an\\nunderstanding of GPUs. Familiarity with Transformer architecture,\\nattention mechanisms, and key-value (KV) caching is advantageous.\\nLLMs Meet Databases (Section 2.3): Intended for participants ex-\\nploring new research opportunities at the intersection of databases\\nand LLMs. A background in databases, including OLAP, relational\\nalgebra, cost-based query optimization, and approximate/adaptive\\nquery processing, will be helpful.\\nOur goal is to bridge the gap between essential LLM knowledge\\nand the database community, enabling researchers already utiliz-\\ning LLMs to uncover and develop unexplored ideas. Rather than\\nmerely listing state-of-the-art papers, we employ consistent visu-\\nals and focus on core concepts and insights, facilitating a deeper\\nunderstanding and navigation of the evolving LLM landscape.\\n1.2\\nTutorial Length\\nThe intended length of this tutorial is 1.5 hour, with 40, 30, and 20\\nminutes each for Sections 2.1, 2.2, and 2.3, respectively.\\n2\\nTutorial Outline\\nThe tutorial is structured into three main sections, addressing crit-\\nical aspects of LLMs and their interplay with database systems.\\narXiv:2412.18022v1  [cs.DB]  23 Dec 2024\\nImproving Bare LLMs (§2.1.2)\\nFine-tuning, LoRA, RLHF\\nBackground (§2.1.1)\\nAutoregressive training, in-context learning\\nHallucination, lost-in-the-middle problem\\nScaling laws\\nMaking LLMs Interact with the World (§2.1.3)\\nKnowledge/memory/tool retrieval\\nMaking LLMs Self-drive (§2.1.4)\\nSelf-reflection, adaptivity\\nChain-of-thought, multi-hop, \\nmulti-path reasoning\\nAgentic LLMs, network of LLMs\\nSemantic variable, compound AI\\nBackground (§2.2.1)\\nAttention operation, key-value (KV) caching\\nPrefill/decode/preempt/refill stages\\nPipeline/model parallelism\\nLLMs Behave as DBMSs (§2.2.2)\\nContinuous batching, KV cache paging\\nPrefill-decode disaggregation\\nKV cache/attention offloading\\nNano-batching\\nOperation (§2.2.3)\\nFlash/sparse/flex attention \\nData (§2.2.4)\\nKV compression, model quantization, SSM\\nHardware (§2.2.5)\\nRoofline model\\nWorkload (§2.2.6)\\nScheduling, prefix KV sharing, speculation\\nDBAs and DBMS Internal (§2.3.1)\\nDatabase tuning, query optimization, text2sql\\nAdaptive Cost-based Scheduling (§2.3.2)\\nCost model for LLMs, CSP\\nMixed Relational-LLM Workload (§2.3.3)\\nSemantic operators, benchmark\\nMulti-objective Query Optimization (§2.3.4)\\nAccuracy-efficiency trade-off\\nLLM-database System (§2.3.5)\\nIntegration with OLAP databases\\nConvergence and Future (§2.3.6)\\nDisaggregation, adaptive query processing\\nTrustworthy LLMs (§2.1)\\nEfficient LLMs (§2.2)\\nLLMs Meet Databases (§2.3)\\nFigure 1: Tutorial outline (each subsection with keywords).\\nFigure 1 visualizes the outline with keywords for each subsection.\\nSection 2.1 focuses on improving the trustworthiness of LLMs, ex-\\nploring challenges such as hallucination and context limitations\\nwhile presenting state-of-the-art solutions to improve the accu-\\nracy and reliability of generated outputs. Section 2.2 emphasizes\\nefficiency, covering optimization strategies for inference, data man-\\nagement, and hardware utilization. Finally, Section 2.3 highlights\\nthe convergence of LLMs and databases, exploring opportunities for\\nintegration, new workloads, and emerging system designs. Since\\nthe field is changing fast, we will regularly reflect new information\\nuntil the tutorial date.\\n2.1\\nTrustworthy LLMs\\nThe first part of the tutorial explains the efforts to reduce halluci-\\nnations and make LLMs more trustworthy, using an analogy that\\nLLMs resemble humans. We explain background (Section 2.1.1),\\nhow LLMs can solely improve (Section 2.1.2), how LLMs can im-\\nprove by interacting with the external world (Section 2.1.3), and\\nhow LLMs can automatically make such decisions and interact with\\nother LLMs (Section 2.1.4).\\n2.1.1\\nBackground. Large Language Models (LLMs) function as\\ntext-in, text-out systems, generating texts based on their training.\\nTraining an LLM is akin to nurturing a child: by exposing it to\\nextensive text data, the model acquires world knowledge and rea-\\nsoning abilities. This process involves predicting the most probable\\nnext token in a sequence, a type of self-supervised learning. For a\\nsequence of tokens, the model learns to predict the latter tokens\\nbased on the preceding ones, enabling it to generate coherent text\\ncontinuations.\\nFine-tuning refines this process for specific tasks or domains,\\nsimilar to how individuals specialize in particular professions. In\\ncontrast, in-context learning provides additional information or\\nexamples within the input without altering the model’s parameters,\\nakin to consulting external references during an open-book exam.\\nMany prompting techniques [19, 34, 112, 240, 247, 258, 275, 319]\\nincluding chain-of-thought prompting [144, 289] and its variants\\n[18, 312] may leverage in-context learning to enhance performance.\\nDuring inference, LLMs generate texts autoregressively, produc-\\ning one token at a time. This process may involve deterministic\\nmethods like greedy or beam search, or probabilistic approaches\\nsuch as nucleus sampling [80, 114], which helps avoid selecting\\nlow-probability tokens.\\nHowever, LLMs experience hallucinations [13, 124, 270, 305],\\ngenerating plausible-sounding but incorrect or fabricated infor-\\nmation. This is an unavoidable aspect of LLMs [13, 305] which\\narises from limitations in capturing real-world knowledge, inherent\\napproximations in training and inference, input noise, etc. Even\\nslight input perturbations can significantly influence hallucinations\\n[65, 101], and the detection of hallucinations has been a major\\nproblem [47, 54, 77, 195, 204, 233, 264].\\nAdditionally, the lost-in-the-middle problem [117, 181] indicates\\nthat LLMs may struggle to utilize information located in the middle\\nof long contexts, often performing better when relevant informa-\\ntion is at the beginning or end of the input, exhibiting a U-shaped\\nperformance curve. This phenomenon has been attributed to inher-\\nent attention biases within LLMs, where tokens at the start and end\\nof the input receive higher attention, regardless of their relevance\\n[117]. This tendency can lead to increased hallucinations as context\\nlengthens [230].\\nScaling laws [113, 130, 221] explain that error rates decrease as\\nmodel size and training data increase, with optimal scaling requiring\\nproportional growth in both [113]. However, this may not hold for\\nsmaller models [221]. Laws can also relate to temporal loss in the\\ntraining curve [297], downstream tasks [121], model quantization\\n[306], transfer learning [14, 111], number of generated samples [27],\\nand inference time [205] with the advance of using long, complex\\nreasoning paths. Due to automatic prompting techniques [43, 136,\\n255, 302, 330] and that larger models tend to be less sensitive to\\nprompt variations [75], we focus less on prompting techniques.\\nTarget. The audience will distinguish pre-training, fine-tuning, and\\nin-context learning phases of LLMs, and understand the inherent\\nchallenges in making LLMs trustworthy.\\n2.1.2\\nImproving Bare LLMs. We briefly explain the approaches to\\nimprove the LLM itself to make it more trustworthy. Since LLM\\nis a specific class of machine learning (ML) models, general ML\\napproaches to enhance accuracy may work for LLMs. However, as\\nsuch approaches have been extensively studied from the classic ML\\nera, we target more LLM-specific approaches.\\nAs it is infeasible to increase the model size indefinitely, and the\\nmodels typically follow the Transformer architecture [274], efforts\\nhave been put to increase or augment training data (where LLMs\\nthemselves can be used to generate data) [36, 64, 82, 88, 142, 157,\\n163, 235, 241, 279, 341], improve data quality (again, LLMs can be\\nused to clean data) [23, 66, 72, 103, 143, 328], make inferences more\\nrobust [91, 276], and apply better training and fine-tuning methods.\\nSpecifically, fine-tuning covers a broad spectrum of work for\\nexample, parameter-efficient fine-tuning (PEFT) [116, 119, 152, 162,\\n222], instruction tuning [51, 198, 199, 243, 285, 288], reinforce-\\nment learning from human feedback (RLHF) [30, 52, 57, 81, 102,\\n134, 167, 208, 250, 262], and direct preference optimization (DPO)\\n[83, 138, 234, 338]. RLHF leverages human feedback to train a\\nreward model in reinforcement learning (RL), guiding the LLM\\nthrough RL to produce desired outputs. DPO simplifies the align-\\nment process by directly optimizing the policy model without a\\nseparate reward model. While these approaches rely on RL that\\ncontinuously interacts with human or the world external to LLMs,\\nsuch interactions are often limited to training and do not occur in\\ninferences, which we explain in the following sections.\\nOther than the training methods, a new model architecture of\\ndifferential Transformer [315] reduces the distractions of the models\\nto focus on unnecessary information in the long context, which\\nworks similarly to robust decoding strategies [91, 276].\\nTarget. The audience will learn about tuning LLMs to make them\\nmore trustworthy and aligned with user intentions.\\n2.1.3\\nMaking LLMs Interact with the World: Adding Eyes and Hands.\\nLLMs alone can encounter knowledge, memory, and capability limi-\\ntations [326]. Their knowledge is confined to the static information\\nencoded during training, leading to potential inaccuracies over\\ntime. Memory constraints arise from limited context windows, hin-\\ndering the handling of extended conversations. Additionally, their\\ntext-based nature restricts interactions with the physical world. To\\naddress these challenges, LLMs can retrieve knowledge, memory,\\nand tools.\\nThis section focuses on what and how to retrieve. When to re-\\ntrieve is the key to autonomy and will be detailed in the next section.\\nKnowledge retrieval is represented by well-known retrieval-\\naugmented generation (RAG) [76, 90, 126, 154, 166]. Based on the\\ndata type, it can fetch knowledge from knowledge graphs [38, 109,\\n169, 211, 225, 245, 266, 301, 309], tables [9, 22, 40, 45, 96, 98, 115, 128,\\n150, 158, 190, 265, 320], images [41, 42, 314], not just documents.\\nThe data may be chunked/vectorized, stored in vector databases,\\nthen similar chunks are searched online. While vector similarities\\nare typically used, more advanced similarity scores are possible,\\ne.g., using dual or cross encoders [203, 239].\\nMemory retrieval attempts to overcome the limited context\\nsize of LLMs by storing previously seen tokens as key-value pairs\\n[25, 183, 193, 281, 294] and fetching relevant pairs in upcoming\\nrequests, managing memory stores in hierarchical or partitioned\\nway [145, 287] or even as a database [118]. Fetching information\\nfrom long input can also be done without maintaining a separate\\nmemory store, but by sparsifying the model layers [15, 186]. One\\ncan relate low-rank adapters and mixture-of-experts [29, 71, 78,\\n110, 119, 155, 228, 292] with memory retrieval since lightweight\\nmodel parameters are fine-tuned per specific task and domain, and\\ndynamically fetched at online inferences.\\nTool retrieval searches for the APIs to interact with external\\nenvironments [188, 197, 218, 229, 232, 246, 286, 300]. One can con-\\nnect LLMs with databases to call SQLs that can help answering\\nuser questions [22]. Constrained decoding [20, 69, 92, 93] allows\\noutput to follow specific structure which can increase correctness\\nand efficiency.\\nThe challenges in retrieval include the followings. 1) Heterogene-\\nity: LLMs are text-based, but knowledge can be of any type. Even for\\ntext retrieval, heterogeneous lengths and intents between queries\\nand documents can lead to suboptimal retrieval accuracy [74, 89],\\nand the vector-similarity search may be too simple to retrieve neces-\\nsary information [87, 210]. 2) Scalability: Not only that LLMs have\\nlimited context or data they can utilize per inference, but main-\\ntaining a large set of retrieval entities and retrieving a subset may\\nincur overheads [269, 303]. While approximation can mitigate the\\nsearch overhead and make the search negligible to LLM inference\\ncosts, it is limited to vector-similarity search, and generalization\\nto more complex searches [131, 135, 137, 239] remains challenging.\\n3) Sparsity: This is also relevant to data sparsity and noise [56],\\nwhere relevant data is sparse compared to large information pools.\\n4) Reliability: Retrieved knowledge may be imperfect [277].\\nTarget. The audience will understand how LLMs can interact with\\nthe world and exploit external knowledge to overcome the limits\\nin using LLMs alone.\\n2.1.4\\nMaking LLMs Self-drive: Adding Brain. Now we have more\\npowerful models and interactions with the world. The last part is\\nhow we can make LLMs smart enough to maximize these capacities,\\nadding autonomy. Self-consistency and major voting enables a sim-\\nple yet effective solution for increasing consistency [283], however,\\nit fails to generate accurate and diverse answers [33, 44, 46] and\\nis yet passive. More active approaches include self-reflection and\\nadaptive retrieval [11, 32, 123, 125, 159, 185, 331], which adaptively\\nretrieves information multiple times based on the generated output,\\nmodel confidence, query complexity, or fine-tuned policies. This is\\nparticularly helpful for chain-of-thought/multi-hop reasoning and\\nquestion answering [175, 278, 282, 284, 329].\\nThe next step is to use multiple reasoning paths instead of a single\\npath. This multi-path reasoning has been an effective approach\\nfor driving LLMs [224, 226, 324, 332]. While the exact mechanism\\nremains closed, OpenAI’s o1 model is assumed to plan subtasks,\\nconduct these, and revise the results to decide whether to extend\\nthe current plan or generate different plans, forming a tree-like\\nreasoning structure. They suggest a new scaling law that LLM\\naccuracy increases with inference time, not only with training time\\nand data [205].\\nAgentic LLM indicates that LLMs can act as agents, selecting\\nactions based on observations [49, 179, 256, 313]. Multiple agents\\nexploit collaborative reasoning, parallel processing, diversity, and\\nspecialization akin to humans [35, 104, 172, 212, 214, 227, 299].\\nSemantic variables [173] regard LLM input and output tokens as\\ndependent variables to explicitly model control flows.\\nA broader view includes compound AI [323] where AI and non-\\nAI components interact with each other, including retrievals, control\\nflows, agentic LLMs, and more. An interesting example is automated\\nresearch process [187, 259].\\nTarget. The audience will learn about approaches to make LLMs\\nself-driving and build systems around LLMs for complex tasks.\\n2.2\\nEfficient LLMs\\nThe second part of the tutorial demystifies the internals of LLM\\ninference process and explains the efforts to make it more efficient,\\nusing an analogy that LLMs behave as DBMSs. We explain back-\\nground (Section 2.2.1) and how LLM inference systems resemble\\nDBMSs in improving their efficiency (Section 2.2.2). We then ex-\\nplain further work for each dimension of operation (Section 2.2.3),\\ndata (Section 2.2.4), hardware (Section 2.2.5), and workload (Section\\n2.2.6).\\n2.2.1\\nBackground. The dominant Transformer architecture em-\\nploys an attention mechanism [274] that calculates similarity scores\\nbetween a token and its preceding tokens, effectively capturing\\ninter-token relationships and managing extended contexts. This\\nprocess has quadratic complexity, but key-value (KV) caching [58]\\noptimizes it by storing and reusing these computations, reducing\\nthe complexity to linear during inference. Non-attention operations\\nmostly consist of matrix multiplications and activations.\\nInference in Large Language Models (LLMs) involves two pri-\\nmary phases: prefill and decode. During the prefill, the model pro-\\ncesses input tokens to generate the initial output token. The at-\\ntention operates with quadratic complexity due to the absence of\\nprecomputed KVs, making it compute-intensive. During the decode,\\nthe model generates subsequent tokens sequentially, each time us-\\ning the last generated token as input. Here, the attention leverages\\nKV caching, resulting in linear complexity relative to the number\\nof processed tokens and reading their KVs, which makes this phase\\nmore memory-intensive.\\nIn case of multiple requests, they face a race condition as in\\nmulti-tenant systems. If the GPU memory is insufficient to keep\\nall requests’ KVs, some running requests are preempted (evicted),\\nreleasing their KVs from the memory, and restarted (refilled) later\\n[146]. Due to the low PCIe bandwidth, the released KVs are often\\nrecomputed when restarted, instead of offloading to other storage\\ndevices and loading back. Multiple requests in either prefill or\\ndecode steps can be batched to amortize the cost of loading model\\nweights from GPU memory.\\nNote that the model weights also occupy the GPU memory. When\\nthe model size exceeds a single GPU capacity, techniques like model\\nand pipeline parallelism [105, 120, 257] distribute model weights\\nacross multiple GPUs. This partitioning introduces data transfer\\noverhead between GPUs.\\nTarget. The audience will understand the KV caching and different\\nphases of LLM inference requests, and how they compete for the\\nsame GPU resource.\\n2.2.2\\nLLM Inference Systems: LLMs Behave as DBMSs. LLM infer-\\nence systems (e.g., vLLM [146]) behave similarly to (in-memory)\\nDBMS. KVs and model weights correspond to the data, which are\\nmaintained in GPU memory. Operators include matrix multipli-\\ncations, activations, attentions, and data transfers. Compared to\\nOLAP in databases, the operations are simpler yet much more time-\\nsensitive, where the requests should be served in real-time.\\nSignificant efforts have been made to increase the efficiency of\\nLLM inference [342], largely based on operating and database sys-\\ntems. Orca [318] forms a new batch of requests after each iteration\\n(of prefills and decodes) whenever resources are available. Thus, a\\nnew request does not have to wait for all current running requests\\nto finish, just like the pipelining in OS. vLLM [146] adopts paging\\nand virtual memory to manage KVs, reducing memory fragmenta-\\ntion and enlarging the batch size. Since prefills are typically more\\ncostly than decodes, making stalls for decodes when batched to-\\ngether, Sarathi [4, 5] chunks prefills to reduce pipeline bubbles.\\nSome other work [217, 263, 339] rather disaggregates the prefills\\nand decodes into different GPUs, so the workload for each GPU\\nis homogeneous. vTensor [298] decouples the KV cache manage-\\nment and attention computation of vLLM for better flexibility and\\nperformance. NanoFlow [343] splits each batch into nano-batches\\nfor finer-grained pipelining, increasing the overlap of computation,\\nmemory operation, and data transfer between GPUs. It also hides\\nCPU scheduling latency by asynchronous scheduling. InfiniGen\\n[149] offloads the KVs to CPU memory to extend the KV cache and\\nreloads the KVs from CPU layer-wise, but fetches a subset of KVs\\nfor efficiency, similarly to sparse attentions (Section 2.2.3). InstIn-\\nfer [213] offloads KVs and attention computations to flash drives,\\njust like the storage-disaggregation and computation pushdown in\\ndatabases [310]. NEO [127] selectively offloads attention computa-\\ntions and KVs from GPU to CPU, in order to maximize both GPU\\nand CPU utilization.\\nTarget. The audience will understand why LLMs behave similarly\\nto DBMSs and how database techniques can improve LLM infer-\\nence efficiency. In subsequent sections, the audience will learn\\nabout efforts and challenges in further improving efficiency in four\\ndimensions: operation data, hardware, and workload.\\n2.2.3\\nOperation: Attention. While matrix multiplications take the\\nmajor portion in LLM latency in general [5], attentions can domi-\\nnate the runtime for large inputs due to their quadratic complexity.\\nFlashAttention [59, 60, 249] has become a de facto standard as an\\nefficient attention implementation, utilizing recent GPU technolo-\\ngies to boost the inference speed. The ideas include kernel operator\\nfusion and GPU cache-aware KV transfer. As in approximate query\\nprocessing (AQP) in databases, sparse attentions [15, 178, 186] do\\nnot compute the full attention scores for all preceding tokens but\\na subset as an approximation. Some attentions rather optimize\\nfor long contexts [2, 62]. FlexAttention [107] offers flexible and\\nperformant implementation of such attentions.\\n2.2.4\\nData: KV and Model Weights. Reading KVs from GPU mem-\\nory in decode-attentions is similar to sequential table scan. As KVs\\nare maintained per each attention layer, reading KVs for a layer\\ncan overlap with other layers’ operators [149, 263]. While offload-\\ning KVs and attention computation have been popular recently\\n[127, 149, 213], we need to be careful as it is challenging to predict\\nthe output lengths of LLM requests and thus their utilization pat-\\nterns, and a KV for a single token may consume a few MBs. KVs\\nof long documents can be precomputed, compressed, and fetched\\nfor later retrievals [184]. To reduce memory latency, one can opt\\nfor KV sharing across different attention heads [7, 26, 50], KV com-\\npression [63, 129, 176, 184, 236], model quantization [94, 97, 106,\\n147, 164, 200, 251, 295, 306], or different model architectures than\\nTransformer, such as State Space Models (SSMs) [61, 100] that do\\nnot rely on attentions, thereby not generating KVs. While hybrid\\narchitectures [10, 67, 99, 108, 171, 223, 237] can balance between\\nthe efficiency of SSMs and memorization capacity of Transformers,\\nSSMs remain niche in the market [17]. A recent work even shows\\nthat tokenizers can be removed from the models [209].\\n2.2.5\\nHardware: Theory and Practice. We briefly explain 1) the\\nroofline model [322] and 2) some efforts to overcome the hard-\\nware limits [161, 238, 253, 261, 317, 333] or leverage advanced hard-\\nware for LLM inference [170, 304]. The roofline model is based\\non the computation speed (e.g., GPU FLOPS) and memory band-\\nwidth, which acts as a theoretical hardware bound and determines\\nwhether an operator is compute-intensive or memory-intensive\\nacross different inputs.\\n2.2.6\\nWorkload: Scheduling, Prefix Sharing, Speculation. To handle\\nmultiple LLM requests, LLM inference systems implement request\\nscheduler to send LLM requests to appropriate machines or GPUs\\nto maximize throughput or minimize latency. Assuming indepen-\\ndent requests, early schedulers either prioritize prefills [146] or\\ndecodes [4], which tend to optimize latency or throughput, respec-\\ntively. More complex schedulers consider fairness [252, 291] while\\ncompromising performance, or predict the output lengths of re-\\nquests (not known in advance) and schedule shorter requests first\\n[85, 231, 337].\\nIf different LLM requests can share a prefix in their inputs, the\\nKVs of the prefix can be stored just once and reused for multiple\\nrequests [334, 335]. This forms a trie structure with shared prefixes.\\nHowever, a single-token difference in inputs may invalidate the\\nsharing of KVs of all subsequent tokens. To increase the sharing\\nopportunity, [311] uses the KVs of multiple token sequences to\\napproximate the KVs of the concatenated sequence. The mechanism\\nis similar to the speculation in OLAP [260] and healing protocol in\\ntransactions [293] in databases.\\nThis speculation and healing patterns also appear in speculative\\ndecoding [153] and model cascades [39, 177, 316, 325, 336], acceler-\\nating the generation of tokens by leveraging smaller, faster models\\nthen validate the tokens using larger models, since the validation\\ncosts less than the generation.\\n2.3\\nLLMs Meet Databases\\nThe last part of the tutorial discusses the intersection between\\nLLMs and databases, opportunities and challenges in how we can\\nexploit LLMs for databases, how the development of databases can\\nhelp LLMs, and how we can exploit new types of workloads and\\nintegrations of LLMs and databases. We explain from more well-\\nknown to more untapped, deeper integrations in Sections 2.3.1-2.3.5\\nand provide more proactive visions in Section 2.3.6.\\n2.3.1\\nLLMs for DBs: DBAs and DBMS Internal. We briefly explain\\nhow LLMs are utilized for well-known tasks of DBAs and DBMS\\ninternals such as database tuning [271, 340], text2sql [151, 224] and\\nquery optimization [8, 168]. As we mentioned in Section 1, we will\\nnot cover every detail, as many of these efforts are covered in a\\nprevious tutorial [194] and its additional list of papers [55].\\n2.3.2\\nDBs for LLMs: Adaptive Cost-based Scheduling. Unlike the\\nsophisticated query optimizers in DBMSs, LLMs lack cost models\\nand cost-based scheduling of LLM requests. [3] measures the batch\\ntimes across various inputs (number of tokens to process and KV\\nsize to read). [322] computes batch times based on the roofline\\nmodels. These can be used to model batch times and formulate the\\nproblem of finding optimal schedules as a constrained satisfaction\\nproblem (CSP) [139]. While schedulers try to avoid preemptions to\\nmaximize performance, [139] shows that harnessing preemptions\\ncan rather reduce overall latency compared to zero-preemptions.\\nAs the exact hardware utilization of each request is not known in\\nadvance, the scheduling should be adaptive based on the observa-\\ntions, and it has not been explored much to schedule dependent\\nrequests connected via semantic variables or shared prefixes [139].\\n2.3.3\\nDBs with LLMs: Mixed Relational-LLM Workload. Not only\\nsolving existing tasks with LLMs, LLMs offer new functionalities\\nwhen integrated into DBMSs. Semantic operators [216] extend rela-\\ntional operators to batch-process the tabular data with LLMs (e.g.,\\nfilters and joins using LLMs), which can be regarded as an AQP.\\nWorkloads with LLMs provide a justification to use LLMs inside\\nDBMSs (heavy LLM calls in plan optimization can be negligible com-\\npared to query execution with LLMs). However, different pipelines\\n(with semantic operators) lead to different accuracy and efficiency,\\nthus defining the equivalence between two pipelines is non-trivial.\\nFurthermore, more complex pipelines or LLM calls do not always\\nguarantee higher accuracy [37, 74], and searching similar entities\\nwith LLMs can be replaced with efficient vector-similarity searches\\n[177, 242] as a type of model cascade.\\n2.3.4\\nDBs with LLMs: Multi-objective Query Optimization and Bench-\\nmarks. The challenge is therefore how we can automatically find\\ngood pipelines for mixed relational-LLM workloads under the multi-\\nobjective of accuracy and efficiency [22, 273, 321] as in compound\\nAI systems [95, 244, 267]. This calls for development of accurate\\ncost models and accuracy-prediction models for LLMs and mixed\\nrelational-LLM workloads, in order to enable the holistic optimiza-\\ntion of query plans consisting of both relational and non-relational\\noperators. The cost model itself can be learned via LLMs (or any ML\\nmodels), possibly using RLHF or feedback from query execution\\nwithout human intervention, where such an automatic training\\ndata generation is one of the advantages of solving database tasks\\ncompared to conventional ML tasks (e.g., natural language process-\\ning with human-labeled translation data) [140]. Another model for\\npredicting the output accuracy or detecting hallucination may be\\nchosen from the scaling laws (using the general fact that larger\\nmodels are more trustworthy) or separately trained.\\nTo balance efficiency and accuracy, during the physical query\\noptimization we should select proper models (ones used for ex-\\necution) to avoid calling heavy LLMs unnecessarily. Depending\\non the complexity of the task, simple ML models with a small set\\nof supervised data [123], or larger deep generative models such\\nas in tabular foundation models tailored to domain-specific data\\n[141, 160, 308], or LLMs with world knowledge and reasoning capac-\\nity [296] can fit the task with different accuracy-efficiency trade-offs.\\nSmall language models (SLMs) [1, 16, 67, 189, 196, 202, 226] are also\\na good choice. Automatically finding the best prompt configuration\\n[136, 280] tailored to the mixed workloads and more (e.g., previ-\\nously mentioned fine-tuning or multi-hop/multi-path reasoning\\nwith adaptivity during inference) might be desired.\\nFurthermore, unlike the TPC benchmarks for databases, another\\nproblem is that there is no comprehensive benchmark for relational-\\nLLM workloads yet. [22, 182] provide exploratory benchmarks\\nwithout focusing on semantic joins.\\n2.3.5\\nDBs with LLMs: Integrated System. Other than the cost mod-\\nels, we also need DBMSs with native LLM support to increase\\nthe optimization opportunities, alike systems for relational-vector\\nworkloads [242, 327]. Current prototypes for relational-LLM work-\\nloads [177, 182, 216] separate table processing (e.g., pandas [192])\\nand LLM inference engine (e.g., vLLM [146]).\\nTo maximize efficiency and scalability, we should focus on hard-\\nware utilization, data movement [122], caching hot data, locating\\ncomputations close to data (e.g., computation pushdown in storage-\\naggregation setting) [84, 191], asynchronous API calls [95], balanc-\\ning loads, and multi-tenancy just like in DBMSs [248, 310]. One\\nalso has to decide whether to maintain a separate vector database\\nfor faster online vector retrievals, or use just-in-time vectorization\\nfor reducing storage overhead. This also applies to precomputing\\nKVs of data tokens [184] for faster LLM inferences or not, but with\\na higher caution as KVs are typically larger than vectors.\\n2.3.6\\nConvergence and Future. We envision LLMs and databases\\nto converge (e.g., neuro-symbolic systems [48, 79, 268, 321]), more\\nthan just applying the techniques from one domain to another. A\\nnew LLM inference system optimized for DBMSs might be devel-\\noped from an open-source cloud DBMS, utilizing recent implemen-\\ntations and optimizations for processing relational operators, such\\nas storage-disaggregation and computation pushdown for scalable\\ndata and model management [310], GPU-based OLAP processing\\n[86, 219, 220] for the full use of GPUs for both relational opera-\\ntors and LLMs, hybrid operators with heterogeneous data transfer\\npaths [53, 310], adaptive query execution [307] and more. A unified\\nquery optimizer and data model for both relational data, KVs, and\\nmodel weights, could offer opportunities for better data manage-\\nment and hardware utilization. Finally, if we look into the near\\nfuture, we could also harness the emerging CXL technology for\\nmemory disaggregation [6, 148, 180] to manage model weights\\nand KVs, and increased interest in pruning unnecessary data in\\nOLAP [21, 24, 174, 206] could lead to higher trustworthiness (due\\nto reduced noise) and efficiency (due to less data to process) in the\\nrelational-LLM workloads, with connections to online aggregation\\n[254] and incremental view maintenance [28].\\nTarget. The audience will understand the different depths of LLM-\\ndatabase integrations and be able to find interesting research topics\\nfrom each of the integration, which are closely related to the current\\nand near-future trends of databases.\\n3\\nRelated Tutorial\\nXupeng et al. [194] presented a tutorial at SIGMOD 2024 about\\nthe role of data management in the development (training, fine-\\ntuning) and deployment (inference) of LLMs. It focused on how the\\nknowledge is encoded into model parameters and extracted during\\nthe inference, and explained the concept of KV caching but not LLM\\ninference systems. Trummer [272] presented at VLDB 2023 about\\nTransformer architecture, pre-training/fine-tuning/prompting in\\nLLMs, and LLM applications in data management. As pointed out\\nby [194], most of other tutorials presented at SIGMOD and VLDB\\nabout AI and databases focused on traditional machine learning\\nand deep learning tasks not tailored to LLMs [31, 70, 156, 165, 201,\\n207, 290], or specific LLM-related applications such as tabular data\\nunderstanding [12] and queries with natural languages [132, 133].\\nDong et al. [68] presented at SIGKDD 2023 about the role of LLMs\\nin building intelligent AR/VR assistants.\\nIn this tutorial, we will focus on more recent, general approaches\\nto enhance the trustworthiness and efficiency of LLMs, which have\\nnot been addressed in previous tutorials. For trustworthiness, we\\nwill start with enhancing LLMs alone, LLMs with tools, and agentic\\nLLMs and collaboration. For efficiency, we will explain how LLM\\ninference systems resemble DBMSs. Then we will discuss how we\\ncan integrate LLMs and databases in depth. We expect that these\\nare what researchers, who aim to use LLMs in their applications or\\noptimize LLMs using database techniques, need to know about. In-\\nstead of a common analogy that LLMs are knowledge bases as they\\ngenerate plausible facts, we will use analogies that LLMs behave as\\nDBMSs and improve as how humans solve challenging problems.\\nReferences\\n[1] Marah I Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed\\nAwadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harki-\\nrat S. Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Mar-\\ntin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul\\nChopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan\\nIter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng\\nHao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauff-\\nmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko,\\nJames R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi\\nLin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick,\\nBarun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin,\\nMarko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi,\\nAmin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi\\nSharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang,\\nPhilipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang,\\nZiyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang,\\nLi Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. 2024.\\nPhi-3 Technical Report: A Highly Capable Language Model Locally on Your\\nPhone. CoRR abs/2404.14219 (2024). https://doi.org/10.48550/ARXIV.2404.14219\\narXiv:2404.14219\\n[2] Amey Agrawal, Junda Chen, Íñigo Goiri, Ramachandran Ramjee, Chaojie\\nZhang, Alexey Tumanov, and Esha Choukse. 2024. Mnemosyne: Paralleliza-\\ntion Strategies for Efficiently Serving Multi-Million Context Length LLM\\nInference Requests Without Approximations.\\nCoRR abs/2409.17264 (2024).\\nhttps://doi.org/10.48550/ARXIV.2409.17264 arXiv:2409.17264\\n[3] Amey Agrawal, Nitin Kedia, Jayashree Mohan, Ashish Panwar, Nipun\\nKwatra, Bhargav S. Gulavani, Ramachandran Ramjee, and Alexey Tu-\\nmanov. 2024.\\nVIDUR: A Large-Scale Simulation Framework for LLM\\nInference. In Proceedings of the Seventh Annual Conference on Machine\\nLearning and Systems, MLSys 2024, Santa Clara, CA, USA, May 13-16,\\n2024, Phillip B. Gibbons, Gennady Pekhimenko, and Christopher De Sa\\n(Eds.). mlsys.org. https://proceedings.mlsys.org/paper_files/paper/2024/hash/\\nb74a8de47d2b3c928360e0a011f48351-Abstract-Conference.html\\n[4] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwa-\\ntra, Bhargav S. Gulavani, Alexey Tumanov, and Ramachandran Ramjee. 2024.\\nTaming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve.\\nIn 18th USENIX Symposium on Operating Systems Design and Implementation,\\nOSDI 2024, Santa Clara, CA, USA, July 10-12, 2024, Ada Gavrilovska and Dou-\\nglas B. Terry (Eds.). USENIX Association, 117–134. https://www.usenix.org/\\nconference/osdi24/presentation/agrawal\\n[5] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S.\\nGulavani, and Ramachandran Ramjee. 2023. SARATHI: Efficient LLM Inference\\nby Piggybacking Decodes with Chunked Prefills. CoRR abs/2308.16369 (2023).\\nhttps://doi.org/10.48550/ARXIV.2308.16369 arXiv:2308.16369\\n[6] Minseon Ahn, Thomas Willhalm, Norman May, Donghun Lee, Suprasad Mutalik\\nDesai, Daniel Booss, Jungmin Kim, Navneet Singh, Daniel Ritter, and Oliver\\nRebholz. 2024. An Examination of CXL Memory Use Cases for In-Memory\\nDatabase Management Systems using SAP HANA. Proc. VLDB Endow. 17, 12\\n(2024), 3827–3840. https://www.vldb.org/pvldb/vol17/p3827-ahn.pdf\\n[7] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico\\nLebrón, and Sumit Sanghai. 2023. GQA: Training Generalized Multi-Query\\nTransformer Models from Multi-Head Checkpoints. In Proceedings of the 2023\\nConference on Empirical Methods in Natural Language Processing, EMNLP 2023,\\nSingapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali\\n(Eds.). Association for Computational Linguistics, 4895–4901. https://doi.org/\\n10.18653/V1/2023.EMNLP-MAIN.298\\n[8] Peter Akioyamen, Zixuan Yi, and Ryan Marcus. 2024. The Unreasonable Ef-\\nfectiveness of LLMs for Query Optimization. arXiv preprint arXiv:2411.02862\\n(2024).\\n[9] Uday Allu, Biddwan Ahmed, and Vishesh Tripathi. 2024. Beyond Extraction:\\nContextualising Tabular Data for Efficient Summarisation by Language Mod-\\nels. CoRR abs/2401.02333 (2024). https://doi.org/10.48550/ARXIV.2401.02333\\narXiv:2401.02333\\n[10] Quentin Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge. 2024.\\nBlackMamba: Mixture of Experts for State-Space Models. CoRR abs/2402.01771\\n(2024). https://doi.org/10.48550/ARXIV.2402.01771 arXiv:2402.01771\\n[11] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi.\\n2024. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-\\nReflection. In The Twelfth International Conference on Learning Representations,\\nICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.\\nnet/forum?id=hSyW5go0v8\\n[12] Gilbert Badaro and Paolo Papotti. 2022. Transformers for Tabular Data Repre-\\nsentation: A Tutorial on Models and Applications. Proc. VLDB Endow. 15, 12\\n(2022), 3746–3749. https://doi.org/10.14778/3554821.3554890\\n[13] Sourav Banerjee, Ayushi Agarwal, and Saloni Singla. 2024. LLMs Will Always\\nHallucinate, and We Need to Live With This. CoRR abs/2409.05746 (2024).\\nhttps://doi.org/10.48550/ARXIV.2409.05746 arXiv:2409.05746\\n[14] Matthew Barnett. 2024.\\nAn Empirical Study of Scaling Laws for Trans-\\nfer. CoRR abs/2408.16947 (2024). https://doi.org/10.48550/ARXIV.2408.16947\\narXiv:2408.16947\\n[15] Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The\\nLong-Document Transformer. CoRR abs/2004.05150 (2020). arXiv:2004.05150\\nhttps://arxiv.org/abs/2004.05150\\n[16] Loubna Ben Allal, Anton Lozhkov, and Elie Bakouch. 2024. SmolLM - blazingly\\nfast and remarkably powerful. https://huggingface.co/blog/smollm. Accessed:\\nDecember 15, 2024.\\n[17] Nathan Benaich and Ian Hogarth. 2024. State of AI Report 2024. https://www.\\nstateof.ai.\\n[18] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski,\\nLukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Pi-\\notr Nyczyk, and Torsten Hoefler. 2024. Graph of Thoughts: Solving Elaborate\\nProblems with Large Language Models. In Thirty-Eighth AAAI Conference on\\nArtificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applica-\\ntions of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational\\nAdvances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver,\\nCanada, Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan (Eds.).\\nAAAI Press, 17682–17690. https://doi.org/10.1609/AAAI.V38I16.29720\\n[19] Maciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger, Nils Blach,\\nPiotr Nyczyk, Marcin Copik, Grzegorz Kwasniewski, Jürgen Müller, Lukas Gian-\\ninazzi, Ales Kubicek, Hubert Niewiadomski, Onur Mutlu, and Torsten Hoefler.\\n2024. Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of\\nThoughts. CoRR abs/2401.14295 (2024). https://doi.org/10.48550/ARXIV.2401.\\n14295 arXiv:2401.14295\\n[20] Luca Beurer-Kellner, Marc Fischer, and Martin T. Vechev. 2024. Guiding LLMs\\nThe Right Way: Fast, Non-Invasive Constrained Generation. In Forty-first In-\\nternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July\\n21-27, 2024. OpenReview.net. https://openreview.net/forum?id=pXaEYzrFae\\n[21] Altan Birler, Alfons Kemper, and Thomas Neumann. 2024. Robust Join Process-\\ning with Diamond Hardened Joins. Proc. VLDB Endow. 17, 11 (2024), 3215–3228.\\nhttps://www.vldb.org/pvldb/vol17/p3215-birler.pdf\\n[22] Asim Biswal, Liana Patel, Siddarth Jha, Amog Kamsetty, Shu Liu, Joseph E.\\nGonzalez, Carlos Guestrin, and Matei Zaharia. 2024. Text2SQL is Not Enough:\\nUnifying AI and Databases with TAG. CoRR abs/2408.14717 (2024).\\nhttps:\\n//doi.org/10.48550/ARXIV.2408.14717 arXiv:2408.14717\\n[23] Quinten Bolding, Baohao Liao, Brandon James Denis, Jun Luo, and Christof\\nMonz. 2023. Ask Language Model to Clean Your Noisy Translation Data. In\\nFindings of the Association for Computational Linguistics: EMNLP 2023, Singapore,\\nDecember 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Associ-\\nation for Computational Linguistics, 3215–3236. https://doi.org/10.18653/V1/\\n2023.FINDINGS-EMNLP.212\\n[24] Angela Bonifati, Stefania Dumbrava, George Fletcher, Jan Hidders, Matthias\\nHofer, Wim Martens, Filip Murlak, Joshua Shinavier, Slawek Staworko, and\\nDominik Tomaszuk. 2023. Threshold Queries. SIGMOD Rec. 52, 1 (2023), 64–73.\\nhttps://doi.org/10.1145/3604437.3604452\\n[25] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Ruther-\\nford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan\\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman\\nRing, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cas-\\nsirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osin-\\ndero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improv-\\ning Language Models by Retrieving from Trillions of Tokens. In International\\nConference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland,\\nUSA (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaudhuri,\\nStefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (Eds.).\\nPMLR, 2206–2240. https://proceedings.mlr.press/v162/borgeaud22a.html\\n[26] William Brandon, Mayank Mishra, Aniruddha Nrusimha, Rameswar Panda, and\\nJonathan Ragan-Kelley. 2024. Reducing Transformer Key-Value Cache Size with\\nCross-Layer Attention. CoRR abs/2405.12981 (2024). https://doi.org/10.48550/\\nARXIV.2405.12981 arXiv:2405.12981\\n[27] Bradley C. A. Brown, Jordan Juravsky, Ryan Saul Ehrlich, Ronald Clark, Quoc V.\\nLe, Christopher Ré, and Azalia Mirhoseini. 2024. Large Language Monkeys:\\nScaling Inference Compute with Repeated Sampling. CoRR abs/2407.21787\\n(2024). https://doi.org/10.48550/ARXIV.2407.21787 arXiv:2407.21787\\n[28] Mihai Budiu, Tej Chajed, Frank McSherry, Leonid Ryzhyk, and Val Tannen. 2023.\\nDBSP: Automatic Incremental View Maintenance for Rich Query Languages.\\nProc. VLDB Endow. 16, 7 (2023), 1601–1614. https://doi.org/10.14778/3587136.\\n3587137\\n[29] Shiyi Cao, Shu Liu, Tyler Griggs, Peter Schafhalter, Xiaoxuan Liu, Ying Sheng,\\nJoseph E Gonzalez, Matei Zaharia, and Ion Stoica. 2024. MoE-Lightning: High-\\nThroughput MoE Inference on Memory-constrained GPUs. arXiv preprint\\narXiv:2411.11217 (2024).\\n[30] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy\\nScheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro\\nFreire, Tony Tong Wang, Samuel Marks, Charbel-Raphaël Ségerie, Micah Carroll,\\nAndi Peng, Phillip J. K. Christoffersen, Mehul Damani, Stewart Slocum, Usman\\nAnwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii\\nKrasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Biyik, Anca D.\\nDragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. 2023. Open\\nProblems and Fundamental Limitations of Reinforcement Learning from Human\\nFeedback. Trans. Mach. Learn. Res. 2023 (2023). https://openreview.net/forum?\\nid=bx24KpJ4Eb\\n[31] Chengliang Chai, Nan Tang, Ju Fan, and Yuyu Luo. 2023. Demystifying Artificial\\nIntelligence for Data Preparation. In Companion of the 2023 International Confer-\\nence on Management of Data, SIGMOD/PODS 2023, Seattle, WA, USA, June 18-23,\\n2023, Sudipto Das, Ippokratis Pandis, K. Selçuk Candan, and Sihem Amer-Yahia\\n(Eds.). ACM, 13–20. https://doi.org/10.1145/3555041.3589406\\n[32] Andong Chen, Lianzhang Lou, Kehai Chen, Xuefeng Bai, Yang Xiang, Muyun\\nYang, Tiejun Zhao, and Min Zhang. 2024. DUAL-REFLECT: Enhancing Large\\nLanguage Models for Reflective Translation through Dual Learning Feedback\\nMechanisms. In Proceedings of the 62nd Annual Meeting of the Association for\\nComputational Linguistics, ACL 2024 - Short Papers, Bangkok, Thailand, August 11-\\n16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for\\nComputational Linguistics, 693–704. https://aclanthology.org/2024.acl-short.64\\n[33] Angelica Chen, Jason Phang, Alicia Parrish, Vishakh Padmakumar, Chen Zhao,\\nSamuel R. Bowman, and Kyunghyun Cho. 2024. Two Failures of Self-Consistency\\nin the Multi-Step Reasoning of LLMs. Trans. Mach. Learn. Res. 2024 (2024).\\nhttps://openreview.net/forum?id=5nBqY1y96B\\n[34] Banghao Chen, Zhaofeng Zhang, Nicolas Langrené, and Shengxin Zhu. 2023.\\nUnleashing the potential of prompt engineering in Large Language Models: a\\ncomprehensive review. CoRR abs/2310.14735 (2023). https://doi.org/10.48550/\\nARXIV.2310.14735 arXiv:2310.14735\\n[35] Huaben Chen, Wenkang Ji, Lufeng Xu, and Shiyu Zhao. 2023. Multi-Agent\\nConsensus Seeking via Large Language Models. CoRR abs/2310.20151 (2023).\\nhttps://doi.org/10.48550/ARXIV.2310.20151 arXiv:2310.20151\\n[36] Jiaao Chen, Derek Tam, Colin Raffel, Mohit Bansal, and Diyi Yang. 2023. An\\nEmpirical Survey of Data Augmentation for Limited Data Learning in NLP.\\nTrans. Assoc. Comput. Linguistics 11 (2023), 191–211. https://doi.org/10.1162/\\nTACL_A_00542\\n[37] Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei\\nZaharia, and James Zou. 2024. Are more llm calls all you need? towards scaling\\nlaws of compound inference systems. arXiv preprint arXiv:2403.02419 (2024).\\n[38] Liyi Chen, Panrong Tong, Zhongming Jin, Ying Sun, Jieping Ye, and Hui Xiong.\\n2024. Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language\\nModel on Knowledge Graphs. CoRR abs/2410.23875 (2024). https://doi.org/10.\\n48550/ARXIV.2410.23875 arXiv:2410.23875\\n[39] Lingjiao Chen, Matei Zaharia, and James Zou. 2023.\\nFrugalGPT: How to\\nUse Large Language Models While Reducing Cost and Improving Perfor-\\nmance. CoRR abs/2305.05176 (2023). https://doi.org/10.48550/ARXIV.2305.05176\\narXiv:2305.05176\\n[40] Wenhu Chen. 2023. Large Language Models are few(1)-shot Table Reasoners. In\\nFindings of the Association for Computational Linguistics: EACL 2023, Dubrovnik,\\nCroatia, May 2-6, 2023, Andreas Vlachos and Isabelle Augenstein (Eds.). Associ-\\nation for Computational Linguistics, 1090–1100. https://doi.org/10.18653/V1/\\n2023.FINDINGS-EACL.83\\n[41] Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William W. Cohen. 2022.\\nMuRAG: Multimodal Retrieval-Augmented Generator for Open Question An-\\nswering over Images and Text. In Proceedings of the 2022 Conference on Em-\\npirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi,\\nUnited Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zornitsa Kozareva,\\nand Yue Zhang (Eds.). Association for Computational Linguistics, 5558–5570.\\nhttps://doi.org/10.18653/V1/2022.EMNLP-MAIN.375\\n[42] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W. Cohen. 2023.\\nRe-Imagen: Retrieval-Augmented Text-to-Image Generator. In The Eleventh\\nInternational Conference on Learning Representations, ICLR 2023, Kigali, Rwanda,\\nMay 1-5, 2023. OpenReview.net. https://openreview.net/forum?id=XSEBx0iSjFQ\\n[43] Weizhe Chen, Sven Koenig, and Bistra Dilkina. 2024.\\nRePrompt: Plan-\\nning by Automatic Prompt Engineering for Large Language Models Agents.\\nCoRR abs/2406.11132 (2024).\\nhttps://doi.org/10.48550/ARXIV.2406.11132\\narXiv:2406.11132\\n[44] Wenqing Chen, Weicheng Wang, Zhixuan Chu, Kui Ren, Zibin Zheng, and\\nZhichao Lu. 2024. Self-Para-Consistency: Improving Reasoning Tasks at Low\\nCost for Large Language Models. In Findings of the Association for Computational\\nLinguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16,\\n2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for\\nComputational Linguistics, 14162–14167.\\nhttps://doi.org/10.18653/V1/2024.\\nFINDINGS-ACL.842\\n[45] Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and\\nWilliam Yang Wang. 2020. HybridQA: A Dataset of Multi-Hop Question An-\\nswering over Tabular and Textual Data. In Findings of the Association for Compu-\\ntational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020 (Findings of\\nACL, Vol. EMNLP 2020), Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association\\nfor Computational Linguistics, 1026–1036. https://doi.org/10.18653/V1/2020.\\nFINDINGS-EMNLP.91\\n[46] Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin,\\nSushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. 2023. Univer-\\nsal Self-Consistency for Large Language Model Generation. CoRR abs/2311.17311\\n(2023). https://doi.org/10.48550/ARXIV.2311.17311 arXiv:2311.17311\\n[47] Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang, Qiang Li,\\nYue Shen, Lei Liang, Jinjie Gu, and Huajun Chen. 2024. Unified Hallucination\\nDetection for Multimodal Large Language Models. In Proceedings of the 62nd\\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long\\nPapers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, Lun-Wei Ku, Andre\\nMartins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,\\n3235–3252. https://doi.org/10.18653/V1/2024.ACL-LONG.178\\n[48] Zui Chen, Zihui Gu, Lei Cao, Ju Fan, Samuel Madden, and Nan Tang. 2023.\\nSymphony: Towards Natural Language Query Answering over Multi-modal\\nData Lakes. In 13th Conference on Innovative Data Systems Research, CIDR 2023,\\nAmsterdam, The Netherlands, January 8-11, 2023. www.cidrdb.org. https://www.\\ncidrdb.org/cidr2023/papers/p51-chen.pdf\\n[49] Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong,\\nWenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, and Xiuqiang He.\\n2024. Exploring Large Language Model based Intelligent Agents: Definitions,\\nMethods, and Prospects. CoRR abs/2401.03428 (2024). https://doi.org/10.48550/\\nARXIV.2401.03428 arXiv:2401.03428\\n[50] Sai Sena Chinnakonduru and Astarag Mohapatra. 2024. Weighted Grouped\\nQuery Attention in Transformers. CoRR abs/2407.10855 (2024). https://doi.org/\\n10.48550/ARXIV.2407.10855 arXiv:2407.10855\\n[51] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021. Unifying Vision-and-\\nLanguage Tasks via Text Generation. In Proceedings of the 38th International\\nConference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event\\n(Proceedings of Machine Learning Research, Vol. 139), Marina Meila and Tong\\nZhang (Eds.). PMLR, 1931–1942. http://proceedings.mlr.press/v139/cho21a.html\\n[52] Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg,\\nand Dario Amodei. 2017. Deep Reinforcement Learning from Human Pref-\\nerences. In Advances in Neural Information Processing Systems 30: Annual\\nConference on Neural Information Processing Systems 2017, December 4-9,\\n2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Ben-\\ngio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman\\nGarnett (Eds.). 4299–4307.\\nhttps://proceedings.neurips.cc/paper/2017/hash/\\nd5e2c0adad503c91f91df240d0cd4e49-Abstract.html\\n[53] Periklis Chrysogelos, Manos Karpathiotakis, Raja Appuswamy, and Anastasia\\nAilamaki. 2019. HetExchange: Encapsulating heterogeneous CPU-GPU par-\\nallelism in JIT compiled engines. Proc. VLDB Endow. 12, 5 (2019), 544–556.\\nhttps://doi.org/10.14778/3303753.3303760\\n[54] Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, and\\nJames R. Glass. 2024. Lookback Lens: Detecting and Mitigating Contextual Hallu-\\ncinations in Large Language Models Using Only Attention Maps. In Proceedings\\nof the 2024 Conference on Empirical Methods in Natural Language Processing,\\nEMNLP 2024, Miami, FL, USA, November 12-16, 2024, Yaser Al-Onaizan, Mohit\\nBansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics,\\n1419–1436. https://aclanthology.org/2024.emnlp-main.84\\n[55] code4DB. 2024. LLM4DB: A Curated List of Resources on Large Language\\nModels for Databases. https://github.com/code4DB/LLM4DB. Accessed: 2024-\\n11-30.\\n[56] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare\\nCampagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024. The\\nPower of Noise: Redefining Retrieval for RAG Systems. In Proceedings of the 47th\\nInternational ACM SIGIR Conference on Research and Development in Information\\nRetrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024, Grace Hui Yang,\\nHongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, and Yi Zhang (Eds.).\\nACM, 719–729. https://doi.org/10.1145/3626772.3657834\\n[57] Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou\\nWang, and Yaodong Yang. 2024. Safe RLHF: Safe Reinforcement Learning\\nfrom Human Feedback. In The Twelfth International Conference on Learning\\nRepresentations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net.\\nhttps://openreview.net/forum?id=TyFrPOKYXw\\n[58] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and\\nRuslan Salakhutdinov. 2019. Transformer-XL: Attentive Language Models\\nbeyond a Fixed-Length Context. In Proceedings of the 57th Conference of the\\nAssociation for Computational Linguistics, ACL 2019, Florence, Italy, July 28-\\nAugust 2, 2019, Volume 1: Long Papers, Anna Korhonen, David R. Traum, and\\nLluís Màrquez (Eds.). Association for Computational Linguistics, 2978–2988.\\nhttps://doi.org/10.18653/V1/P19-1285\\n[59] Tri Dao. 2024.\\nFlashAttention-2: Faster Attention with Better Parallelism\\nand Work Partitioning. In The Twelfth International Conference on Learning\\nRepresentations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net.\\nhttps://openreview.net/forum?id=mZn2Xyh9Ec\\n[60] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher\\nRé. 2022.\\nFlashAttention: Fast and Memory-Efficient Exact Attention\\nwith IO-Awareness. In Advances in Neural Information Processing Sys-\\ntems 35: Annual Conference on Neural Information Processing Systems\\n2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9,\\n2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho,\\nand A. Oh (Eds.).\\nhttp://papers.nips.cc/paper_files/paper/2022/hash/\\n67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html\\n[61] Tri Dao and Albert Gu. 2024. Transformers are SSMs: Generalized Models\\nand Efficient Algorithms Through Structured State Space Duality. In Forty-first\\nInternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July\\n21-27, 2024. OpenReview.net. https://openreview.net/forum?id=ztn8FCR1td\\n[62] Tri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. 2023.\\nFlash-\\nDecoding for long-context inference. https://pytorch.org/blog/flash-decoding/.\\nAccessed: December 15, 2024.\\n[63] Alessio Devoto, Yu Zhao, Simone Scardapane, and Pasquale Minervini. 2024. A\\nSimple and Effective L_2 Norm-Based Strategy for KV Cache Compression. In\\nProceedings of the 2024 Conference on Empirical Methods in Natural Language Pro-\\ncessing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, Yaser Al-Onaizan,\\nMohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Lin-\\nguistics, 18476–18499. https://aclanthology.org/2024.emnlp-main.1027\\n[64] Bosheng Ding, Chengwei Qin, Ruochen Zhao, Tianze Luo, Xinze Li, Guizhen\\nChen, Wenhan Xia, Junjie Hu, Anh Tuan Luu, and Shafiq Joty. 2024. Data\\nAugmentation using LLMs: Data Perspectives, Learning Paradigms and Chal-\\nlenges. In Findings of the Association for Computational Linguistics, ACL 2024,\\nBangkok, Thailand and virtual meeting, August 11-16, 2024, Lun-Wei Ku, Andre\\nMartins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,\\n1679–1705. https://doi.org/10.18653/V1/2024.FINDINGS-ACL.97\\n[65] Peng Ding, Jingyu Wu, Jun Kuang, Dan Ma, Xuezhi Cao, Xunliang Cai, Shi Chen,\\nJiajun Chen, and Shujian Huang. 2024. Hallu-PI: Evaluating Hallucination in\\nMulti-modal Large Language Models within Perturbed Inputs. In Proceedings of\\nthe 32nd ACM International Conference on Multimedia, MM 2024, Melbourne, VIC,\\nAustralia, 28 October 2024 - 1 November 2024, Jianfei Cai, Mohan S. Kankanhalli,\\nBalakrishnan Prabhakaran, Susanne Boll, Ramanathan Subramanian, Liang\\nZheng, Vivek K. Singh, Pablo César, Lexing Xie, and Dong Xu (Eds.). ACM,\\n10707–10715. https://doi.org/10.1145/3664647.3681251\\n[66] Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco,\\nDirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Documenting\\nLarge Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus.\\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language\\nProcessing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11\\nNovember, 2021, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and\\nScott Wen-tau Yih (Eds.). Association for Computational Linguistics, 1286–1305.\\nhttps://doi.org/10.18653/V1/2021.EMNLP-MAIN.98\\n[67] Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil\\nMahabaleshwarkar, Shih-Yang Liu, Matthijs Van Keirsbilck, Min-Hung Chen,\\nYoshi Suhara, et al. 2024. Hymba: A Hybrid-head Architecture for Small Lan-\\nguage Models. arXiv preprint arXiv:2411.13676 (2024).\\n[68] Xin Luna Dong, Seungwhan Moon, Yifan Ethan Xu, Kshitiz Malik, and Zhou\\nYu. 2023. Towards Next-Generation Intelligent Assistants Leveraging LLM\\nTechniques. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge\\nDiscovery and Data Mining, KDD 2023, Long Beach, CA, USA, August 6-10, 2023,\\nAmbuj K. Singh, Yizhou Sun, Leman Akoglu, Dimitrios Gunopulos, Xifeng\\nYan, Ravi Kumar, Fatma Ozcan, and Jieping Ye (Eds.). ACM, 5792–5793. https:\\n//doi.org/10.1145/3580305.3599572\\n[69] Yixin Dong, Charlie F Ruan, Yaxing Cai, Ruihang Lai, Ziyi Xu, Yilong Zhao, and\\nTianqi Chen. 2024. XGrammar: Flexible and Efficient Structured Generation\\nEngine for Large Language Models. arXiv preprint arXiv:2411.15100 (2024).\\n[70] Alexey Drutsa, Valentina Fedorova, Dmitry Ustalov, Olga Megorskaya,\\nEvfrosiniya Zerminova, and Daria Baidakova. 2020. Crowdsourcing Practice\\nfor Efficient Data Labeling: Aggregation, Incremental Relabeling, and Pric-\\ning. In Proceedings of the 2020 International Conference on Management of\\nData, SIGMOD Conference 2020, online conference [Portland, OR, USA], June\\n14-19, 2020, David Maier, Rachel Pottinger, AnHai Doan, Wang-Chiew Tan,\\nAbdussalam Alawini, and Hung Q. Ngo (Eds.). ACM, 2623–2627.\\nhttps:\\n//doi.org/10.1145/3318464.3383127\\n[71] Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin,\\nYuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat,\\nBarret Zoph, Liam Fedus, Maarten P. Bosma, Zongwei Zhou, Tao Wang,\\nYu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen S.\\nMeier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V. Le, Yonghui\\nWu, Zhifeng Chen, and Claire Cui. 2022.\\nGLaM: Efficient Scaling of Lan-\\nguage Models with Mixture-of-Experts. In International Conference on Machine\\nLearning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA (Proceedings\\nof Machine Learning Research, Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka,\\nLe Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (Eds.). PMLR, 5547–5569.\\nhttps://proceedings.mlr.press/v162/du22c.html\\n[72] Yaxin Du, Rui Ye, Yuchi Fengting, Wanru Zhao, Jingjing Qu, Yanfeng Wang, and\\nSiheng Chen. 2024. Data Quality Control in Federated Instruction-tuning of\\nLarge Language Models. CoRR abs/2410.11540 (2024). https://doi.org/10.48550/\\nARXIV.2410.11540 arXiv:2410.11540\\n[73] Fabio Duarte. 2024.\\nNumber of ChatGPT Users (Dec 2024).\\nhttps://\\nexplodingtopics.com/blog/chatgpt-users. Accessed: 2024-12-15.\\n[74] Matous Eibich, Shivay Nagpal, and Alexander Fred-Ojala. 2024. ARAGOG:\\nAdvanced RAG Output Grading. CoRR abs/2404.01037 (2024). https://doi.org/\\n10.48550/ARXIV.2404.01037 arXiv:2404.01037\\n[75] Oluwole Fagbohun, Rachel M. Harrison, and Anton Dereventsov. 2024. An\\nEmpirical Categorization of Prompting Techniques for Large Language Models:\\nA Practitioner’s Guide. CoRR abs/2402.14837 (2024). https://doi.org/10.48550/\\nARXIV.2402.14837 arXiv:2402.14837\\n[76] Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin,\\nTat-Seng Chua, and Qing Li. 2024.\\nA Survey on RAG Meeting LLMs: To-\\nwards Retrieval-Augmented Large Language Models. In Proceedings of the 30th\\nACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2024,\\nBarcelona, Spain, August 25-29, 2024, Ricardo Baeza-Yates and Francesco Bonchi\\n(Eds.). ACM, 6491–6501. https://doi.org/10.1145/3637528.3671470\\n[77] Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. 2024. Detecting\\nhallucinations in large language models using semantic entropy. Nat. 630, 8017\\n(2024), 625–630. https://doi.org/10.1038/S41586-024-07421-0\\n[78] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch Transformers:\\nScaling to Trillion Parameter Models with Simple and Efficient Sparsity. J. Mach.\\nLearn. Res. 23 (2022), 120:1–120:39. https://jmlr.org/papers/v23/21-0998.html\\n[79] Jonathan Feldstein, Paulius Dilkas, Vaishak Belle, and Efthymia Tsamoura. 2024.\\nMapping the Neuro-Symbolic AI Landscape by Architectures: A Handbook on\\nAugmenting Deep Learning Through Symbolic Reasoning. CoRR abs/2410.22077\\n(2024). https://doi.org/10.48550/ARXIV.2410.22077 arXiv:2410.22077\\n[80] Matthew Finlayson, John Hewitt, Alexander Koller, Swabha Swayamdipta, and\\nAshish Sabharwal. 2024. Closing the Curious Case of Neural Text Degeneration.\\nIn The Twelfth International Conference on Learning Representations, ICLR 2024,\\nVienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?\\nid=dONpC9GL1o\\n[81] Evan Frick, Tianle Li, Connor Chen, Wei-Lin Chiang, Anastasios N. Angelopou-\\nlos, Jiantao Jiao, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. 2024.\\nHow to Evaluate Reward Models for RLHF.\\nCoRR abs/2410.14872 (2024).\\nhttps://doi.org/10.48550/ARXIV.2410.14872 arXiv:2410.14872\\n[82] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi,\\nRuiqi Zhong, Scott Yih, Luke Zettlemoyer, and Mike Lewis. 2023. InCoder: A\\nGenerative Model for Code Infilling and Synthesis. In The Eleventh International\\nConference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5,\\n2023. OpenReview.net. https://openreview.net/forum?id=hQwb-lbM6EL\\n[83] Yuhan Fu, Ruobing Xie, Xingwu Sun, Zhanhui Kang, and Xirong Li. 2024. Miti-\\ngating Hallucination in Multimodal Large Language Model via Hallucination-\\ntargeted Direct Preference Optimization. arXiv preprint arXiv:2411.10436 (2024).\\n[84] Yao Fu, Leyang Xue, Yeqi Huang, Andrei-Octavian Brabete, Dmitrii Ustiugov,\\nYuvraj Patel, and Luo Mai. 2024. ServerlessLLM: Locality-Enhanced Serverless\\nInference for Large Language Models. CoRR abs/2401.14351 (2024).\\nhttps:\\n//doi.org/10.48550/ARXIV.2401.14351 arXiv:2401.14351\\n[85] Yichao Fu, Siqi Zhu, Runlong Su, Aurick Qiao, Ion Stoica, and Hao Zhang. 2024.\\nEfficient LLM Scheduling by Learning to Rank. CoRR abs/2408.15792 (2024).\\nhttps://doi.org/10.48550/ARXIV.2408.15792 arXiv:2408.15792\\n[86] Henning Funke and Jens Teubner. 2020. Data-parallel query processing on\\nnon-uniform data. Proceedings of the VLDB Endowment 13, 6 (2020), 884–897.\\n[87] Hang Gao and Yongfeng Zhang. 2024. VRSD: Rethinking Similarity and Diversity\\nfor Retrieval in Large Language Models. CoRR abs/2407.04573 (2024). https:\\n//doi.org/10.48550/ARXIV.2407.04573 arXiv:2407.04573\\n[88] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles\\nFoster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser,\\nand Connor Leahy. 2021. The Pile: An 800GB Dataset of Diverse Text for\\nLanguage Modeling. CoRR abs/2101.00027 (2021). arXiv:2101.00027 https:\\n//arxiv.org/abs/2101.00027\\n[89] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2023. Precise Zero-Shot\\nDense Retrieval without Relevance Labels. In Proceedings of the 61st Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\nACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber,\\nand Naoaki Okazaki (Eds.). Association for Computational Linguistics, 1762–\\n1777. https://doi.org/10.18653/V1/2023.ACL-LONG.99\\n[90] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi,\\nYi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023.\\nRetrieval-Augmented Generation for Large Language Models: A Survey.\\nCoRR abs/2312.10997 (2023).\\nhttps://doi.org/10.48550/ARXIV.2312.10997\\narXiv:2312.10997\\n[91] Aryo Pradipta Gema, Chen Jin, Ahmed Abdulaal, Tom Diethe, Philip Teare,\\nBeatrice Alex, Pasquale Minervini, and Amrutha Saseendran. 2024. DeCoRe:\\nDecoding by Contrasting Retrieval Heads to Mitigate Hallucinations. arXiv\\npreprint arXiv:2410.18860 (2024).\\n[92] Saibo Geng, Berkay Döner, Chris Wendler, Martin Josifoski, and Robert West.\\n2024.\\nSketch-Guided Constrained Decoding for Boosting Blackbox Large\\nLanguage Models without Logit Access. In Proceedings of the 62nd Annual\\nMeeting of the Association for Computational Linguistics, ACL 2024 - Short Pa-\\npers, Bangkok, Thailand, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and\\nVivek Srikumar (Eds.). Association for Computational Linguistics, 234–245.\\nhttps://aclanthology.org/2024.acl-short.23\\n[93] Saibo Geng, Martin Josifoski, Maxime Peyrard, and Robert West. 2023. Grammar-\\nConstrained Decoding for Structured NLP Tasks without Finetuning. In Proceed-\\nings of the 2023 Conference on Empirical Methods in Natural Language Processing,\\nEMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and\\nKalika Bali (Eds.). Association for Computational Linguistics, 10932–10952.\\nhttps://doi.org/10.18653/V1/2023.EMNLP-MAIN.674\\n[94] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney,\\nand Kurt Keutzer. 2021. A Survey of Quantization Methods for Efficient Neural\\nNetwork Inference. CoRR abs/2103.13630 (2021). arXiv:2103.13630 https://arxiv.\\norg/abs/2103.13630\\n[95] In Gim, Seung-seob Lee, and Lin Zhong. 2024. Asynchronous LLM Function\\nCalling. arXiv preprint arXiv:2412.07017 (2024).\\n[96] Michael R. Glass, Xueqing Wu, Ankita Rajaram Naik, Gaetano Rossiello,\\nand Alfio Gliozzo. 2023. Retrieval-Based Transformer for Table Augmenta-\\ntion. In Findings of the Association for Computational Linguistics: ACL 2023,\\nToronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and\\nNaoaki Okazaki (Eds.). Association for Computational Linguistics, 5635–5648.\\nhttps://doi.org/10.18653/V1/2023.FINDINGS-ACL.348\\n[97] Ruihao Gong, Yang Yong, Shiqiao Gu, Yushi Huang, Chengtao Lv, Yunchen\\nZhang, Dacheng Tao, and Xianglong Liu. 2024. LLMC: Benchmarking Large\\nLanguage Model Quantization with a Versatile Compression Toolkit. In Proceed-\\nings of the 2024 Conference on Empirical Methods in Natural Language Processing:\\nEMNLP 2024 - Industry Track, Miami, Florida, USA, November 12-16, 2024, Franck\\nDernoncourt, Daniel Preotiuc-Pietro, and Anastasia Shimorina (Eds.). Associ-\\nation for Computational Linguistics, 132–152. https://aclanthology.org/2024.\\nemnlp-industry.12\\n[98] Yury Gorishniy, Ivan Rubachev, Nikolay Kartashev, Daniil Shlenskii, Akim\\nKotelnikov, and Artem Babenko. 2023. TabR: Unlocking the Power of Retrieval-\\nAugmented Tabular Deep Learning. CoRR abs/2307.14338 (2023). https://doi.\\norg/10.48550/ARXIV.2307.14338 arXiv:2307.14338\\n[99] Albert Gu and Tri Dao. 2023. Mamba: Linear-Time Sequence Modeling with\\nSelective State Spaces. CoRR abs/2312.00752 (2023). https://doi.org/10.48550/\\nARXIV.2312.00752 arXiv:2312.00752\\n[100] Albert Gu, Karan Goel, and Christopher Ré. 2022. Efficiently Modeling Long\\nSequences with Structured State Spaces. In The Tenth International Conference\\non Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenRe-\\nview.net. https://openreview.net/forum?id=uYLFoz1vlAC\\n[101] Nuno Miguel Guerreiro, Duarte M. Alves, Jonas Waldendorf, Barry Haddow,\\nAlexandra Birch, Pierre Colombo, and André F. T. Martins. 2023. Hallucinations\\nin Large Multilingual Translation Models. Trans. Assoc. Comput. Linguistics 11\\n(2023), 1500–1517. https://doi.org/10.1162/TACL_A_00615\\n[102] Çaglar Gülçehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova,\\nLotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen\\nWang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, and\\nNando de Freitas. 2023. Reinforced Self-Training (ReST) for Language Model-\\ning. CoRR abs/2308.08998 (2023). https://doi.org/10.48550/ARXIV.2308.08998\\narXiv:2308.08998\\n[103] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del\\nGiorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa,\\nOlli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien\\nBubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023.\\nTextbooks Are All You Need. CoRR abs/2306.11644 (2023). https://doi.org/10.\\n48550/ARXIV.2306.11644 arXiv:2306.11644\\n[104] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V.\\nChawla, Olaf Wiest, and Xiangliang Zhang. 2024. Large Language Model Based\\nMulti-agents: A Survey of Progress and Challenges. In Proceedings of the Thirty-\\nThird International Joint Conference on Artificial Intelligence, IJCAI 2024, Jeju,\\nSouth Korea, August 3-9, 2024. ijcai.org, 8048–8057.\\nhttps://www.ijcai.org/\\nproceedings/2024/890\\n[105] Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil R.\\nDevanur, Gregory R. Ganger, and Phillip B. Gibbons. 2018. PipeDream: Fast\\nand Efficient Pipeline Parallel DNN Training. CoRR abs/1806.03377 (2018).\\narXiv:1806.03377 http://arxiv.org/abs/1806.03377\\n[106] Jahid Hasan. 2024. Optimizing Large Language Models through Quantiza-\\ntion: A Comparative Analysis of PTQ and QAT Techniques. arXiv preprint\\narXiv:2411.06084 (2024).\\n[107] Horace He, Driss Guessous, Yanbo Liang, and Joy Dong. 2024. FlexAttention:\\nThe Flexibility of PyTorch with the Performance of FlashAttention. https:\\n//pytorch.org/blog/flexattention/.\\n[108] Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo,\\nand Yunhe Wang. 2024. DenseMamba: State Space Models with Dense Hidden\\nConnection for Efficient Large Language Models. CoRR abs/2403.00818 (2024).\\nhttps://doi.org/10.48550/ARXIV.2403.00818 arXiv:2403.00818\\n[109] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann\\nLeCun, Xavier Bresson, and Bryan Hooi. 2024.\\nG-Retriever: Retrieval-\\nAugmented Generation for Textual Graph Understanding and Question Answer-\\ning. CoRR abs/2402.07630 (2024). https://doi.org/10.48550/ARXIV.2402.07630\\narXiv:2402.07630\\n[110] Xu Owen He. 2024. Mixture of A Million Experts. CoRR abs/2407.04153 (2024).\\nhttps://doi.org/10.48550/ARXIV.2407.04153 arXiv:2407.04153\\n[111] Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 2021.\\nScaling Laws for Transfer.\\nCoRR abs/2102.01293 (2021).\\narXiv:2102.01293\\nhttps://arxiv.org/abs/2102.01293\\n[112] Michael Hewing and Vincent Leinhos. 2024. The Prompt Canvas: A Literature-\\nBased Practitioner Guide for Creating Effective Prompts in Large Language\\nModels. arXiv preprint arXiv:2412.05127 (2024).\\n[113] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,\\nTrevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes\\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den\\nDriessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan,\\nErich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training\\nCompute-Optimal Large Language Models. CoRR abs/2203.15556 (2022). https:\\n//doi.org/10.48550/ARXIV.2203.15556 arXiv:2203.15556\\n[114] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The\\nCurious Case of Neural Text Degeneration. In 8th International Conference on\\nLearning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\\nOpenReview.net. https://openreview.net/forum?id=rygGQyrFvH\\n[115] Zijin Hong, Zheng Yuan, Hao Chen, Qinggang Zhang, Feiran Huang, and Xiao\\nHuang. 2024. Knowledge-to-SQL: Enhancing SQL Generation with Data Expert\\nLLM. In Findings of the Association for Computational Linguistics, ACL 2024,\\nBangkok, Thailand and virtual meeting, August 11-16, 2024, Lun-Wei Ku, Andre\\nMartins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,\\n10997–11008. https://doi.org/10.18653/V1/2024.FINDINGS-ACL.653\\n[116] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin\\nde Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\\nParameter-Efficient Transfer Learning for NLP. In Proceedings of the 36th In-\\nternational Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long\\nBeach, California, USA (Proceedings of Machine Learning Research, Vol. 97), Ka-\\nmalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, 2790–2799. http:\\n//proceedings.mlr.press/v97/houlsby19a.html\\n[117] Cheng-Yu Hsieh, Yung-Sung Chuang, Chun-Liang Li, Zifeng Wang, Long T.\\nLe, Abhishek Kumar, James R. Glass, Alexander Ratner, Chen-Yu Lee, Ranjay\\nKrishna, and Tomas Pfister. 2024. Found in the middle: Calibrating Positional\\nAttention Bias Improves Long Context Utilization. In Findings of the Association\\nfor Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting,\\nAugust 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.).\\nAssociation for Computational Linguistics, 14982–14995. https://doi.org/10.\\n18653/V1/2024.FINDINGS-ACL.890\\n[118] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao.\\n2023. ChatDB: Augmenting LLMs with Databases as Their Symbolic Mem-\\nory. CoRR abs/2306.03901 (2023). https://doi.org/10.48550/ARXIV.2306.03901\\narXiv:2306.03901\\n[119] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\\nWang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of\\nLarge Language Models. In The Tenth International Conference on Learning\\nRepresentations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\\nhttps://openreview.net/forum?id=nZeVKeeFYf9\\n[120] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen,\\nMia Xu Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and\\nZhifeng Chen. 2019. GPipe: Efficient Training of Giant Neural Networks us-\\ning Pipeline Parallelism. In Advances in Neural Information Processing Systems\\n32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS\\n2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo\\nLarochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Ro-\\nman Garnett (Eds.). 103–112. https://proceedings.neurips.cc/paper/2019/hash/\\n093f65e080a295f8076b1c5722a46aa2-Abstract.html\\n[121] Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei\\nVassilvitskii, and Sanmi Koyejo. 2024. Scaling Laws for Downstream Task\\nPerformance of Large Language Models. CoRR abs/2402.04177 (2024). https:\\n//doi.org/10.48550/ARXIV.2402.04177 arXiv:2402.04177\\n[122] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler.\\n2021. Data Movement Is All You Need: A Case Study on Optimizing Transform-\\ners. In Proceedings of the Fourth Conference on Machine Learning and Systems,\\nMLSys 2021, virtual, April 5-9, 2021, Alex Smola, Alex Dimakis, and Ion Stoica\\n(Eds.). mlsys.org. https://proceedings.mlsys.org/paper_files/paper/2021/hash/\\nbc86e95606a6392f51f95a8de106728d-Abstract.html\\n[123] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong Park.\\n2024. Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language\\nModels through Question Complexity. In Proceedings of the 2024 Conference of\\nthe North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico\\nCity, Mexico, June 16-21, 2024, Kevin Duh, Helena Gómez-Adorno, and Steven\\nBethard (Eds.). Association for Computational Linguistics, 7036–7050. https:\\n//doi.org/10.18653/V1/2024.NAACL-LONG.389\\n[124] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,\\nYejin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of Hallucination in\\nNatural Language Generation. ACM Comput. Surv. 55, 12 (2023), 248:1–248:38.\\nhttps://doi.org/10.1145/3571730\\n[125] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung.\\n2023. Towards Mitigating Hallucination in Large Language Models via Self-\\nReflection. CoRR abs/2310.06271 (2023). https://doi.org/10.48550/ARXIV.2310.\\n06271 arXiv:2310.06271\\n[126] Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, and Ji-Rong Wen.\\n2023. StructGPT: A General Framework for Large Language Model to Rea-\\nson over Structured Data. In Proceedings of the 2023 Conference on Empirical\\nMethods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10,\\n2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Com-\\nputational Linguistics, 9237–9251. https://doi.org/10.18653/V1/2023.EMNLP-\\nMAIN.574\\n[127] Xuanlin Jiang, Yang Zhou, Shiyi Cao, Ion Stoica, and Minlan Yu. 2024. NEO:\\nSaving GPU Memory Crisis with CPU Offloading for Online LLM Inference.\\narXiv preprint arXiv:2411.01142 (2024).\\n[128] Deokhyung Kang, Baikjin Jung, Yunsu Kim, and Gary Geunbae Lee. 2024. De-\\nnoising Table-Text Retrieval for Open-Domain Question Answering. In Pro-\\nceedings of the 2024 Joint International Conference on Computational Linguis-\\ntics, Language Resources and Evaluation, LREC/COLING 2024, 20-25 May, 2024,\\nTorino, Italy, Nicoletta Calzolari, Min-Yen Kan, Véronique Hoste, Alessandro\\nLenci, Sakriani Sakti, and Nianwen Xue (Eds.). ELRA and ICCL, 4634–4640.\\nhttps://aclanthology.org/2024.lrec-main.414\\n[129] Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar\\nKrishna, and Tuo Zhao. 2024. Gear: An efficient kv cache compression recipefor\\nnear-lossless generative inference of llm. arXiv preprint arXiv:2403.05527 (2024).\\n[130] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin\\nChess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.\\n2020. Scaling Laws for Neural Language Models. CoRR abs/2001.08361 (2020).\\narXiv:2001.08361 https://arxiv.org/abs/2001.08361\\n[131] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu,\\nSergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval\\nfor Open-Domain Question Answering. In Proceedings of the 2020 Conference\\non Empirical Methods in Natural Language Processing, EMNLP 2020, Online,\\nNovember 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu\\n(Eds.). Association for Computational Linguistics, 6769–6781. https://doi.org/\\n10.18653/V1/2020.EMNLP-MAIN.550\\n[132] George Katsogiannis-Meimarakis and Georgia Koutrika. 2021. A Deep Dive into\\nDeep Learning Approaches for Text-to-SQL Systems. In SIGMOD ’21: Interna-\\ntional Conference on Management of Data, Virtual Event, China, June 20-25, 2021,\\nGuoliang Li, Zhanhuai Li, Stratos Idreos, and Divesh Srivastava (Eds.). ACM,\\n2846–2851. https://doi.org/10.1145/3448016.3457543\\n[133] George Katsogiannis-Meimarakis, Mike Xydas, and Georgia Koutrika. 2023.\\nNatural Language Interfaces for Databases with Deep Learning. Proc. VLDB\\nEndow. 16, 12 (2023), 3878–3881. https://doi.org/10.14778/3611540.3611575\\n[134] Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke Hüllermeier. 2023. A\\nSurvey of Reinforcement Learning from Human Feedback. CoRR abs/2312.14925\\n(2023). https://doi.org/10.48550/ARXIV.2312.14925 arXiv:2312.14925\\n[135] Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael\\nBendersky. 2024. Bridging the Preference Gap between Retrievers and LLMs.\\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16,\\n2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for\\nComputational Linguistics, 10438–10451.\\nhttps://doi.org/10.18653/V1/2024.\\nACL-LONG.562\\n[136] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav\\nSanthanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi,\\nHanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. 2024.\\nDSPy: Compiling Declarative Language Model Calls into State-of-the-Art\\nPipelines. In The Twelfth International Conference on Learning Representa-\\ntions, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net.\\nhttps:\\n//openreview.net/forum?id=sY5N0zY5Od\\n[137] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage\\nSearch via Contextualized Late Interaction over BERT. In Proceedings of the 43rd\\nInternational ACM SIGIR conference on research and development in Information\\nRetrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020, Jimmy X. Huang,\\nYi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun\\nLiu (Eds.). ACM, 39–48. https://doi.org/10.1145/3397271.3401075\\n[138] Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon\\nKim, and Chanjun Park. 2024.\\nsDPO: Don’t Use Your Data All at Once.\\nCoRR abs/2403.19270 (2024).\\nhttps://doi.org/10.48550/ARXIV.2403.19270\\narXiv:2403.19270\\n[139] Kyoungmin Kim, Kijae Hong, Caglar Gulcehre, and Anastasia Ailamaki. 2024.\\nThe Effect of Scheduling and Preemption on the Efficiency of LLM Inference\\nServing. arXiv preprint arXiv:2411.07447 (2024).\\n[140] Kyoungmin Kim, Jisung Jung, In Seo, Wook-Shin Han, Kangwoo Choi, and\\nJaehyok Chong. 2022. Learned Cardinality Estimation: An In-depth Study. In\\nSIGMOD ’22: International Conference on Management of Data, Philadelphia, PA,\\nUSA, June 12 - 17, 2022, Zachary G. Ives, Angela Bonifati, and Amr El Abbadi\\n(Eds.). ACM, 1214–1227. https://doi.org/10.1145/3514221.3526154\\n[141] Kyoungmin Kim, Sangoh Lee, Injung Kim, and Wook-Shin Han. 2024. ASM:\\nHarmonizing Autoregressive Model, Sampling, and Multi-dimensional Statistics\\nMerging for Cardinality Estimation. Proc. ACM Manag. Data 2, 1 (2024), 45:1–\\n45:27. https://doi.org/10.1145/3639300\\n[142] Seungone Kim, Juyoung Suk, Xiang Yue, Vijay Viswanathan, Seongyun Lee,\\nYizhong Wang, Kiril Gashteovski, Carolin Lawrence, Sean Welleck, and Graham\\nNeubig. 2024. Evaluating Language Models as Synthetic Data Generators. arXiv\\npreprint arXiv:2412.03679 (2024).\\n[143] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Yacine\\nJernite, Margaret Mitchell, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf,\\nDzmitry Bahdanau, Leandro von Werra, and Harm de Vries. 2023. The Stack: 3\\nTB of permissively licensed source code. Trans. Mach. Learn. Res. 2023 (2023).\\nhttps://openreview.net/forum?id=pxpbTdUEpD\\n[144] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke\\nIwasawa. 2022. Large Language Models are Zero-Shot Reasoners. In Advances in\\nNeural Information Processing Systems 35: Annual Conference on Neural Informa-\\ntion Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 -\\nDecember 9, 2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave,\\nK. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper_files/paper/2022/hash/\\n8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html\\n[145] Juri Kong, Hong Liang, Yuan Zhang, Hongxiang Li, Pengcheng Shen, and Fang\\nLu. 2024. Dynamic semantic memory retention in large language models: An\\nexploration of spontaneous retrieval mechanisms. (2024).\\n[146] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,\\nCody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient\\nMemory Management for Large Language Model Serving with PagedAttention.\\nIn Proceedings of the 29th Symposium on Operating Systems Principles, SOSP\\n2023, Koblenz, Germany, October 23-26, 2023, Jason Flinn, Margo I. Seltzer, Pe-\\nter Druschel, Antoine Kaufmann, and Jonathan Mace (Eds.). ACM, 611–626.\\nhttps://doi.org/10.1145/3600006.3613165\\n[147] Jiedong Lang, Zhehao Guo, and Shuyu Huang. 2024.\\nA Comprehensive\\nStudy on Quantization Techniques for Large Language Models. arXiv preprint\\narXiv:2411.02530 (2024).\\n[148] Sangjin Lee, Alberto Lerner, Philippe Bonnet, and Philippe Cudré-Mauroux.\\n2024. Database Kernels: Seamless Integration of Database Systems and Fast\\nStorage via CXL. In 14th Conference on Innovative Data Systems Research, CIDR\\n2024, Chaminade, HI, USA, January 14-17, 2024. www.cidrdb.org. https://www.\\ncidrdb.org/cidr2024/papers/p43-lee.pdf\\n[149] Wonbeom Lee, Jungi Lee, Junghwan Seo, and Jaewoong Sim. 2024. InfiniGen:\\nEfficient Generative Inference of Large Language Models with Dynamic KV\\nCache Management. In 18th USENIX Symposium on Operating Systems Design\\nand Implementation, OSDI 2024, Santa Clara, CA, USA, July 10-12, 2024, Ada\\nGavrilovska and Douglas B. Terry (Eds.). USENIX Association, 155–172. https:\\n//www.usenix.org/conference/osdi24/presentation/lee\\n[150] Younghun Lee, Sungchul Kim, Tong Yu, Ryan A. Rossi, and Xiang Chen. 2024.\\nLearning to Reduce: Optimal Representations of Structured Data in Prompting\\nLarge Language Models. CoRR abs/2402.14195 (2024). https://doi.org/10.48550/\\nARXIV.2402.14195 arXiv:2402.14195\\n[151] Fangyu Lei, Jixuan Chen, Yuxiao Ye, Ruisheng Cao, Dongchan Shin, Hongjin Su,\\nZhaoqing Suo, Hongcheng Gao, Wenjing Hu, Pengcheng Yin, et al. 2024. Spider\\n2.0: Evaluating language models on real-world enterprise text-to-sql workflows.\\narXiv preprint arXiv:2411.07763 (2024).\\n[152] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale\\nfor Parameter-Efficient Prompt Tuning. In Proceedings of the 2021 Conference\\non Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual\\nEvent / Punta Cana, Dominican Republic, 7-11 November, 2021, Marie-Francine\\nMoens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association\\nfor Computational Linguistics, 3045–3059. https://doi.org/10.18653/V1/2021.\\nEMNLP-MAIN.243\\n[153] Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast Inference from\\nTransformers via Speculative Decoding. In International Conference on Machine\\nLearning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA (Proceedings of Ma-\\nchine Learning Research, Vol. 202), Andreas Krause, Emma Brunskill, Kyunghyun\\nCho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR,\\n19274–19286. https://proceedings.mlr.press/v202/leviathan23a.html\\n[154] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\\nKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim\\nRocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented\\nGeneration for Knowledge-Intensive NLP Tasks. In Advances in Neural In-\\nformation Processing Systems 33: Annual Conference on Neural Information\\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo\\nLarochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and\\nHsuan-Tien Lin (Eds.).\\nhttps://proceedings.neurips.cc/paper/2020/hash/\\n6b493230205f780e1bc26945df7481e5-Abstract.html\\n[155] Dengchun Li, Yingzi Ma, Naizheng Wang, Zhiyuan Cheng, Lei Duan, Jie Zuo,\\nCal Yang, and Mingjie Tang. 2024. MixLoRA: Enhancing Large Language Models\\nFine-Tuning with LoRA based Mixture of Experts. CoRR abs/2404.15159 (2024).\\nhttps://doi.org/10.48550/ARXIV.2404.15159 arXiv:2404.15159\\n[156] Guoliang Li, Xuanhe Zhou, and Lei Cao. 2021. AI Meets Database: AI4DB and\\nDB4AI. In SIGMOD ’21: International Conference on Management of Data, Virtual\\nEvent, China, June 20-25, 2021, Guoliang Li, Zhanhuai Li, Stratos Idreos, and\\nDivesh Srivastava (Eds.). ACM, 2859–2866.\\nhttps://doi.org/10.1145/3448016.\\n3457542\\n[157] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak\\nGadre, Hritik Bansal, Etash Kumar Guha, Sedrick Keh, Kushal Arora, Saurabh\\nGarg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee\\nChen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bit-\\nton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh\\nGardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah M. Pratt, Sunny\\nSanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan,\\nJieyu Zhang, Khyathi Raghavi Chandu, Thao Nguyen, Igor Vasiljevic, Sham M.\\nKakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke\\nZettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander To-\\nshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia\\nJitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Lud-\\nwig Schmidt, and Vaishaal Shankar. 2024. DataComp-LM: In search of the next\\ngeneration of training sets for language models. CoRR abs/2406.11794 (2024).\\nhttps://doi.org/10.48550/ARXIV.2406.11794 arXiv:2406.11794\\n[158] Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin\\nWang, Bowen Qin, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma,\\nGuoliang Li, Kevin Chen-Chuan Chang, Fei Huang, Reynold Cheng, and\\nYongbin Li. 2023.\\nCan LLM Already Serve as A Database Interface? A\\nBIg Bench for Large-Scale Database Grounded Text-to-SQLs. In Advances\\nin Neural Information Processing Systems 36: Annual Conference on Neural\\nInformation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA,\\nDecember 10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate\\nSaenko, Moritz Hardt, and Sergey Levine (Eds.).\\nhttp://papers.nips.cc/\\npaper_files/paper/2023/hash/83fc8fab1710363050bbd1d4b8cc0021-Abstract-\\nDatasets_and_Benchmarks.html\\n[159] Lingyu Li, Yixu Wang, Haiquan Zhao, Shuqi Kong, Yan Teng, Chunbo Li, and\\nYingchun Wang. 2024. Reflection-Bench: probing AI intelligence with reflection.\\narXiv preprint arXiv:2410.16270 (2024).\\n[160] Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\\nDanielle Rifinski Fainman, Dongmei Zhang, and Surajit Chaudhuri. 2024. Table-\\nGPT: Table Fine-tuned GPT for Diverse Table Tasks. Proc. ACM Manag. Data 2,\\n3 (2024), 176. https://doi.org/10.1145/3654979\\n[161] Xiangyu Li, Yuanchun Li, Yuanzhe Li, Ting Cao, and Yunxin Liu. 2024. FlexNN:\\nEfficient and Adaptive DNN Inference on Memory-Constrained Edge Devices.\\nIn Proceedings of the 30th Annual International Conference on Mobile Computing\\nand Networking, ACM MobiCom 2024, Washington D.C., DC, USA, November\\n18-22, 2024, Weisong Shi, Deepak Ganesan, and Nicholas D. Lane (Eds.). ACM,\\n709–723. https://doi.org/10.1145/3636534.3649391\\n[162] Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous\\nPrompts for Generation. In Proceedings of the 59th Annual Meeting of the As-\\nsociation for Computational Linguistics and the 11th International Joint Con-\\nference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long\\nPapers), Virtual Event, August 1-6, 2021, Chengqing Zong, Fei Xia, Wenjie Li, and\\nRoberto Navigli (Eds.). Association for Computational Linguistics, 4582–4597.\\nhttps://doi.org/10.18653/V1/2021.ACL-LONG.353\\n[163] Yichuan Li, Kaize Ding, Jianling Wang, and Kyumin Lee. 2024. Empowering\\nLarge Language Models for Textual Data Augmentation. In Findings of the\\nAssociation for Computational Linguistics, ACL 2024, Bangkok, Thailand and\\nvirtual meeting, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek\\nSrikumar (Eds.). Association for Computational Linguistics, 12734–12751. https:\\n//doi.org/10.18653/V1/2024.FINDINGS-ACL.756\\n[164] Yuhang Li, Mingzhu Shen, Jian Ma, Yan Ren, Mingxin Zhao, Qi Zhang, Ruihao\\nGong, Fengwei Yu, and Junjie Yan. 2021. MQBench: Towards Reproducible\\nand Deployable Model Quantization Benchmark. In Proceedings of the Neural\\nInformation Processing Systems Track on Datasets and Benchmarks 1, NeurIPS\\nDatasets and Benchmarks 2021, December 2021, virtual, Joaquin Vanschoren\\nand Sai-Kit Yeung (Eds.). https://datasets-benchmarks-proceedings.neurips.cc/\\npaper/2021/hash/c20ad4d76fe97759aa27a0c99bff6710-Abstract-round1.html\\n[165] Yuliang Li, Xiaolan Wang, Zhengjie Miao, and Wang-Chiew Tan. 2021. Data\\nAugmentation for ML-driven Data Preparation and Integration. Proc. VLDB\\nEndow. 14, 12 (2021), 3182–3185. https://doi.org/10.14778/3476311.3476403\\n[166] Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky.\\n2024. Retrieval Augmented Generation or Long-Context LLMs? A Compre-\\nhensive Study and Hybrid Approach. In Proceedings of the 2024 Conference\\non Empirical Methods in Natural Language Processing: EMNLP 2024 - Industry\\nTrack, Miami, Florida, USA, November 12-16, 2024, Franck Dernoncourt, Daniel\\nPreotiuc-Pietro, and Anastasia Shimorina (Eds.). Association for Computational\\nLinguistics, 881–893. https://aclanthology.org/2024.emnlp-industry.66\\n[167] Zihao Li, Zhuoran Yang, and Mengdi Wang. 2023.\\nReinforcement Learn-\\ning with Human Feedback: Learning Dynamic Choices via Pessimism.\\nCoRR abs/2305.18438 (2023).\\nhttps://doi.org/10.48550/ARXIV.2305.18438\\narXiv:2305.18438\\n[168] Zhaodonghui Li, Haitao Yuan, Huiming Wang, Gao Cong, and Lidong Bing.\\n2024. LLM-R2: A Large Language Model Enhanced Rule-based Rewrite System\\nfor Boosting Query Efficiency. CoRR abs/2404.12872 (2024). https://doi.org/10.\\n48550/ARXIV.2404.12872 arXiv:2404.12872\\n[169] Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Sihang\\nZhou, Xinwang Liu, Fuchun Sun, and Kunlun He. 2024. A Survey of Knowledge\\nGraph Reasoning on Graph Types: Static, Dynamic, and Multi-Modal. IEEE\\nTrans. Pattern Anal. Mach. Intell. 46, 12 (2024), 9456–9478. https://doi.org/10.\\n1109/TPAMI.2024.3417451\\n[170] Sean Lie. 2023. Cerebras Architecture Deep Dive: First Look Inside the Hard-\\nware/Software Co-Design for Deep Learning. IEEE Micro 43, 3 (2023), 18–30.\\nhttps://doi.org/10.1109/MM.2023.3256384\\n[171] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedi-\\ngos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri\\nAbend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael\\nGokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor\\nZusman, and Yoav Shoham. 2024. Jamba: A Hybrid Transformer-Mamba Lan-\\nguage Model. CoRR abs/2403.19887 (2024). https://doi.org/10.48550/ARXIV.\\n2403.19887 arXiv:2403.19887\\n[172] Jonathan Light, Min Cai, Weiqin Chen, Guanzhi Wang, Xiusi Chen, Wei Cheng,\\nYisong Yue, and Ziniu Hu. 2024. Strategist: Learning Strategic Skills by LLMs\\nvia Bi-Level Tree Search. CoRR abs/2408.10635 (2024). https://doi.org/10.48550/\\nARXIV.2408.10635 arXiv:2408.10635\\n[173] Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen\\nChen, and Lili Qiu. 2024. Parrot: Efficient Serving of LLM-based Applications\\nwith Semantic Variable. In 18th USENIX Symposium on Operating Systems Design\\nand Implementation, OSDI 2024, Santa Clara, CA, USA, July 10-12, 2024, Ada\\nGavrilovska and Douglas B. Terry (Eds.). USENIX Association, 929–945. https:\\n//www.usenix.org/conference/osdi24/presentation/lin-chaofan\\n[174] Yiming Lin and Sharad Mehrotra. 2024. PLAQUE: Automated Predicate Learning\\nat Query Time. Proc. ACM Manag. Data 2, 1 (2024), 46:1–46:25. https://doi.org/\\n10.1145/3639301\\n[175] Zicheng Lin, Tian Liang, Jiahao Xu, Xing Wang, Ruilin Luo, Chufan Shi, Siheng\\nLi, Yujiu Yang, and Zhaopeng Tu. 2024. Critical Tokens Matter: Token-Level\\nContrastive Estimation Enhence LLM’s Reasoning Capability. arXiv preprint\\narXiv:2411.19943 (2024).\\n[176] Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza Haffari, and Bohan\\nZhuang. 2024. MiniCache: KV Cache Compression in Depth Dimension for\\nLarge Language Models. CoRR abs/2405.14366 (2024). https://doi.org/10.48550/\\nARXIV.2405.14366 arXiv:2405.14366\\n[177] Chunwei Liu, Matthew Russo, Michael J. Cafarella, Lei Cao, Peter Baile Chen, Zui\\nChen, Michael J. Franklin, Tim Kraska, Samuel Madden, and Gerardo Vitagliano.\\n2024. A Declarative System for Optimizing AI Workloads. CoRR abs/2405.14696\\n(2024). https://doi.org/10.48550/ARXIV.2405.14696 arXiv:2405.14696\\n[178] Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang,\\nQi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang,\\nYuqing Yang, and Lili Qiu. 2024. RetrievalAttention: Accelerating Long-Context\\nLLM Inference via Vector Retrieval. CoRR abs/2409.10516 (2024). https://doi.\\norg/10.48550/ARXIV.2409.10516 arXiv:2409.10516\\n[179] Hao Liu and Pieter Abbeel. 2023. Emergent Agentic Transformer from Chain of\\nHindsight Experience. In International Conference on Machine Learning, ICML\\n2023, 23-29 July 2023, Honolulu, Hawaii, USA (Proceedings of Machine Learning\\nResearch, Vol. 202), Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara\\nEngelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 21362–21374.\\nhttps://proceedings.mlr.press/v202/liu23a.html\\n[180] Jinshu Liu, Hamid Hadian, Hanchen Xu, Daniel S. Berger, and Huaicheng Li.\\n2024. Dissecting CXL Memory Performance at Scale: Analysis, Modeling, and\\nOptimization. CoRR abs/2409.14317 (2024). https://doi.org/10.48550/ARXIV.\\n2409.14317 arXiv:2409.14317\\n[181] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,\\nFabio Petroni, and Percy Liang. 2024.\\nLost in the Middle: How Language\\nModels Use Long Contexts. Trans. Assoc. Comput. Linguistics 12 (2024), 157–173.\\nhttps://doi.org/10.1162/TACL_A_00638\\n[182] Shu Liu, Asim Biswal, Audrey Cheng, Xiangxi Mo, Shiyi Cao, Joseph E. Gonzalez,\\nIon Stoica, and Matei Zaharia. 2024. Optimizing LLM Queries in Relational\\nWorkloads. CoRR abs/2403.05821 (2024). https://doi.org/10.48550/ARXIV.2403.\\n05821 arXiv:2403.05821\\n[183] Weijie Liu, Zecheng Tang, Juntao Li, Kehai Chen, and Min Zhang.\\n2024.\\nMemLong: Memory-Augmented Retrieval for Long Text Modeling.\\nCoRR abs/2408.16967 (2024).\\nhttps://doi.org/10.48550/ARXIV.2408.16967\\narXiv:2408.16967\\n[184] Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng\\nZhang, Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, Michael\\nMaire, Henry Hoffmann, Ari Holtzman, and Junchen Jiang. 2024. CacheGen:\\nKV Cache Compression and Streaming for Fast Large Language Model Serving.\\nIn Proceedings of the ACM SIGCOMM 2024 Conference, ACM SIGCOMM 2024,\\nSydney, NSW, Australia, August 4-8, 2024. ACM, 38–56. https://doi.org/10.1145/\\n3651890.3672274\\n[185] Yanming Liu, Xinyue Peng, Xuhong Zhang, Weihao Liu, Jianwei Yin, Jiannan\\nCao, and Tianyu Du. 2024. RA-ISF: Learning to Answer and Understand from\\nRetrieval Augmentation via Iterative Self-Feedback. In Findings of the Association\\nfor Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting,\\nAugust 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.).\\nAssociation for Computational Linguistics, 4730–4749. https://doi.org/10.18653/\\nV1/2024.FINDINGS-ACL.281\\n[186] Chao Lou, Zixia Jia, Zilong Zheng, and Kewei Tu. 2024. Sparser is Faster\\nand Less is More: Efficient Sparse Attention for Long-Range Transform-\\ners. CoRR abs/2406.16747 (2024). https://doi.org/10.48550/ARXIV.2406.16747\\narXiv:2406.16747\\n[187] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David\\nHa. 2024. The AI Scientist: Towards Fully Automated Open-Ended Scientific\\nDiscovery. CoRR abs/2408.06292 (2024). https://doi.org/10.48550/ARXIV.2408.\\n06292 arXiv:2408.06292\\n[188] Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Felix\\nBai, Shuang Ma, Shen Ma, Mengyu Li, Guoli Yin, Zirui Wang, and Ruoming\\nPang. 2024. ToolSandbox: A Stateful, Conversational, Interactive Evaluation\\nBenchmark for LLM Tool Use Capabilities. CoRR abs/2408.04682 (2024). https:\\n//doi.org/10.48550/ARXIV.2408.04682 arXiv:2408.04682\\n[189] Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang,\\nNicholas D. Lane, and Mengwei Xu. 2024. Small Language Models: Survey,\\nMeasurements, and Insights. CoRR abs/2409.15790 (2024). https://doi.org/10.\\n48550/ARXIV.2409.15790 arXiv:2409.15790\\n[190] Kaixin Ma, Hao Cheng, Xiaodong Liu, Eric Nyberg, and Jianfeng Gao. 2022.\\nOpen-domain Question Answering via Chain of Reasoning over Heterogeneous\\nKnowledge. In Findings of the Association for Computational Linguistics: EMNLP\\n2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zor-\\nnitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics,\\n5360–5374. https://doi.org/10.18653/V1/2022.FINDINGS-EMNLP.392\\n[191] Ziming Mao, Tian Xia, Zhanghao Wu, Wei-Lin Chiang, Tyler Griggs, Romil\\nBhardwaj, Zongheng Yang, Scott Shenker, and Ion Stoica. 2024.\\nSky-\\nServe: Serving AI Models across Regions and Clouds with Spot Instances.\\nCoRR abs/2411.01438 (2024).\\nhttps://doi.org/10.48550/ARXIV.2411.01438\\narXiv:2411.01438\\n[192] Wes McKinney. 2010. Data Structures for Statistical Computing in Python. In\\nProceedings of the 9th Python in Science Conference, Stéfan van der Walt and\\nJarrod Millman (Eds.). 56–61. https://doi.org/10.25080/Majora-92bf1922-00a\\n[193] Kevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, and David\\nBau. 2023. Mass-Editing Memory in a Transformer. In The Eleventh International\\nConference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5,\\n2023. OpenReview.net. https://openreview.net/forum?id=MkbcAHIYgyS\\n[194] Xupeng Miao, Zhihao Jia, and Bin Cui. 2024. Demystifying Data Management\\nfor Large Language Models. In Companion of the 2024 International Conference\\non Management of Data, SIGMOD/PODS 2024, Santiago AA, Chile, June 9-15, 2024,\\nPablo Barceló, Nayat Sánchez-Pi, Alexandra Meliou, and S. Sudarshan (Eds.).\\nACM, 547–555. https://doi.org/10.1145/3626246.3654683\\n[195] Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham\\nNeubig, Yulia Tsvetkov, and Hannaneh Hajishirzi. 2024. Fine-grained Hallucina-\\ntion Detection and Editing for Language Models. CoRR abs/2401.06855 (2024).\\nhttps://doi.org/10.48550/ARXIV.2401.06855 arXiv:2401.06855\\n[196] M. Mehdi Mojarradi, Lingyi Yang, Robert McCraith, and Adam Mahdi.\\n2024. Improving In-Context Learning with Small Language Model Ensem-\\nbles. CoRR abs/2410.21868 (2024). https://doi.org/10.48550/ARXIV.2410.21868\\narXiv:2410.21868\\n[197] Laurent Mombaerts, Terry Ding, Adi Banerjee, Florian Felice, Jonathan Taws,\\nand Tarik Borogovac. 2024. Meta Knowledge for Retrieval Augmented Large\\nLanguage Models. CoRR abs/2408.09017 (2024). https://doi.org/10.48550/ARXIV.\\n2408.09017 arXiv:2408.09017\\n[198] Niklas Muennighoff, Qian Liu, Armel Randy Zebaze, Qinkai Zheng, Binyuan\\nHui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and\\nShayne Longpre. 2024. OctoPack: Instruction Tuning Code Large Language\\nModels. In The Twelfth International Conference on Learning Representations,\\nICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.\\nnet/forum?id=mw1PWNSWZP\\n[199] Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. 2024. Generative Representational Instruc-\\ntion Tuning. CoRR abs/2402.09906 (2024). https://doi.org/10.48550/ARXIV.2402.\\n09906 arXiv:2402.09906\\n[200] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart\\nvan Baalen, and Tijmen Blankevoort. 2021. A White Paper on Neural Network\\nQuantization. CoRR abs/2106.08295 (2021). arXiv:2106.08295 https://arxiv.org/\\nabs/2106.08295\\n[201] Fatemeh Nargesian, Abolfazl Asudeh, and H. V. Jagadish. 2022. Responsible\\nData Integration: Next-generation Challenges. In SIGMOD ’22: International\\nConference on Management of Data, Philadelphia, PA, USA, June 12 - 17, 2022,\\nZachary G. Ives, Angela Bonifati, and Amr El Abbadi (Eds.). ACM, 2458–2464.\\nhttps://doi.org/10.1145/3514221.3522567\\n[202] Chien Van Nguyen, Xuan Shen, Ryan Aponte, Yu Xia, Samyadeep Basu, Zheng-\\nmian Hu, Jian Chen, Mihir Parmar, Sasidhar Kunapuli, Joe Barrow, Junda Wu,\\nAshish Singh, Yu Wang, Jiuxiang Gu, Franck Dernoncourt, Nesreen K. Ahmed,\\nNedim Lipka, Ruiyi Zhang, Xiang Chen, Tong Yu, Sungchul Kim, Hanieh Deil-\\namsalehy, Namyong Park, Mike Rimer, Zhehao Zhang, Huanrui Yang, Ryan A.\\nRossi, and Thien Huu Nguyen. 2024.\\nA Survey of Small Language Mod-\\nels. CoRR abs/2410.20011 (2024). https://doi.org/10.48550/ARXIV.2410.20011\\narXiv:2410.20011\\n[203] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma,\\nVincent Y. Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. 2022.\\nLarge Dual Encoders Are Generalizable Retrievers. In Proceedings of the 2022\\nConference on Empirical Methods in Natural Language Processing, EMNLP 2022,\\nAbu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zornitsa\\nKozareva, and Yue Zhang (Eds.). Association for Computational Linguistics,\\n9844–9855. https://doi.org/10.18653/V1/2022.EMNLP-MAIN.669\\n[204] Noa Nonkes, Sergei Agaronian, Evangelos Kanoulas, and Roxana Petcu. 2024.\\nLeveraging Graph Structures to Detect Hallucinations in Large Language Mod-\\nels. CoRR abs/2407.04485 (2024). https://doi.org/10.48550/ARXIV.2407.04485\\narXiv:2407.04485\\n[205] OpenAI. 2024. Introducing OpenAI o1. https://openai.com/o1.\\nAccessed:\\n2024-12-15.\\n[206] Laurel J. Orr, Srikanth Kandula, and Surajit Chaudhuri. 2019. Pushing Data-\\nInduced Predicates Through Joins in Big-Data Clusters. Proc. VLDB Endow. 13,\\n3 (2019), 252–265. https://doi.org/10.14778/3368289.3368292\\n[207] Laurel J. Orr, Atindriyo Sanyal, Xiao Ling, Karan Goel, and Megan Leszczyn-\\nski. 2021.\\nManaging ML Pipelines: Feature Stores and the Coming Wave\\nof Embedding Ecosystems.\\nProc. VLDB Endow. 14, 12 (2021), 3178–3181.\\nhttps://doi.org/10.14778/3476311.3476402\\n[208] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,\\nPamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,\\nAmanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\\n2022. Training language models to follow instructions with human feedback. In\\nAdvances in Neural Information Processing Systems 35: Annual Conference on Neu-\\nral Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, No-\\nvember 28 - December 9, 2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle\\nBelgrave, K. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper_files/paper/\\n2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html\\n[209] Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin\\nMuller, Margaret Li, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer,\\nGargi Ghosh, Mike Lewis, Ari Holtzman, and Srini Iyer. 2024. Byte Latent Trans-\\nformer: Patches Scale Better Than Tokens. (2024). https://ai.meta.com/research/\\npublications/byte-latent-transformer-patches-scale-better-than-tokens/\\n[210] James Jie Pan, Jianguo Wang, and Guoliang Li. 2024. Survey of vector database\\nmanagement systems. VLDB J. 33, 5 (2024), 1591–1615. https://doi.org/10.1007/\\nS00778-024-00864-X\\n[211] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu.\\n2024. Unifying large language models and knowledge graphs: A roadmap. IEEE\\nTransactions on Knowledge and Data Engineering (2024).\\n[212] Xuchen Pan, Dawei Gao, Yuexiang Xie, Zhewei Wei, Yaliang Li, Bolin Ding,\\nJi-Rong Wen, and Jingren Zhou. 2024. Very Large-Scale Multi-Agent Simulation\\nin AgentScope. CoRR abs/2407.17789 (2024). https://doi.org/10.48550/ARXIV.\\n2407.17789 arXiv:2407.17789\\n[213] Xiurui Pan, Endian Li, Qiao Li, Shengwen Liang, Yizhou Shan, Ke Zhou, Yingwei\\nLuo, Xiaolin Wang, and Jie Zhang. 2024. InstInfer: In-Storage Attention Of-\\nfloading for Cost-Effective Long-Context LLM Inference. CoRR abs/2409.04992\\n(2024). https://doi.org/10.48550/ARXIV.2409.04992 arXiv:2409.04992\\n[214] Joon Sung Park, Joseph C. O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy\\nLiang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra\\nof Human Behavior. In Proceedings of the 36th Annual ACM Symposium on\\nUser Interface Software and Technology, UIST 2023, San Francisco, CA, USA, 29\\nOctober 2023- 1 November 2023, Sean Follmer, Jeff Han, Jürgen Steimle, and\\nNathalie Henry Riche (Eds.). ACM, 2:1–2:22. https://doi.org/10.1145/3586183.\\n3606763\\n[215] Dylan Patel and Afzal Ahmad. 2023. The Inference Cost of Search Disruption –\\nLarge Language Model Cost Analysis. https://www.semianalysis.com/p/the-\\ninference-cost-of-search-disruption. Accessed: 2024-12-15.\\n[216] Liana Patel, Siddharth Jha, Carlos Guestrin, and Matei Zaharia. 2024. LOTUS: En-\\nabling Semantic Queries with LLMs Over Tables of Unstructured and Structured\\nData. CoRR abs/2407.11418 (2024). https://doi.org/10.48550/ARXIV.2407.11418\\narXiv:2407.11418\\n[217] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed\\nMaleki, and Ricardo Bianchini. 2024. Splitwise: Efficient Generative LLM Infer-\\nence Using Phase Splitting. In 51st ACM/IEEE Annual International Symposium\\non Computer Architecture, ISCA 2024, Buenos Aires, Argentina, June 29 - July 3,\\n2024. IEEE, 118–132. https://doi.org/10.1109/ISCA59077.2024.00019\\n[218] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023. Gorilla:\\nLarge Language Model Connected with Massive APIs. CoRR abs/2305.15334\\n(2023). https://doi.org/10.48550/ARXIV.2305.15334 arXiv:2305.15334\\n[219] Johns Paul, Bingsheng He, Shengliang Lu, and Chiew Tong Lau. 2020. Improving\\nexecution efficiency of just-in-time compilation based query processing on GPUs.\\nProceedings of the VLDB Endowment 14, 2 (2020), 202–214.\\n[220] Johns Paul, Shengliang Lu, and Bingsheng He. 2021. Database Systems on GPUs.\\nFound. Trends Databases 11, 1 (2021), 1–108. https://doi.org/10.1561/1900000076\\n[221] Tim Pearce and Jinyeop Song. 2024. Reconciling Kaplan and Chinchilla Scaling\\nLaws. CoRR abs/2406.12907 (2024). https://doi.org/10.48550/ARXIV.2406.12907\\narXiv:2406.12907\\n[222] Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna\\nGurevych. 2021. AdapterFusion: Non-Destructive Task Composition for Transfer\\nLearning. In Proceedings of the 16th Conference of the European Chapter of the\\nAssociation for Computational Linguistics: Main Volume, EACL 2021, Online,\\nApril 19 - 23, 2021, Paola Merlo, Jörg Tiedemann, and Reut Tsarfaty (Eds.).\\nAssociation for Computational Linguistics, 487–503. https://doi.org/10.18653/\\nV1/2021.EACL-MAIN.39\\n[223] Jonathan Pilault, Mahan Fathi, Orhan Firat, Chris Pal, Pierre-Luc Bacon, and\\nRoss Goroshin. 2023. Block-State Transformers. In Advances in Neural Infor-\\nmation Processing Systems 36: Annual Conference on Neural Information Pro-\\ncessing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,\\n2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt,\\nand Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/\\n16ccd203e9e3696a7ab0dcf568316379-Abstract-Conference.html\\n[224] Mohammadreza Pourreza, Hailong Li, Ruoxi Sun, Yeounoh Chung, Shayan\\nTalaei, Gaurav Tarlok Kakkar, Yu Gan, Amin Saberi, Fatma Ozcan, and Sercan Ö.\\nArik. 2024. CHASE-SQL: Multi-Path Reasoning and Preference Optimized\\nCandidate Selection in Text-to-SQL. CoRR abs/2410.01943 (2024). https://doi.\\norg/10.48550/ARXIV.2410.01943 arXiv:2410.01943\\n[225] Tavva Prudhvith, Chakrabarty Swattik, and Selvakumar Prakash. 2024. En-\\nhancing Retrieval Augmented Generation Systems with Knowledge Graphs. In\\n2024 International Conference on Electrical, Computer and Energy Technologies\\n(ICECET. IEEE, 1–8.\\n[226] Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and\\nMao Yang. 2024. Mutual Reasoning Makes Smaller LLMs Stronger Problem-\\nSolvers. CoRR abs/2408.06195 (2024).\\nhttps://doi.org/10.48550/ARXIV.2408.\\n06195 arXiv:2408.06195\\n[227] Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize\\nChen, Cheng Yang, Zhiyuan Liu, and Maosong Sun. 2024.\\nScaling Large-\\nLanguage-Model-based Multi-Agent Collaboration. CoRR abs/2406.07155 (2024).\\nhttps://doi.org/10.48550/ARXIV.2406.07155 arXiv:2406.07155\\n[228] Yulei Qian, Fengcun Li, Xiangyang Ji, Xiaoyu Zhao, Jianchao Tan, Kefeng Zhang,\\nand Xunliang Cai. 2024. EPS-MoE: Expert Pipeline Scheduler for Cost-Efficient\\nMoE Inference. CoRR abs/2410.12247 (2024). https://doi.org/10.48550/ARXIV.\\n2410.12247 arXiv:2410.12247\\n[229] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin,\\nXin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian,\\nRuobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong\\nSun. 2024. ToolLLM: Facilitating Large Language Models to Master 16000+ Real-\\nworld APIs. In The Twelfth International Conference on Learning Representations,\\nICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.\\nnet/forum?id=dHng2O0Jjr\\n[230] Han Qiu, Jiaxing Huang, Peng Gao, Qin Qi, Xiaoqin Zhang, Ling Shao, and\\nShijian Lu. 2024.\\nLongHalQA: Long-Context Hallucination Evaluation for\\nMultiModal Large Language Models. CoRR abs/2410.09962 (2024).\\nhttps:\\n//doi.org/10.48550/ARXIV.2410.09962 arXiv:2410.09962\\n[231] Haoran Qiu, Weichao Mao, Archit Patke, Shengkun Cui, Saurabh Jha, Chen\\nWang, Hubertus Franke, Zbigniew T. Kalbarczyk, Tamer Basar, and Ravis-\\nhankar K. Iyer. 2024. Efficient Interactive LLM Serving with Proxy Model-\\nbased Sequence Length Prediction.\\nCoRR abs/2404.08509 (2024).\\nhttps:\\n//doi.org/10.48550/ARXIV.2404.08509 arXiv:2404.08509\\n[232] Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei\\nYin, Jun Xu, and Ji-Rong Wen. 2024. Towards Completeness-Oriented Tool Re-\\ntrieval for Large Language Models. In Proceedings of the 33rd ACM International\\nConference on Information and Knowledge Management, CIKM 2024, Boise, ID,\\nUSA, October 21-25, 2024, Edoardo Serra and Francesca Spezzano (Eds.). ACM,\\n1930–1940. https://doi.org/10.1145/3627673.3679847\\n[233] Ernesto Quevedo, Jorge Yero, Rachel Koerner, Pablo Rivas, and Tomás Cerný.\\n2024. Detecting Hallucinations in Large Language Model Generation: A Token\\nProbability Approach. CoRR abs/2405.19648 (2024). https://doi.org/10.48550/\\nARXIV.2405.19648 arXiv:2405.19648\\n[234] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Ste-\\nfano Ermon, and Chelsea Finn. 2023. Direct Preference Optimization: Your\\nLanguage Model is Secretly a Reward Model. In Advances in Neural Infor-\\nmation Processing Systems 36: Annual Conference on Neural Information Pro-\\ncessing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,\\n2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt,\\nand Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/\\na85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html\\n[235] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits\\nof Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn.\\nRes. 21 (2020), 140:1–140:67. https://jmlr.org/papers/v21/20-074.html\\n[236] Isaac Rehg. 2024. KV-Compress: Paged KV-Cache Compression with Variable\\nCompression Rates per Attention Head. CoRR abs/2410.00161 (2024). https:\\n//doi.org/10.48550/ARXIV.2410.00161 arXiv:2410.00161\\n[237] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen.\\n2024. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context\\nLanguage Modeling. arXiv preprint arXiv:2406.07522 (2024).\\n[238] Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar, and Stephen W.\\nKeckler. 2016. vDNN: Virtualized deep neural networks for scalable, memory-\\nefficient neural network design. In 49th Annual IEEE/ACM International Sympo-\\nsium on Microarchitecture, MICRO 2016, Taipei, Taiwan, October 15-19, 2016. IEEE\\nComputer Society, 18:1–18:13. https://doi.org/10.1109/MICRO.2016.7783721\\n[239] Guilherme Rosa, Luiz Bonifacio, Vitor Jeronymo, Hugo Abonizio, Marzieh\\nFadaee, Roberto Lotufo, and Rodrigo Nogueira. 2022.\\nIn defense of cross-\\nencoders for zero-shot retrieval. arXiv preprint arXiv:2212.06121 (2022).\\n[240] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal,\\nand Aman Chadha. 2024. A Systematic Survey of Prompt Engineering in Large\\nLanguage Models: Techniques and Applications. CoRR abs/2402.07927 (2024).\\nhttps://doi.org/10.48550/ARXIV.2402.07927 arXiv:2402.07927\\n[241] Gaurav Sahu, Pau Rodríguez, Issam H. Laradji, Parmida Atighehchian, David\\nVázquez, and Dzmitry Bahdanau. 2022. Data Augmentation for Intent Classi-\\nfication with Off-the-shelf Large Language Models. In Proceedings of the 4th\\nWorkshop on NLP for Conversational AI, ConvAI@ACL 2022, Dublin, Ireland,\\nMay 27, 2022, Bing Liu, Alexandros Papangelis, Stefan Ultes, Abhinav Rastogi,\\nYun-Nung Chen, Georgios Spithourakis, Elnaz Nouri, and Weiyan Shi (Eds.).\\nAssociation for Computational Linguistics, 47–57. https://doi.org/10.18653/V1/\\n2022.NLP4CONVAI-1.5\\n[242] Viktor Sanca and Anastasia Ailamaki. 2024. Efficient Data Access Paths for\\nMixed Vector-Relational Search. In Proceedings of the 20th International Workshop\\non Data Management on New Hardware, DaMoN 2024, Santiago, Chile, 10 June\\n2024, Carsten Binnig and Nesime Tatbul (Eds.). ACM, 6:1–6:9. https://doi.org/\\n10.1145/3662010.3663448\\n[243] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika,\\nZaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful\\nBari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla,\\nTaewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan\\nChang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin\\nYong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos\\nRozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason Alan Fries,\\nRyan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and\\nAlexander M. Rush. 2022. Multitask Prompted Training Enables Zero-Shot Task\\nGeneralization. In The Tenth International Conference on Learning Representations,\\nICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. https://openreview.\\nnet/forum?id=9Vrb9D0WI4\\n[244] Keshav Santhanam, Deepti Raghavan, Muhammad Shahir Rahman, Thejas\\nVenkatesh, Neha Kunjal, Pratiksha Thaker, Philip Alexander Levis, and Matei\\nZaharia. 2024. ALTO: An Efficient Network Orchestrator for Compound AI\\nSystems. In Proceedings of the 4th Workshop on Machine Learning and Systems,\\nEuroMLSys 2024, Athens, Greece, 22 April 2024. ACM, 117–125. https://doi.org/\\n10.1145/3642970.3655844\\n[245] Bhaskarjit Sarmah, Dhagash Mehta, Benika Hall, Rohan Rao, Sunil Patel, and\\nStefano Pasquali. 2024. HybridRAG: Integrating Knowledge Graphs and Vec-\\ntor Retrieval Augmented Generation for Efficient Information Extraction. In\\nProceedings of the 5th ACM International Conference on AI in Finance. 608–616.\\n[246] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli,\\nEric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.\\nToolformer: Language Models Can Teach Themselves to Use Tools. In Advances\\nin Neural Information Processing Systems 36: Annual Conference on Neural Infor-\\nmation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December\\n10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz\\nHardt, and Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/\\nhash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html\\n[247] Sander Schulhoff, Michael Ilie, Nishant Balepur, Konstantine Kahadze, Amanda\\nLiu, Chenglei Si, Yinheng Li, Aayush Gupta, HyoJung Han, Sevien Schulhoff,\\nPranav Sandeep Dulepet, Saurav Vidyadhara, Dayeon Ki, Sweta Agrawal, Chau\\nPham, Gerson C. Kroiz, Feileen Li, Hudson Tao, Ashay Srivastava, Hevan-\\nder Da Costa, Saloni Gupta, Megan L. Rogers, Inna Goncearenco, Giuseppe\\nSarli, Igor Galynker, Denis Peskoff, Marine Carpuat, Jules White, Shyamal Anad-\\nkat, Alexander Miserlis Hoyle, and Philip Resnik. 2024. The Prompt Report:\\nA Systematic Survey of Prompting Techniques. CoRR abs/2406.06608 (2024).\\nhttps://doi.org/10.48550/ARXIV.2406.06608 arXiv:2406.06608\\n[248] Robert Schulze, Tom Schreiber, Ilya Yatsishin, Ryadh Dahimene, and Alexey\\nMilovidov. 2024. ClickHouse-Lightning Fast Analytics for Everyone. Proceedings\\nof the VLDB Endowment 17, 12 (2024), 3731–3744.\\n[249] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and\\nTri Dao. 2024. FlashAttention-3: Fast and Accurate Attention with Asynchrony\\nand Low-precision. CoRR abs/2407.08608 (2024).\\nhttps://doi.org/10.48550/\\nARXIV.2407.08608 arXiv:2407.08608\\n[250] Lior Shani, Aviv Rosenberg, Asaf B. Cassel, Oran Lang, Daniele Calandriello,\\nAvital Zipori, Hila Noga, Orgad Keller, Bilal Piot, Idan Szpektor, Avinatan\\nHassidim, Yossi Matias, and Rémi Munos. 2024. Multi-turn Reinforcement\\nLearning from Preference Human Feedback.\\nCoRR abs/2405.14655 (2024).\\nhttps://doi.org/10.48550/ARXIV.2405.14655 arXiv:2405.14655\\n[251] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li,\\nKaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. 2024. OmniQuant: Omnidirec-\\ntionally Calibrated Quantization for Large Language Models. In The Twelfth Inter-\\nnational Conference on Learning Representations, ICLR 2024, Vienna, Austria, May\\n7-11, 2024. OpenReview.net. https://openreview.net/forum?id=8Wuvhh0LYW\\n[252] Ying Sheng, Shiyi Cao, Dacheng Li, Banghua Zhu, Zhuohan Li, Danyang\\nZhuo, Joseph E. Gonzalez, and Ion Stoica. 2024. Fairness in Serving Large\\nLanguage Models. In 18th USENIX Symposium on Operating Systems Design\\nand Implementation, OSDI 2024, Santa Clara, CA, USA, July 10-12, 2024, Ada\\nGavrilovska and Douglas B. Terry (Eds.). USENIX Association, 965–988. https:\\n//www.usenix.org/conference/osdi24/presentation/sheng\\n[253] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi\\nChen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. 2023. FlexGen:\\nHigh-Throughput Generative Inference of Large Language Models with a Single\\nGPU. In International Conference on Machine Learning, ICML 2023, 23-29 July\\n2023, Honolulu, Hawaii, USA (Proceedings of Machine Learning Research, Vol. 202),\\nAndreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan\\nSabato, and Jonathan Scarlett (Eds.). PMLR, 31094–31116. https://proceedings.\\nmlr.press/v202/sheng23a.html\\n[254] Nikhil Sheoran, Supawit Chockchowwat, Arav Chheda, Suwen Wang, Riya\\nVerma, and Yongjoo Park. 2023. A Step Toward Deep Online Aggregation. Proc.\\nACM Manag. Data 1, 2 (2023), 124:1–124:28. https://doi.org/10.1145/3589269\\n[255] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer\\nSingh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with\\nAutomatically Generated Prompts. In Proceedings of the 2020 Conference on\\nEmpirical Methods in Natural Language Processing, EMNLP 2020, Online, Novem-\\nber 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.).\\nAssociation for Computational Linguistics, 4222–4235. https://doi.org/10.18653/\\nV1/2020.EMNLP-MAIN.346\\n[256] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan,\\nand Shunyu Yao. 2023.\\nReflexion: language agents with verbal rein-\\nforcement learning. In Advances in Neural Information Processing Sys-\\ntems 36: Annual Conference on Neural Information Processing Systems 2023,\\nNeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, Alice\\nOh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and\\nSergey Levine (Eds.).\\nhttp://papers.nips.cc/paper_files/paper/2023/hash/\\n1b44b878bb782e6954cd888628510e90-Abstract-Conference.html\\n[257] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared\\nCasper, and Bryan Catanzaro. 2019.\\nMegatron-LM: Training Multi-Billion\\nParameter Language Models Using Model Parallelism. CoRR abs/1909.08053\\n(2019). arXiv:1909.08053 http://arxiv.org/abs/1909.08053\\n[258] Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jor-\\ndan L. Boyd-Graber, and Lijuan Wang. 2023. Prompting GPT-3 To Be Reliable.\\nIn The Eleventh International Conference on Learning Representations, ICLR 2023,\\nKigali, Rwanda, May 1-5, 2023. OpenReview.net. https://openreview.net/forum?\\nid=98p5x51L5af\\n[259] Tomer Simon. 2024. The scientist of the scientist. AI Soc. 39, 2 (2024), 803–804.\\nhttps://doi.org/10.1007/S00146-022-01544-6\\n[260] Panagiotis Sioulas, Viktor Sanca, Ioannis Mytilinis, and Anastasia Ailamaki.\\n2021. Accelerating Complex Analytics using Speculation. In 11th Conference\\non Innovative Data Systems Research, CIDR 2021, Virtual Event, January 11-15,\\n2021, Online Proceedings. www.cidrdb.org. http://cidrdb.org/cidr2021/papers/\\ncidr2021_paper03.pdf\\n[261] Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen. 2024. PowerInfer: Fast\\nLarge Language Model Serving with a Consumer-grade GPU. In Proceedings\\nof the ACM SIGOPS 30th Symposium on Operating Systems Principles, SOSP\\n2024, Austin, TX, USA, November 4-6, 2024, Emmett Witchel, Christopher J.\\nRossbach, Andrea C. Arpaci-Dusseau, and Kimberly Keeton (Eds.). ACM, 590–\\n606. https://doi.org/10.1145/3694715.3695964\\n[262] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe,\\nChelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. 2020.\\nLearning to summarize with human feedback. In Advances in Neural In-\\nformation Processing Systems 33: Annual Conference on Neural Information\\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo\\nLarochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and\\nHsuan-Tien Lin (Eds.).\\nhttps://proceedings.neurips.cc/paper/2020/hash/\\n1f89885d556929e98d3ef9b86448f951-Abstract.html\\n[263] Foteini Strati, Sara McAllister, Amar Phanishayee, Jakub Tarnawski, and Ana\\nKlimovic. 2024. DéjàVu: KV-cache Streaming for Fast, Fault-tolerant Generative\\nLLM Serving. In Forty-first International Conference on Machine Learning, ICML\\n2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. https://openreview.\\nnet/forum?id=AbGbGZFYOD\\n[264] Weihang Su, Changyue Wang, Qingyao Ai, Yiran Hu, Zhijing Wu, Yujia Zhou,\\nand Yiqun Liu. 2024. Unsupervised Real-Time Hallucination Detection based\\non the Internal States of Large Language Models. In Findings of the Association\\nfor Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting,\\nAugust 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.).\\nAssociation for Computational Linguistics, 14379–14391. https://doi.org/10.\\n18653/V1/2024.FINDINGS-ACL.854\\n[265] Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. 2024.\\nTable meets llm: Can large language models understand structured table data?\\na benchmark and empirical study. In Proceedings of the 17th ACM International\\nConference on Web Search and Data Mining. 645–654.\\n[266] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin,\\nYeyun Gong, Heung-Yeung Shum, and Jian Guo. 2023.\\nThink-on-Graph:\\nDeep and Responsible Reasoning of Large Language Model with Knowledge\\nGraph. CoRR abs/2307.07697 (2023). https://doi.org/10.48550/ARXIV.2307.07697\\narXiv:2307.07697\\n[267] Xin Tan, Yimin Jiang, Yitao Yang, and Hong Xu. 2024. Teola: Towards End-\\nto-End Optimization of LLM-based Applications. CoRR abs/2407.00326 (2024).\\nhttps://doi.org/10.48550/ARXIV.2407.00326 arXiv:2407.00326\\n[268] James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel,\\nand Alon Y. Levy. 2021. From Natural Language Processing to Neural Databases.\\nProc. VLDB Endow. 14, 6 (2021), 1033–1039. https://doi.org/10.14778/3447689.\\n3447706\\n[269] Bing Tian, Haikun Liu, Yuhang Tang, Shihai Xiao, Zhuohui Duan, Xiaofei Liao,\\nXuecang Zhang, Junhua Zhu, and Yu Zhang. 2024. FusionANNS: An Efficient\\nCPU/GPU Cooperative Processing Architecture for Billion-scale Approximate\\nNearest Neighbor Search. CoRR abs/2409.16576 (2024). https://doi.org/10.48550/\\nARXIV.2409.16576 arXiv:2409.16576\\n[270] S. M. Towhidul Islam Tonmoy, S. M. Mehedi Zaman, Vinija Jain, Anku\\nRani, Vipula Rawte, Aman Chadha, and Amitava Das. 2024. A Comprehen-\\nsive Survey of Hallucination Mitigation Techniques in Large Language Mod-\\nels. CoRR abs/2401.01313 (2024). https://doi.org/10.48550/ARXIV.2401.01313\\narXiv:2401.01313\\n[271] Immanuel Trummer. 2022. DB-BERT: A Database Tuning Tool that \"Reads\\nthe Manual\". In SIGMOD ’22: International Conference on Management of Data,\\nPhiladelphia, PA, USA, June 12 - 17, 2022, Zachary G. Ives, Angela Bonifati, and\\nAmr El Abbadi (Eds.). ACM, 190–203. https://doi.org/10.1145/3514221.3517843\\n[272] Immanuel Trummer. 2023. From BERT to GPT-3 Codex: Harnessing the Potential\\nof Very Large Language Models for Data Management. CoRR abs/2306.09339\\n(2023). https://doi.org/10.48550/ARXIV.2306.09339 arXiv:2306.09339\\n[273] Matthias Urban and Carsten Binnig. 2024. CAESURA: Language Models as\\nMulti-Modal Query Planners. In 14th Conference on Innovative Data Systems\\nResearch, CIDR 2024, Chaminade, HI, USA, January 14-17, 2024. www.cidrdb.org.\\nhttps://www.cidrdb.org/cidr2024/papers/p14-urban.pdf\\n[274] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.\\nAttention is\\nAll you Need. In Advances in Neural Information Processing Systems 30: An-\\nnual Conference on Neural Information Processing Systems 2017, December 4-\\n9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy\\nBengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman\\nGarnett (Eds.). 5998–6008.\\nhttps://proceedings.neurips.cc/paper/2017/hash/\\n3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\\n[275] Shubham Vatsal and Harsh Dubey. 2024. A Survey of Prompt Engineering Meth-\\nods in Large Language Models for Different NLP Tasks. CoRR abs/2407.12994\\n(2024). https://doi.org/10.48550/ARXIV.2407.12994 arXiv:2407.12994\\n[276] Jonas Waldendorf, Barry Haddow, and Alexandra Birch. 2024. Contrastive\\nDecoding Reduces Hallucinations in Large Multilingual Machine Translation\\nModels. In Proceedings of the 18th Conference of the European Chapter of the\\nAssociation for Computational Linguistics, EACL 2024 - Volume 1: Long Papers, St.\\nJulian’s, Malta, March 17-22, 2024, Yvette Graham and Matthew Purver (Eds.).\\nAssociation for Computational Linguistics, 2526–2539. https://aclanthology.\\norg/2024.eacl-long.155\\n[277] Fei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen, and Sercan Ö. Arik. 2024.\\nAstute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\\nConflicts for Large Language Models. CoRR abs/2410.07176 (2024).\\nhttps:\\n//doi.org/10.48550/ARXIV.2410.07176 arXiv:2410.07176\\n[278] Jinyuan Wang, Junlong Li, and Hai Zhao. 2023. Self-prompted chain-of-thought\\non large language models for open-domain multi-hop reasoning. arXiv preprint\\narXiv:2310.13552 (2023).\\n[279] Ke Wang, Jiahui Zhu, Minjie Ren, Zeming Liu, Shiwei Li, Zongye Zhang,\\nChenkai Zhang, Xiaoyu Wu, Qiqi Zhan, Qingjie Liu, and Yunhong Wang.\\n2024. A Survey on Data Synthesis and Augmentation for Large Language Mod-\\nels. CoRR abs/2410.12896 (2024). https://doi.org/10.48550/ARXIV.2410.12896\\narXiv:2410.12896\\n[280] Wenyi Wang, Hisham A Alyahya, Dylan R Ashley, Oleg Serikov, Dmitrii\\nKhizbullin, Francesco Faccio, and Jürgen Schmidhuber. 2024. How to Cor-\\nrectly do Semantic Backpropagation on Language-based Agentic Systems. arXiv\\npreprint arXiv:2412.03624 (2024).\\n[281] Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and\\nFuru Wei. 2023. Augmenting Language Models with Long-Term Memory. In Ad-\\nvances in Neural Information Processing Systems 36: Annual Conference on Neural\\nInformation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, De-\\ncember 10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko,\\nMoritz Hardt, and Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/\\n2023/hash/ebd82705f44793b6f9ade5a669d0f0bf-Abstract-Conference.html\\n[282] Xiaochen Wang, Junqing He, Liang Chen, Reza Haf Zhe Yang, Yiru Wang,\\nXiangdi Meng, Kunhao Pan, and Zhifang Sui. 2024. SG-FSM: A Self-Guiding\\nZero-Shot Prompting Paradigm for Multi-Hop Question Answering Based on\\nFinite State Machine. arXiv preprint arXiv:2410.17021 (2024).\\n[283] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan\\nNarang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-Consistency Im-\\nproves Chain of Thought Reasoning in Language Models. In The Eleventh Inter-\\nnational Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May\\n1-5, 2023. OpenReview.net. https://openreview.net/forum?id=1PL1NIMMrw\\n[284] Xuezhi Wang and Denny Zhou. 2024. Chain-of-Thought Reasoning Without\\nPrompting. CoRR abs/2402.10200 (2024). https://doi.org/10.48550/ARXIV.2402.\\n10200 arXiv:2402.10200\\n[285] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith,\\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-Instruct: Aligning Lan-\\nguage Models with Self-Generated Instructions. In Proceedings of the 61st Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\nACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber,\\nand Naoaki Okazaki (Eds.). Association for Computational Linguistics, 13484–\\n13508. https://doi.org/10.18653/V1/2023.ACL-LONG.754\\n[286] Zhiruo Wang, Zhoujun Cheng, Hao Zhu, Daniel Fried, and Graham Neubig.\\n2024. What Are Tools Anyway? A Survey from the Language Model Perspec-\\ntive. CoRR abs/2403.15452 (2024). https://doi.org/10.48550/ARXIV.2403.15452\\narXiv:2403.15452\\n[287] Zheng Wang, Shu Xian Teo, Jieer Ouyang, Yongjun Xu, and Wei Shi. 2024.\\nM-RAG: Reinforcing Large Language Model Performance through Retrieval-\\nAugmented Generation with Multiple Partitions. In Proceedings of the 62nd\\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long\\nPapers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, Lun-Wei Ku, Andre\\nMartins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,\\n1966–1978. https://doi.org/10.18653/V1/2024.ACL-LONG.108\\n[288] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian\\nLester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022. Finetuned Language\\nModels are Zero-Shot Learners. In The Tenth International Conference on Learning\\nRepresentations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\\nhttps://openreview.net/forum?id=gEZrGCozdqR\\n[289] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter,\\nFei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought\\nPrompting Elicits Reasoning in Large Language Models. In Advances in Neural\\nInformation Processing Systems 35: Annual Conference on Neural Information\\nProcessing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 -\\nDecember 9, 2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave,\\nK. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper_files/paper/2022/hash/\\n9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html\\n[290] Steven Whang and Jae-Gil Lee. 2020. Data Collection and Quality Challenges\\nfor Deep Learning. Proc. VLDB Endow. 13, 12 (2020), 3429–3432. https://doi.org/\\n10.14778/3415478.3415562\\n[291] Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang, Xuanzhe Liu, and\\nXin Jin. 2023. Fast Distributed Inference Serving for Large Language Mod-\\nels. CoRR abs/2305.05920 (2023). https://doi.org/10.48550/ARXIV.2305.05920\\narXiv:2305.05920\\n[292] Xun Wu, Shaohan Huang, and Furu Wei. 2024. Mixture of LoRA Experts. In The\\nTwelfth International Conference on Learning Representations, ICLR 2024, Vienna,\\nAustria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?id=\\nuWvKBCYh4S\\n[293] Yingjun Wu, Chee Yong Chan, and Kian-Lee Tan. 2016. Transaction Healing:\\nScaling Optimistic Concurrency Control on Multicores. In Proceedings of the 2016\\nInternational Conference on Management of Data, SIGMOD Conference 2016, San\\nFrancisco, CA, USA, June 26 - July 01, 2016, Fatma Özcan, Georgia Koutrika, and\\nSam Madden (Eds.). ACM, 1689–1704. https://doi.org/10.1145/2882903.2915202\\n[294] Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy.\\n2022. Memorizing Transformers. In The Tenth International Conference on Learn-\\ning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\\nhttps://openreview.net/forum?id=TrjbxzRcnf-\\n[295] Guangxuan Xiao, Ji Lin, Mickaël Seznec, Hao Wu, Julien Demouth, and Song\\nHan. 2023. SmoothQuant: Accurate and Efficient Post-Training Quantization for\\nLarge Language Models. In International Conference on Machine Learning, ICML\\n2023, 23-29 July 2023, Honolulu, Hawaii, USA (Proceedings of Machine Learning\\nResearch, Vol. 202), Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara\\nEngelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 38087–38099.\\nhttps://proceedings.mlr.press/v202/xiao23c.html\\n[296] Junjie Xing, Yeye He, Mengyu Zhou, Haoyu Dong, Shi Han, Dongmei Zhang,\\nand Surajit Chaudhuri. 2024. Table-LLM-Specialist: Language Model Specialists\\nfor Tables using Iterative Generator-Validator Fine-tuning. CoRR abs/2410.12164\\n(2024). https://doi.org/10.48550/ARXIV.2410.12164 arXiv:2410.12164\\n[297] Yizhe Xiong, Xiansheng Chen, Xin Ye, Hui Chen, Zijia Lin, Haoran Lian, Jianwei\\nNiu, and Guiguang Ding. 2024. Temporal Scaling Law for Large Language\\nModels. CoRR abs/2404.17785 (2024).\\nhttps://doi.org/10.48550/ARXIV.2404.\\n17785 arXiv:2404.17785\\n[298] Jiale Xu, Rui Zhang, Cong Guo, Weiming Hu, Zihan Liu, Feiyang Wu, Yu Feng,\\nShixuan Sun, Changxu Shao, Yuhong Guo, Junping Zhao, Ke Zhang, Minyi\\nGuo, and Jingwen Leng. 2024. vTensor: Flexible Virtual Tensor Management for\\nEfficient LLM Serving. CoRR abs/2407.15309 (2024). https://doi.org/10.48550/\\nARXIV.2407.15309 arXiv:2407.15309\\n[299] Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer,\\nSee-Kiong Ng, and Jiashi Feng. 2024. MAgIC: Investigation of Large Language\\nModel Powered Multi-Agent in Cognition, Adaptability, Rationality and Collab-\\noration. In Proceedings of the 2024 Conference on Empirical Methods in Natural\\nLanguage Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, Yaser\\nAl-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Compu-\\ntational Linguistics, 7315–7332. https://aclanthology.org/2024.emnlp-main.416\\n[300] Qiancheng Xu, Yongqi Li, Heming Xia, and Wenjie Li. 2024. Enhancing Tool\\nRetrieval with Iterative Feedback from Large Language Models. In Findings\\nof the Association for Computational Linguistics: EMNLP 2024, Miami, Florida,\\nUSA, November 12-16, 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung\\nChen (Eds.). Association for Computational Linguistics, 9609–9619.\\nhttps:\\n//aclanthology.org/2024.findings-emnlp.561\\n[301] Sheng Xu, Mike Chen, and Shuwen Chen. 2024. Enhancing Retrieval-Augmented\\nGeneration Models with Knowledge Graphs: Innovative Practices Through a\\nDual-Pathway Approach. In International Conference on Intelligent Computing.\\nSpringer, 398–409.\\n[302] Weijia Xu, Andrzej Banburski, and Nebojsa Jojic. 2024. Reprompting: Automated\\nChain-of-Thought Prompt Inference Through Gibbs Sampling. In Forty-first\\nInternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July\\n21-27, 2024. OpenReview.net. https://openreview.net/forum?id=D8zn1DnTuj\\n[303] Yuming Xu, Hengyu Liang, Jin Li, Shuotao Xu, Qi Chen, Qianxi Zhang, Cheng Li,\\nZiyue Yang, Fan Yang, Yuqing Yang, Peng Cheng, and Mao Yang. 2023. SPFresh:\\nIncremental In-Place Update for Billion-Scale Vector Search. In Proceedings\\nof the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz,\\nGermany, October 23-26, 2023, Jason Flinn, Margo I. Seltzer, Peter Druschel,\\nAntoine Kaufmann, and Jonathan Mace (Eds.). ACM, 545–561. https://doi.org/\\n10.1145/3600006.3613166\\n[304] Yi Xu, Ziming Mao, Xiangxi Mo, Shu Liu, and Ion Stoica. 2024. Pie: Pooling\\nCPU Memory for LLM Inference. arXiv preprint arXiv:2411.09317 (2024).\\n[305] Ziwei Xu, Sanjay Jain, and Mohan S. Kankanhalli. 2024. Hallucination is In-\\nevitable: An Innate Limitation of Large Language Models. CoRR abs/2401.11817\\n(2024). https://doi.org/10.48550/ARXIV.2401.11817 arXiv:2401.11817\\n[306] Zifei Xu, Alexander Lan, Wanzin Yazar, Tristan Webb, Sayeh Sharify, and Xin\\nWang. 2024. Scaling laws for post-training quantized large language mod-\\nels. CoRR abs/2410.12119 (2024). https://doi.org/10.48550/ARXIV.2410.12119\\narXiv:2410.12119\\n[307] Maryann Xue, Yingyi Bu, Abhishek Somani, Wenchen Fan, Ziqi Liu, Steven\\nChen, Herman Van Hovell, Bart Samwel, Mostafa Mokhtar, Rk Korlapati, Andy\\nLam, Yunxiao Ma, Vuk Ercegovac, Jiexing Li, Alexander Behm, Yuanjian Li,\\nXiao Li, Sriram Krishnamurthy, Amit Shukla, Michalis Petropoulos, Sameer\\nParanjpye, Reynold Xin, and Matei Zaharia. 2024. Adaptive and Robust Query\\nExecution for Lakehouses At Scale. Proc. VLDB Endow. 17, 12 (2024), 3947–3959.\\nhttps://www.vldb.org/pvldb/vol17/p3947-bu.pdf\\n[308] Scott Yak, Yihe Dong, Javier Gonzalvo, and Sercan Arik. 2023. IngesTables:\\nScalable and Efficient Training of LLM-Enabled Tabular Foundation Models. In\\nNeurIPS 2023 Second Table Representation Learning Workshop.\\n[309] Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sa-\\njal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun\\nKong, Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Et-\\ning Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Ni-\\nrav Shah, Rakesh Wanga, Anuj Kumar, Wen-tau Yih, and Xin Luna Dong.\\n2024. CRAG - Comprehensive RAG Benchmark. CoRR abs/2406.04744 (2024).\\nhttps://doi.org/10.48550/ARXIV.2406.04744 arXiv:2406.04744\\n[310] Yifei Yang, Xiangyao Yu, Marco Serafini, Ashraf Aboulnaga, and Michael Stone-\\nbraker. 2024. FlexpushdownDB: rethinking computation pushdown for cloud\\nOLAP DBMSs. VLDB J. 33, 5 (2024), 1643–1670. https://doi.org/10.1007/S00778-\\n024-00867-8\\n[311] Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang,\\nKuntai Du, Shan Lu, and Junchen Jiang. 2024. CacheBlend: Fast Large Language\\nModel Serving for RAG with Cached Knowledge Fusion. CoRR abs/2405.16444\\n(2024). https://doi.org/10.48550/ARXIV.2405.16444 arXiv:2405.16444\\n[312] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao,\\nand Karthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solv-\\ning with Large Language Models. In Advances in Neural Information Pro-\\ncessing Systems 36: Annual Conference on Neural Information Processing Sys-\\ntems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023,\\nAlice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt,\\nand Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/\\n271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html\\n[313] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R.\\nNarasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Act-\\ning in Language Models. In The Eleventh International Conference on Learning\\nRepresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\\nhttps://openreview.net/forum?id=WE_vluYUL-X\\n[314] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Richard James, Jure\\nLeskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-Tau Yih. 2023.\\nRetrieval-Augmented Multimodal Language Modeling. In International Con-\\nference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii,\\nUSA (Proceedings of Machine Learning Research, Vol. 202), Andreas Krause,\\nEmma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and\\nJonathan Scarlett (Eds.). PMLR, 39755–39769. https://proceedings.mlr.press/\\nv202/yasunaga23a.html\\n[315] Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, and Furu Wei.\\n2024. Differential transformer. arXiv preprint arXiv:2410.05258 (2024).\\n[316] Zihao Ye, Ruihang Lai, Bo-Ru Lu, Chien-Yu Lin, Size Zheng, Lequn Chen, Tianqi\\nChen, and Luis Ceze. 2024. Cascade inference: Memory bandwidth efficient\\nshared prefix batch decoding.\\n[317] Chengye Yu, Tianyu Wang, Zili Shao, Linjie Zhu, Xu Zhou, and Song Jiang.\\n2024. Twinpilots: A new computing paradigm for gpu-cpu parallel llm inference.\\nIn Proceedings of the 17th ACM International Systems and Storage Conference.\\n91–103.\\n[318] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-\\nGon Chun. 2022. Orca: A Distributed Serving System for Transformer-Based\\nGenerative Models. In 16th USENIX Symposium on Operating Systems Design\\nand Implementation, OSDI 2022, Carlsbad, CA, USA, July 11-13, 2022, Marcos K.\\nAguilera and Hakim Weatherspoon (Eds.). USENIX Association, 521–538. https:\\n//www.usenix.org/conference/osdi22/presentation/yu\\n[319] Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, and Jiajun Chen. 2023. Towards\\nBetter Chain-of-Thought Prompting Strategies: A Survey. CoRR abs/2310.04959\\n(2023). https://doi.org/10.48550/ARXIV.2310.04959 arXiv:2310.04959\\n[320] Ruize Yuan, Xiang Ao, Li Zeng, and Qing He. 2024. DRAMA: Dynamic Multi-\\nGranularity Graph Estimate Retrieval over Tabular and Textual Question An-\\nswering. In Proceedings of the 2024 Joint International Conference on Computa-\\ntional Linguistics, Language Resources and Evaluation, LREC/COLING 2024, 20-25\\nMay, 2024, Torino, Italy, Nicoletta Calzolari, Min-Yen Kan, Véronique Hoste,\\nAlessandro Lenci, Sakriani Sakti, and Nianwen Xue (Eds.). ELRA and ICCL,\\n5365–5375. https://aclanthology.org/2024.lrec-main.477\\n[321] Ye Yuan, Bo Tang, Tianfei Zhou, Zhiwei Zhang, and Jianbin Qin. 2024. nsDB:\\nArchitecting the Next Generation Database by Integrating Neural and Symbolic\\nSystems (Vision). Proc. VLDB Endow. 17, 11 (2024), 3283–3289. https://www.\\nvldb.org/pvldb/vol17/p3283-tang.pdf\\n[322] Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao\\nXue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, Beidi Chen,\\nGuangyu Sun, and Kurt Keutzer. 2024. LLM Inference Unveiled: Survey and\\nRoofline Model Insights. CoRR abs/2402.16363 (2024). https://doi.org/10.48550/\\nARXIV.2402.16363 arXiv:2402.16363\\n[323] Matei Zaharia, Omar Khattab, Lingjiao Chen, Jared Quincy Davis, Heather\\nMiller, Chris Potts, James Zou, Michael Carbin, Jonathan Frankle, Naveen Rao,\\nand Ali Ghodsi. 2024. The Shift from Models to Compound AI Systems. https:\\n//bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/.\\n[324] Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui\\nHuang, Shufei Zhang, Marco Pavone, Yuqiang Li, Wanli Ouyang, and Dongzhan\\nZhou. 2024. LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level\\nMathematical Reasoning. CoRR abs/2410.02884 (2024). https://doi.org/10.48550/\\nARXIV.2410.02884 arXiv:2410.02884\\n[325] Kai Zhang, Liqian Peng, Congchao Wang, Alec Go, and Xiaozhong Liu. 2024.\\nLLM Cascade with Multi-Objective Optimal Consideration. CoRR abs/2410.08014\\n(2024). https://doi.org/10.48550/ARXIV.2410.08014 arXiv:2410.08014\\n[326] Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. 2023.\\nRetrieve Anything To Augment Large Language Models. CoRR abs/2310.07554\\n(2023). https://doi.org/10.48550/ARXIV.2310.07554 arXiv:2310.07554\\n[327] Qianxi Zhang, Shuotao Xu, Qi Chen, Guoxin Sui, Jiadong Xie, Zhizhen Cai,\\nYaoqi Chen, Yinxuan He, Yuqing Yang, Fan Yang, Mao Yang, and Lidong Zhou.\\n2023. VBASE: Unifying Online Vector Similarity Search and Relational Queries\\nvia Relaxed Monotonicity. In 17th USENIX Symposium on Operating Systems\\nDesign and Implementation, OSDI 2023, Boston, MA, USA, July 10-12, 2023, Roxana\\nGeambasu and Ed Nightingale (Eds.). USENIX Association, 377–395.\\nhttps:\\n//www.usenix.org/conference/osdi23/presentation/zhang-qianxi\\n[328] Shuo Zhang, Zezhou Huang, and Eugene Wu. 2024. Data Cleaning Using Large\\nLanguage Models. arXiv preprint arXiv:2410.15547 (2024).\\n[329] Xiaoming Zhang, Ming Wang, Xiaocui Yang, Daling Wang, Shi Feng, and Yifei\\nZhang. 2024. Hierarchical Retrieval-Augmented Generation Model with Rethink\\nfor Multi-hop Question Answering. arXiv preprint arXiv:2408.11875 (2024).\\n[330] Yue Zhang, Hongliang Fei, Dingcheng Li, and Ping Li. 2022. PromptGen: Au-\\ntomatically Generate Prompts using Generative Models. In Findings of the\\nAssociation for Computational Linguistics: NAACL 2022, Seattle, WA, United\\nStates, July 10-15, 2022, Marine Carpuat, Marie-Catherine de Marneffe, and Iván\\nVladimir Meza Ruíz (Eds.). Association for Computational Linguistics, 30–37.\\nhttps://doi.org/10.18653/V1/2022.FINDINGS-NAACL.3\\n[331] Zihan Zhang, Meng Fang, and Ling Chen. 2024. RetrievalQA: Assessing Adap-\\ntive Retrieval-Augmented Generation for Short-form Open-Domain Question\\nAnswering. In Findings of the Association for Computational Linguistics, ACL 2024,\\nBangkok, Thailand and virtual meeting, August 11-16, 2024, Lun-Wei Ku, Andre\\nMartins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,\\n6963–6975. https://doi.org/10.18653/V1/2024.FINDINGS-ACL.415\\n[332] Ziqi Zhang, Cunxiang Wang, Xiao Xiong, Yue Zhang, and Donglin Wang. 2024.\\nNash CoT: Multi-Path Inference with Preference Equilibrium. In Proceedings\\nof the 2024 Conference on Empirical Methods in Natural Language Processing,\\nEMNLP 2024, Miami, FL, USA, November 12-16, 2024, Yaser Al-Onaizan, Mohit\\nBansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics,\\n14572–14587. https://aclanthology.org/2024.emnlp-main.807\\n[333] Xuanlei Zhao, Bin Jia, Haotian Zhou, Ziming Liu, Shenggan Cheng, and Yang\\nYou. 2024. HeteGen: Efficient Heterogeneous Parallel Inference for Large Lan-\\nguage Models on Resource-Constrained Devices. In Proceedings of the Seventh\\nAnnual Conference on Machine Learning and Systems, MLSys 2024, Santa Clara,\\nCA, USA, May 13-16, 2024, Phillip B. Gibbons, Gennady Pekhimenko, and Christo-\\npher De Sa (Eds.). mlsys.org. https://proceedings.mlsys.org/paper_files/paper/\\n2024/hash/5431dca75a8d2abc1fb51e89e8324f10-Abstract-Conference.html\\n[334] Yilong Zhao, Shuo Yang, Kan Zhu, Lianmin Zheng, Baris Kasikci, Yang Zhou,\\nJiarong Xing, and Ion Stoica. 2024. BlendServe: Optimizing Offline Inference for\\nAuto-regressive Large Models with Resource-aware Batching. arXiv preprint\\narXiv:2411.16102 (2024).\\n[335] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun,\\nCody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez,\\nClark W. Barrett, and Ying Sheng. 2023. Efficiently Programming Large Lan-\\nguage Models using SGLang. CoRR abs/2312.07104 (2023). https://doi.org/10.\\n48550/ARXIV.2312.07104 arXiv:2312.07104\\n[336] Wenhao Zheng, Yixiao Chen, Weitong Zhang, Souvik Kundu, Yun Li,\\nZhengzhong Liu, Eric P Xing, Hongyi Wang, and Huaxiu Yao. 2024. CITER:\\nCollaborative Inference for Efficient Large Language Model Decoding with\\nToken-Level Routing. In Adaptive Foundation Models: Evolving AI for Personal-\\nized and Efficient Learning.\\n[337] Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang, and Yang\\nYou. 2023. Response Length Perception and Sequence Scheduling: An LLM-\\nEmpowered LLM Inference Pipeline. In Advances in Neural Information Pro-\\ncessing Systems 36: Annual Conference on Neural Information Processing Sys-\\ntems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023,\\nAlice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt,\\nand Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/\\nce7ff3405c782f761fac7f849b41ae9a-Abstract-Conference.html\\n[338] Han Zhong, Guhao Feng, Wei Xiong, Li Zhao, Di He, Jiang Bian, and Li-\\nwei Wang. 2024.\\nDPO Meets PPO: Reinforced Token Optimization for\\nRLHF. CoRR abs/2404.18922 (2024). https://doi.org/10.48550/ARXIV.2404.18922\\narXiv:2404.18922\\n[339] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu,\\nXin Jin, and Hao Zhang. 2024. DistServe: Disaggregating Prefill and Decoding for\\nGoodput-optimized Large Language Model Serving. In 18th USENIX Symposium\\non Operating Systems Design and Implementation, OSDI 2024, Santa Clara, CA,\\nUSA, July 10-12, 2024, Ada Gavrilovska and Douglas B. Terry (Eds.). USENIX\\nAssociation, 193–210. https://www.usenix.org/conference/osdi24/presentation/\\nzhong-yinmin\\n[340] Xuanhe Zhou, Guoliang Li, and Zhiyuan Liu. 2023.\\nLLM As DBA.\\nCoRR abs/2308.05481 (2023).\\nhttps://doi.org/10.48550/ARXIV.2308.05481\\narXiv:2308.05481\\n[341] Yue Zhou, Chenlu Guo, Xu Wang, Yi Chang, and Yuan Wu. 2024. A Survey on\\nData Augmentation in Large Model Era. CoRR abs/2401.15422 (2024). https:\\n//doi.org/10.48550/ARXIV.2401.15422 arXiv:2401.15422\\n[342] Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming\\nLou, Luning Wang, Zhihang Yuan, Xiuhong Li, Shengen Yan, Guohao Dai,\\nXiao-Ping Zhang, Yuhan Dong, and Yu Wang. 2024. A Survey on Efficient\\nInference for Large Language Models. CoRR abs/2404.14294 (2024).\\nhttps:\\n//doi.org/10.48550/ARXIV.2404.14294 arXiv:2404.14294\\n[343] Kan Zhu, Yilong Zhao, Liangyu Zhao, Gefei Zuo, Yile Gu, Dedong Xie, Yufei\\nGao, Qinyu Xu, Tian Tang, Zihao Ye, et al. 2024. NanoFlow: Towards Optimal\\nLarge Language Model Serving Throughput. arXiv preprint arXiv:2408.12757\\n(2024).\\n',\n",
       " 'type': 'Document'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be2a4bd",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b22f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chunks = []\n",
    "chunk_by_doc = {}\n",
    "for doc in docs:\n",
    "    doc_chunks = []\n",
    "    for i, chunk in enumerate(text_splitter.split_text(doc.page_content)):\n",
    "        metadata = doc.metadata.copy()\n",
    "        metadata[\"chunk_index\"] = i\n",
    "        doc_chunks.append(Document(page_content=chunk, metadata=metadata))\n",
    "    chunk_by_doc[doc.metadata.get(\"Title\", \"Document\")] = (\n",
    "        doc_chunks\n",
    "    )\n",
    "    final_chunks.extend(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe76230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk Stats : openai defined splitter ---\n",
      "Total chunks: 1873\n",
      "Avg size: 274.45, Max: 333, Min: 56\n"
     ]
    }
   ],
   "source": [
    "chunk_lengths = [\n",
    "    len(tokenizer.encode(chunk.page_content))\n",
    "    for chunk in final_chunks\n",
    "]\n",
    "\n",
    "print(f\"\\n--- Chunk Stats : openai defined splitter ---\")\n",
    "print(f\"Total chunks: {len(final_chunks)}\")\n",
    "print(\n",
    "    f\"Avg size: {np.mean(chunk_lengths):.2f}, Max: {np.max(chunk_lengths)}, Min: {np.min(chunk_lengths)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd11525f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of model_id Chunk Sizes\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(chunk_lengths, bins=20, color=\"skyblue\")\n",
    "plt.title(\"Token Count Distribution per Chunk\")\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c2c31df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ccbab6379a4961aad32ae84152f3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Document:', layout=Layout(width='50%'), options=('Trustworthy and Efficie…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZING CHUNKING 1\n",
    "\n",
    "doc_selector = widgets.Dropdown(\n",
    "    options=list(chunk_by_doc.keys()),\n",
    "    description=\"Document:\",\n",
    "    layout=widgets.Layout(width=\"50%\"),\n",
    ")\n",
    "\n",
    "chunk_slider = widgets.IntSlider(min=0, max=1, step=1, description=\"Chunk:\")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "\n",
    "def update_slider(*args):\n",
    "    selected_doc = doc_selector.value\n",
    "    chunk_slider.max = len(chunk_by_doc[selected_doc]) - 1\n",
    "    chunk_slider.value = 0\n",
    "    show_chunk(0)\n",
    "\n",
    "\n",
    "def show_chunk(i):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        selected_doc = doc_selector.value\n",
    "        chunk = chunk_by_doc[selected_doc][i]\n",
    "        print(chunk.metadata)\n",
    "        print(\"\\n\" + chunk.page_content[:1000])\n",
    "\n",
    "\n",
    "chunk_slider.observe(lambda change: show_chunk(change[\"new\"]), names=\"value\")\n",
    "doc_selector.observe(update_slider, names=\"value\")\n",
    "\n",
    "display(widgets.VBox([doc_selector, chunk_slider, output_area]))\n",
    "update_slider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "728fc94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stored in Qdrant Vector DB ---\n",
      "Collection: LLM-papers openai\n"
     ]
    }
   ],
   "source": [
    "# ## Step 3: Store in Qdrant Vector DB\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "db = Qdrant.from_documents(\n",
    "    documents=final_chunks,\n",
    "    embedding=embedding,\n",
    "    location=\"localhost:6333\",\n",
    "    collection_name=\"LLM-papers openai\",\n",
    "    prefer_grpc=False,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Stored in Qdrant Vector DB ---\")\n",
    "print(f\"Collection: {db.collection_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "618e70d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA\n",
    "texts = [doc.page_content for doc in final_chunks]\n",
    "embeddings = embedding.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "803ce556",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "points = pca.fit_transform(embeddings)\n",
    "\n",
    "# Create color group\n",
    "doc_names = [\n",
    "    doc.metadata.get(\"Title\", \"Doc \" + str(i))\n",
    "    for i, doc in enumerate(final_chunks)\n",
    "]\n",
    "unique_doc_ids = {name: i for i, name in enumerate(set(doc_names))}\n",
    "colors = [unique_doc_ids[name] for name in doc_names]\n",
    "\n",
    "# DataFrame for plot\n",
    "df = pd.DataFrame({\n",
    "    \"x\": points[:, 0],\n",
    "    \"y\": points[:, 1],\n",
    "    \"document\": doc_names,\n",
    "    \"chunk\": [doc.metadata.get(\"chunk_index\", 0) for doc in final_chunks],\n",
    "    \"preview\": [doc.page_content[:100].replace(\"\\n\", \" \") for doc in final_chunks],\n",
    "    \"color_id\": colors\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f0744db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           0,
           "Trustworthy and Efficient LLMs Meet Databases Kyoungmin Kim kyoung-min.kim@epfl.ch EPFL Switzerland "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           1,
           "fields with their ability to understand and generate human-like text. In the database domain, resear"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           2,
           "tegrating LLMs with external data sources, such as vector databases and document retrieval systems i"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           3,
           "further amplify inference demands, as generating final answers may require multiple LLM calls to enh"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           4,
           "niques. Prerequisites include basic database knowledge and an understanding of GPUs. Familiarity wit"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           5,
           "arXiv:2412.18022v1  [cs.DB]  23 Dec 2024 Improving Bare LLMs (§2.1.2) Fine-tuning, LoRA, RLHF Backgr"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           6,
           "Operation (§2.2.3) Flash/sparse/flex attention  Data (§2.2.4) KV compression, model quantization, SS"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           7,
           "Figure 1 visualizes the outline with keywords for each subsection. Section 2.1 focuses on improving "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           8,
           "Background. Large Language Models (LLMs) function as text-in, text-out systems, generating texts bas"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           9,
           "methods like greedy or beam search, or probabilistic approaches such as nucleus sampling [80, 114], "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           10,
           "lengthens [230]. Scaling laws [113, 130, 221] explain that error rates decrease as model size and tr"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           11,
           "such approaches have been extensively studied from the classic ML era, we target more LLM-specific a"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           12,
           "reward model in reinforcement learning (RL), guiding the LLM through RL to produce desired outputs. "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           13,
           "and tools. This section focuses on what and how to retrieve. When to re- trieve is the key to autono"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           14,
           "from long input can also be done without maintaining a separate memory store, but by sparsifying the"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           15,
           "limited context or data they can utilize per inference, but main- taining a large set of retrieval e"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           16,
           "adaptive retrieval [11, 32, 123, 125, 159, 185, 331], which adaptively retrieves information multipl"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           17,
           "Semantic variables [173] regard LLM input and output tokens as dependent variables to explicitly mod"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           18,
           "between a token and its preceding tokens, effectively capturing inter-token relationships and managi"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           19,
           "releasing their KVs from the memory, and restarted (refilled) later [146]. Due to the low PCIe bandw"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           20,
           "OLAP in databases, the operations are simpler yet much more time- sensitive, where the requests shou"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           21,
           "for finer-grained pipelining, increasing the overlap of computation, memory operation, and data tran"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           22,
           "FlashAttention [59, 60, 249] has become a de facto standard as an efficient attention implementation"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           23,
           "of long documents can be precomputed, compressed, and fetched for later retrievals [184]. To reduce "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           24,
           "on the computation speed (e.g., GPU FLOPS) and memory band- width, which acts as a theoretical hardw"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           25,
           "opportunity, [311] uses the KVs of multiple token sequences to approximate the KVs of the concatenat"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           26,
           "internals such as database tuning [271, 340], text2sql [151, 224] and query optimization [8, 168]. A"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           27,
           "requests connected via semantic variables or shared prefixes [139]. 2.3.3 DBs with LLMs: Mixed Relat"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           28,
           "marks. The challenge is therefore how we can automatically find good pipelines for mixed relational-"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           29,
           "on the complexity of the task, simple ML models with a small set of supervised data [123], or larger"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           30,
           "loads [177, 182, 216] separate table processing (e.g., pandas [192]) and LLM inference engine (e.g.,"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           31,
           "oped from an open-source cloud DBMS, utilizing recent implemen- tations and optimizations for proces"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           32,
           "database integrations and be able to find interesting research topics from each of the integration, "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           33,
           "in building intelligent AR/VR assistants. In this tutorial, we will focus on more recent, general ap"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           34,
           "tin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           35,
           "Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masah"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           36,
           "Inference Requests Without Approximations. CoRR abs/2409.17264 (2024). https://doi.org/10.48550/ARXI"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           37,
           "tra, Bhargav S. Gulavani, Alexey Tumanov, and Ramachandran Ramjee. 2024. Taming Throughput-Latency T"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           38,
           "[6] Minseon Ahn, Thomas Willhalm, Norman May, Donghun Lee, Suprasad Mutalik Desai, Daniel Booss, Jun"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           39,
           "10.18653/V1/2023.EMNLP-MAIN.298 [8] Peter Akioyamen, Zixuan Yi, and Ryan Marcus. 2024. The Unreasona"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           40,
           "2024. Self-RAG: Learning to Retrieve, Generate, and Critique through Self- Reflection. In The Twelft"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           41,
           "arXiv:2408.16947 [15] Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long-Doc"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           42,
           "Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applica- tions of Artifici"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           43,
           "14295 arXiv:2401.14295 [20] Luca Beurer-Kellner, Marc Fischer, and Martin T. Vechev. 2024. Guiding L"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           44,
           "Unifying AI and Databases with TAG. CoRR abs/2408.14717 (2024). https: //doi.org/10.48550/ARXIV.2408"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           45,
           "https://doi.org/10.1145/3604437.3604452 [25] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Tre"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           46,
           "PMLR, 2206–2240. https://proceedings.mlr.press/v162/borgeaud22a.html [26] William Brandon, Mayank Mi"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           47,
           "DBSP: Automatic Incremental View Maintenance for Rich Query Languages. Proc. VLDB Endow. 16, 7 (2023"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           48,
           "Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Che"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           49,
           "(Eds.). ACM, 13–20. https://doi.org/10.1145/3555041.3589406 [32] Andong Chen, Lianzhang Lou, Kehai C"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           50,
           "https://openreview.net/forum?id=5nBqY1y96B [34] Banghao Chen, Zhaofeng Zhang, Nicolas Langrené, and "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           51,
           "TACL_A_00542 [37] Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Za"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           52,
           "arXiv:2305.05176 [40] Wenhu Chen. 2023. Large Language Models are few(1)-shot Table Reasoners. In Fi"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           53,
           "https://doi.org/10.18653/V1/2022.EMNLP-MAIN.375 [42] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and Wi"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           54,
           "Cost for Large Language Models. In Findings of the Association for Computational Linguistics, ACL 20"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           55,
           "Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. 2023. Univer- sal Self-Consistency for"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           56,
           "Symphony: Towards Natural Language Query Answering over Multi-modal Data Lakes. In 13th Conference o"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           57,
           "10.48550/ARXIV.2407.10855 arXiv:2407.10855 [51] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           58,
           "https://proceedings.neurips.cc/paper/2017/hash/ d5e2c0adad503c91f91df240d0cd4e49-Abstract.html [53] "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           59,
           "Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, 1419–1436. https://acla"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           60,
           "ACM, 719–729. https://doi.org/10.1145/3626772.3657834 [57] Josef Dai, Xuehai Pan, Ruiyang Sun, Jiami"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           61,
           "https://doi.org/10.18653/V1/P19-1285 [59] Tri Dao. 2024. FlashAttention-2: Faster Attention with Bet"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           62,
           "67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html [61] Tri Dao and Albert Gu. 2024. Transfor"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           63,
           "Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Lin- guistics, 18476–18499. ht"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           64,
           "Jiajun Chen, and Shujian Huang. 2024. Hallu-PI: Evaluating Hallucination in Multi-modal Large Langua"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           65,
           "November, 2021, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). As"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           66,
           "Ambuj K. Singh, Yizhou Sun, Leman Akoglu, Dimitrios Gunopulos, Xifeng Yan, Ravi Kumar, Fatma Ozcan, "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           67,
           "14-19, 2020, David Maier, Rachel Pottinger, AnHai Doan, Wang-Chiew Tan, Abdussalam Alawini, and Hung"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           68,
           "of Machine Learning Research, Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepes"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           69,
           "Advanced RAG Output Grading. CoRR abs/2404.01037 (2024). https://doi.org/ 10.48550/ARXIV.2404.01037 "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           70,
           "Barcelona, Spain, August 25-29, 2024, Ricardo Baeza-Yates and Francesco Bonchi (Eds.). ACM, 6491–650"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           71,
           "Augmenting Deep Learning Through Symbolic Reasoning. CoRR abs/2410.22077 (2024). https://doi.org/10."
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           72,
           "[82] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Sco"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           73,
           "Inference for Large Language Models. CoRR abs/2401.14351 (2024). https: //doi.org/10.48550/ARXIV.240"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           74,
           "[88] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phan"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           75,
           "1777. https://doi.org/10.18653/V1/2023.ACL-LONG.99 [90] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           76,
           "2024. Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language Models without Logit A"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           77,
           "[94] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney, and Kurt Keutzer. 2021. A "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           78,
           "https://doi.org/10.18653/V1/2023.FINDINGS-ACL.348 [97] Ruihao Gong, Yang Yong, Shiqiao Gu, Yushi Hua"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           79,
           "Augmented Tabular Deep Learning. CoRR abs/2307.14338 (2023). https://doi. org/10.48550/ARXIV.2307.14"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           80,
           "in Large Multilingual Translation Models. Trans. Assoc. Comput. Linguistics 11 (2023), 1500–1517. ht"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           81,
           "Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Sal"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           82,
           "https://www.ijcai.org/ proceedings/2024/890 [105] Aaron Harlap, Deepak Narayanan, Amar Phanishayee, "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           83,
           "and Yunhe Wang. 2024. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Larg"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           84,
           "Scaling Laws for Transfer. CoRR abs/2102.01293 (2021). arXiv:2102.01293 https://arxiv.org/abs/2102.0"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           85,
           "//doi.org/10.48550/ARXIV.2203.15556 arXiv:2203.15556 [114] Ari Holtzman, Jan Buys, Li Du, Maxwell Fo"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           86,
           "de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-Efficient Transf"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           87,
           "August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computatio"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           88,
           "https://openreview.net/forum?id=nZeVKeeFYf9 [120] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan F"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           89,
           "Vassilvitskii, and Sanmi Koyejo. 2024. Scaling Laws for Downstream Task Performance of Large Languag"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           90,
           "Models through Question Complexity. In Proceedings of the 2024 Conference of the North American Chap"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           91,
           "2023. Towards Mitigating Hallucination in Large Language Models via Self- Reflection. CoRR abs/2310."
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           92,
           "arXiv preprint arXiv:2411.01142 (2024). [128] Deokhyung Kang, Baikjin Jung, Yunsu Kim, and Gary Geun"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           93,
           "[130] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott G"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           94,
           "Deep Learning Approaches for Text-to-SQL Systems. In SIGMOD ’21: Interna- tional Conference on Manag"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           95,
           "[135] Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. 2024. Bri"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           96,
           "tions, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https: //openreview.net/forum?id="
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           97,
           "arXiv:2403.19270 [139] Kyoungmin Kim, Kijae Hong, Caglar Gulcehre, and Anastasia Ailamaki. 2024. The"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           98,
           "Merging for Cardinality Estimation. Proc. ACM Manag. Data 2, 1 (2024), 45:1– 45:27. https://doi.org/"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           99,
           "[144] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           100,
           "Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the 29th S"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           101,
           "cidrdb.org/cidr2024/papers/p43-lee.pdf [149] Wonbeom Lee, Jungi Lee, Junghwan Seo, and Jaewoong Sim."
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           102,
           "Zhaoqing Suo, Hongcheng Gao, Wenjing Hu, Pengcheng Yin, et al. 2024. Spider 2.0: Evaluating language"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           103,
           "Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA (Proceedings of Ma- chine Learning Resea"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           104,
           "https://proceedings.neurips.cc/paper/2020/hash/ 6b493230205f780e1bc26945df7481e5-Abstract.html [155]"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           105,
           "https://doi.org/10.1145/3448016. 3457542 [157] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, M"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           106,
           "Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander To- shev, Stephanie Wang, Dirk "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           107,
           "in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Syst"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           108,
           "GPT: Table Fine-tuned GPT for Diverse Table Tasks. Proc. ACM Manag. Data 2, 3 (2024), 176. https://d"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           109,
           "Papers), Virtual Event, August 1-6, 2021, Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (E"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           110,
           "Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           111,
           "Track, Miami, Florida, USA, November 12-16, 2024, Franck Dernoncourt, Daniel Preotiuc-Pietro, and An"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           112,
           "[169] Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Sihang Zhou, Xinwang Liu, "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           113,
           "Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           114,
           "with Semantic Variable. In 18th USENIX Symposium on Operating Systems Design and Implementation, OSD"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           115,
           "Zhuang. 2024. MiniCache: KV Cache Compression in Depth Dimension for Large Language Models. CoRR abs"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           116,
           "LLM Inference via Vector Retrieval. CoRR abs/2409.10516 (2024). https://doi. org/10.48550/ARXIV.2409"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           117,
           "2409.14317 arXiv:2409.14317 [181] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele B"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           118,
           "https://doi.org/10.48550/ARXIV.2408.16967 arXiv:2408.16967 [184] Yuhan Liu, Hanchen Li, Yihua Cheng,"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           119,
           "for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           120,
           "06292 arXiv:2408.06292 [188] Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Fel"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           121,
           "Open-domain Question Answering via Chain of Reasoning over Heterogeneous Knowledge. In Findings of t"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           122,
           "Proceedings of the 9th Python in Science Conference, Stéfan van der Walt and Jarrod Millman (Eds.). "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           123,
           "ACM, 547–555. https://doi.org/10.1145/3626246.3654683 [195] Abhika Mishra, Akari Asai, Vidhisha Bala"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           124,
           "and Tarik Borogovac. 2024. Meta Knowledge for Retrieval Augmented Large Language Models. CoRR abs/24"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           125,
           "09906 arXiv:2402.09906 [200] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, M"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           126,
           "mian Hu, Jian Chen, Mihir Parmar, Sasidhar Kunapuli, Joe Barrow, Junda Wu, Ashish Singh, Yu Wang, Ji"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           127,
           "Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab E"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           128,
           "Induced Predicates Through Joins in Big-Data Clusters. Proc. VLDB Endow. 13, 3 (2019), 252–265. http"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           129,
           "Advances in Neural Information Processing Systems 35: Annual Conference on Neu- ral Information Proc"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           130,
           "[210] James Jie Pan, Jianguo Wang, and Guoliang Li. 2024. Survey of vector database management syste"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           131,
           "Luo, Xiaolin Wang, and Jie Zhang. 2024. InstInfer: In-Storage Attention Of- floading for Cost-Effect"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           132,
           "Large Language Model Cost Analysis. https://www.semianalysis.com/p/the- inference-cost-of-search-dis"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           133,
           "Large Language Model Connected with Massive APIs. CoRR abs/2305.15334 (2023). https://doi.org/10.485"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           134,
           "Gurevych. 2021. AdapterFusion: Non-Destructive Task Composition for Transfer Learning. In Proceeding"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           135,
           "16ccd203e9e3696a7ab0dcf568316379-Abstract-Conference.html [224] Mohammadreza Pourreza, Hailong Li, R"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           136,
           "Solvers. CoRR abs/2408.06195 (2024). https://doi.org/10.48550/ARXIV.2408. 06195 arXiv:2408.06195 [22"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           137,
           "2410.12247 arXiv:2410.12247 [229] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           138,
           "[231] Haoran Qiu, Weichao Mao, Archit Patke, Shengkun Cui, Saurabh Jha, Chen Wang, Hubertus Franke, "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           139,
           "1930–1940. https://doi.org/10.1145/3627673.3679847 [233] Ernesto Quevedo, Jorge Yero, Rachel Koerner"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           140,
           "and Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/ a85b405ed65c6477a4fe830"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           141,
           "2024. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling. arX"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           142,
           "and Aman Chadha. 2024. A Systematic Survey of Prompt Engineering in Large Language Models: Technique"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           143,
           "[242] Viktor Sanca and Anastasia Ailamaki. 2024. Efficient Data Access Paths for Mixed Vector-Relati"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           144,
           "Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andre"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           145,
           "10.1145/3642970.3655844 [245] Bhaskarjit Sarmah, Dhagash Mehta, Benika Hall, Rohan Rao, Sunil Patel,"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           146,
           "hash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html [247] Sander Schulhoff, Michael Ilie,"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           147,
           "https://doi.org/10.48550/ARXIV.2406.06608 arXiv:2406.06608 [248] Robert Schulze, Tom Schreiber, Ilya"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           148,
           "Learning from Preference Human Feedback. CoRR abs/2405.14655 (2024). https://doi.org/10.48550/ARXIV."
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           149,
           "Gavrilovska and Douglas B. Terry (Eds.). USENIX Association, 965–988. https: //www.usenix.org/confer"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           150,
           "Verma, and Yongjoo Park. 2023. A Step Toward Deep Online Aggregation. Proc. ACM Manag. Data 1, 2 (20"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           151,
           "forcement learning. In Advances in Neural Information Processing Sys- tems 36: Annual Conference on "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           152,
           "dan L. Boyd-Graber, and Lijuan Wang. 2023. Prompting GPT-3 To Be Reliable. In The Eleventh Internati"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           153,
           "Large Language Model Serving with a Consumer-grade GPU. In Proceedings of the ACM SIGOPS 30th Sympos"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           154,
           "1f89885d556929e98d3ef9b86448f951-Abstract.html [263] Foteini Strati, Sara McAllister, Amar Phanishay"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           155,
           "18653/V1/2024.FINDINGS-ACL.854 [265] Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           156,
           "https://doi.org/10.48550/ARXIV.2407.00326 arXiv:2407.00326 [268] James Thorne, Majid Yazdani, Marzie"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           157,
           "Rani, Vipula Rawte, Aman Chadha, and Amitava Das. 2024. A Comprehen- sive Survey of Hallucination Mi"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           158,
           "[273] Matthias Urban and Carsten Binnig. 2024. CAESURA: Language Models as Multi-Modal Query Planner"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           159,
           "https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html [275]"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           160,
           "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language M"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           161,
           "[280] Wenyi Wang, Hisham A Alyahya, Dylan R Ashley, Oleg Serikov, Dmitrii Khizbullin, Francesco Facc"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           162,
           "[282] Xiaochen Wang, Junqing He, Liang Chen, Reza Haf Zhe Yang, Yiru Wang, Xiangdi Meng, Kunhao Pan,"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           163,
           "10200 arXiv:2402.10200 [285] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           164,
           "arXiv:2403.15452 [287] Zheng Wang, Shu Xian Teo, Jieer Ouyang, Yongjun Xu, and Wei Shi. 2024. M-RAG:"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           165,
           "https://openreview.net/forum?id=gEZrGCozdqR [289] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten B"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           166,
           "10.14778/3415478.3415562 [291] Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang, Xuanzhe Liu, and X"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           167,
           "Francisco, CA, USA, June 26 - July 01, 2016, Fatma Özcan, Georgia Koutrika, and Sam Madden (Eds.). A"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           168,
           "Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 38087–38099. https://proceedings.mlr.p"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           169,
           "Shixuan Sun, Changxu Shao, Yuhong Guo, Junping Zhao, Ke Zhang, Minyi Guo, and Jingwen Leng. 2024. vT"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           170,
           "[300] Qiancheng Xu, Yongqi Li, Heming Xia, and Wenjie Li. 2024. Enhancing Tool Retrieval with Iterat"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           171,
           "[303] Yuming Xu, Hengyu Liang, Jin Li, Shuotao Xu, Qi Chen, Qianxi Zhang, Cheng Li, Ziyue Yang, Fan "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           172,
           "evitable: An Innate Limitation of Large Language Models. CoRR abs/2401.11817 (2024). https://doi.org"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           173,
           "Paranjpye, Reynold Xin, and Matei Zaharia. 2024. Adaptive and Robust Query Execution for Lakehouses "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           174,
           "rav Shah, Rakesh Wanga, Anuj Kumar, Wen-tau Yih, and Xin Luna Dong. 2024. CRAG - Comprehensive RAG B"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           175,
           "[312] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasim"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           176,
           "Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. https://openreview.net/fo"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           177,
           "2024. Differential transformer. arXiv preprint arXiv:2410.05258 (2024). [316] Zihao Ye, Ruihang Lai,"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           178,
           "Aguilera and Hakim Weatherspoon (Eds.). USENIX Association, 521–538. https: //www.usenix.org/confere"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           179,
           "Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (Eds.). ELRA and ICCL, 5365–5375. https://aclantho"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           180,
           "ARXIV.2402.16363 arXiv:2402.16363 [323] Matei Zaharia, Omar Khattab, Lingjiao Chen, Jared Quincy Dav"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           181,
           "LLM Cascade with Multi-Objective Optimal Consideration. CoRR abs/2410.08014 (2024). https://doi.org/"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           182,
           "Geambasu and Ed Nightingale (Eds.). USENIX Association, 377–395. https: //www.usenix.org/conference/"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           183,
           "Vladimir Meza Ruíz (Eds.). Association for Computational Linguistics, 30–37. https://doi.org/10.1865"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           184,
           "EMNLP 2024, Miami, FL, USA, November 12-16, 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           185,
           "[334] Yilong Zhao, Shuo Yang, Kan Zhu, Lianmin Zheng, Baris Kasikci, Yang Zhou, Jiarong Xing, and Io"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           186,
           "Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing. In Ada"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           187,
           "wei Wang. 2024. DPO Meets PPO: Reinforced Token Optimization for RLHF. CoRR abs/2404.18922 (2024). h"
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           188,
           "arXiv:2308.05481 [341] Yue Zhou, Chenlu Guo, Xu Wang, Yi Chang, and Yuan Wu. 2024. A Survey on Data "
          ],
          [
           "Trustworthy and Efficient LLMs Meet Databases",
           189,
           "Gao, Qinyu Xu, Tian Tang, Zihao Ye, et al. 2024. NanoFlow: Towards Optimal Large Language Model Serv"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           0,
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications Irene Weber"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           1,
           "architecture, we examine each LLM component separately. We identify thirteen dimensions along which "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           2,
           "and societal consequences of these systems [36, 39]. Meanwhile, research investigates Artificial Gen"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           3,
           "cations”, “LLM-integrated systems”, “LLM-based ap- plications”, etc. [32, 13, 57]. LLMs are versatil"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           4,
           "to leverage LLMs, often integrating LLMs in mul- tiple ways for distinct purposes. In developing the"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           5,
           "for categorizing the integration of LLMs in applica- tions from an architectural perspective. To be "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           6,
           "consisting of neurons, i.e., very simple processing 2 units, that are organized in layers and connec"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           7,
           "only learns to produce correct language but also ab- sorbs and stores information and factual knowle"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           8,
           "As effective prompts are crucial for unlocking the di- verse capabilities of an LLM, the discipline "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           9,
           "The study develops a taxonomy for LLM components. LLM-integrated applications are described as combi"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           10,
           "tion domains and the tasks they solve in [20]. Most closely related to the taxonomy developed here i"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           11,
           "external systems. This approach proved unsuccessful. The second version was based on the classical t"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           12,
           "cause they effectively position categorized instances within the design space. However, we found tha"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           13,
           "sources, and education. 4 The search revealed a predominance of theoretical re- search on LLM-integr"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           14,
           "tified ten LLM components. 4.2. Sample of LLM-integrated applications Table 1 gives an overview of t"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           15,
           "[5]. In the dialogue section, users converse with an LLM to complete the complex task based on the d"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           16,
           "using the LLM for application control, LowCode Planning actually serves as a prompt generator that s"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           17,
           "TaskPlanning AutoDroid [64] TaskExecutor, MemoryGenerator ProgPrompt [51] ActionPlanning, ScenarioFe"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           18,
           "innovative applications of an LLM in a technical do- main. We categorize three LLM components solvin"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           19,
           "and schedule the automation modules’ skills, and (b) program the involved automation modules such th"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           20,
           "turing a desk and several objects. It has previously been trained to execute basic operations expres"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           21,
           "lated back to a sequence of basic operations in nat- ural language. When the robot executes these op"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           22,
           "ical User Interface (GUI) along with the user’s com- plex order. Before executing irrevocable operat"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           23,
           "desired as output, its prompt includes an HTML rep- resentation of the GUI state, the GUI actions pr"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           24,
           "out considering the current state of the environment. To establish a feedback loop with the environm"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           25,
           "main knowledge is provided as a part of the prompt. SgpTod. SgpTod employs an LLM to implement a cha"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           26,
           "from the user input (e.g., the town and the date of the hotel booking). The TOD system may require s"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           27,
           "system’s belief state. It outputs is an SQL query suited to extract the database entries that match "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           28,
           "their products [44]. These copilots not only provide textual guidance but also perform actions withi"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           29,
           "to display more buttons, e.g., one that generates a pivot chart from the selected data. ExcelCopilot"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           30,
           "• When a user inputs a free-text command, the IntentDetector is invoked to determine and trigger a s"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           31,
           "ties. The LLM-integrated application can then be described as a combination of the LLM components it"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           32,
           "enforced Consumer User, LLM, Program, Engine enforced 5.1. Overview and demonstration The taxonomy i"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           33,
           "are represented by a set of binary features. The re- maining dimensions are encoded as n-valued feat"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           34,
           "the LLM automatically. E.g., users do not interact 10 Invocation Function Prompt Skills Out. Format "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           35,
           "C S A P P P I F U MatrixProduction Manager C S C I P P U I P F S L MatrixProduction Operator A S C P"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           36,
           "S C O P P P R F I P TruckPlatoon A S A O P P P W F U ExcelCopilot ActionExecutor∗ A S A P P L P F C "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           37,
           "repeatedly prompt LowCode Executing or My- CrunchGpt DesignAssistant in multi-turn dia- logues to ob"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           38,
           "implements the UI. On the input side, it allows users to enter data and commands that control the ap"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           39,
           "LLM. E.g., in TruckPlatoon, the output gener- ated by the LLM component can replace a data cock- pit"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           40,
           "Control: The output of the LLM is used for con- trolling the application. E.g., the plans generated "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           41,
           "prompts to reliably elicit output in the requested 12 form [68]. While a broad range of prompt patte"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           42,
           "taxonomy are underlined. The taxonomy distinguishes three prompt parts re- ferred to as Prompt Instr"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           43,
           "Prompt ActionPlanning has no State prompt, nor does LowCode Planning (except the dialogue history wh"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           44,
           "not discriminating and categorizes all cases as Pro- gram. It is retained in the taxonomy for comple"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           45,
           "[45] prompt template + examples DB schema user input question [45] examples SQL query result [37] pr"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           46,
           "CrunchGpt SettingsEditor replaces values in JSON files; TruckPlatoon converts measurements into text"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           47,
           "in formulas; AutoDroid MemoryGenerators ex- plain the effects of GUI elements in Android apps. Plan:"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           48,
           "skills is necessary to make categorizations distinctive and expressive. 5.2.5. Output-related dimens"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           49,
           "an Android app; MyCrunchGpt SettingsEditor outputs JSON. Structure: structured, formalized output ad"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           50,
           "Most sample applications do not check or revise the LLM’s output, though several of them parse and t"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           51,
           "tem) which use it to compute prompts for other LLM components. Engine covers scenarios where the LLM"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           52,
           "Check and Output Revision which are not counted due to insufficient system documentation. We evaluat"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           53,
           "the taxonomy’s ability to illustrate design options and inspire novel uses for LLM integrations in a"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           54,
           "and ease of use, are discussed in section 6. We visualize the taxonomy (or, strictly speaking, cat- "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           55,
           "State Task Check z }| { z }| { Revision Consumer A C D I S C A I O B R W B U L P U L P U L P U L P W"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           56,
           "clarity and comprehensiveness of the available infor- mation, which varies across analyzed systems. "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           57,
           "label and a chatbot response), and similarly by Ma- trixProduction, as detailed section 4.2. Draw- i"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           58,
           "6.2. Usefulness The search for instances of LLM-integrated appli- cations uncovered activities acros"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           59,
           "plications commonly explain background information and details of the application domain in addition"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           60,
           "ing from straightforward solutions (e.g., TruckPla- toon) to highly sophisticated and technically co"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           61,
           "LLM components interact through prompt chaining, where one LLM component’s output feeds into an- oth"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           62,
           "tions are developed and their architectures disclosed in the future. Extensions to the taxonomy coul"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           63,
           "ExcelCopilot N/A combined LLMs in Copilot for Microsoft 365 [43] 7. Conclusion This paper investigat"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           64,
           "sualization based on feature vectors, which is more compact than the established visualizations such"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           65,
           "and implementation options. As the examples show, LLM components can re- place traditionally coded f"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           66,
           "tiple LLMs, the table displays the chosen or best- performing LLM. Although not representative, the "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           67,
           "modify a model to enhance its security, potentially impacting applications that rely on it. Despite "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           68,
           "tasks. The taxonomy developed in this study can sys- tematize such experiments and their outcomes. A"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           69,
           "dustrie 4.0 (version 3.0 rc02). Working Paper, Berlin: Federal Ministry for Economic Affairs 20 and "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           70,
           "Low-code LLM: Visual Pro- gramming over LLMs. (arXiv:2304.08103), April 2023. doi:10.48550/arXiv.230"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           71,
           "Chen, Zhaofeng Zhang, Nicolas Langrené, and Shengxin Zhu. Unleash- ing the potential of prompt engin"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           72,
           "Xiuqiang He. Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Pros"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           73,
           "and Jie M. Zhang. Large Language Models for Software Engineering: Survey and Open Problems. (arXiv:2"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           74,
           "mation systems. MIS quarterly, pages 611–642, 2006. doi:10.2307/25148742. [19] Yanchu Guan, Dong Wan"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           75,
           "[21] Thorsten Händler. A Taxonomy for Au- tonomous LLM-Powered Multi-Agent Architec- tures:. In Proc"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           76,
           "Shafiq Joty, David Schlangen, Ondrej Dusek, Casey Kennington, and Malihe Alikhani, edi- tors, Procee"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           77,
           "Moubarak, Beatriz Mouriño, Brenden Pelkie, Michael Pieler, Mayk Caldas Ramos, Bojana Ranković, Samue"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           78,
           "tions of Large Language Models, July 2023. doi:10.48550/arXiv.2307.10169. [26] Samuel Kernan Freire,"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           79,
           "22 adakis. MyCrunchGPT: A LLM Assisted Frame- work for Scientific Machine Learning. Jour- nal of Mac"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           80,
           "and Naoaki Okazaki, editors, Findings of the as- sociation for computational linguistics: ACL 2023, "
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           81,
           "[33] Yuchen Liu, Luigi Palmieri, Sebastian Koch, Ilche Georgievski, and Marco Aiello. DELTA: Decompo"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           82,
           "[36] Scott McLean, Gemma J. M. Read, Jason Thompson, Chris Baber, Neville A. Stanton, and Paul M. Sa"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           83,
           "Schick, Jane Dwivedi-Yu, Asli Ce- likyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. Augmente"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           84,
           "Gen- erative AI and ChatGPT: Applications, chal- lenges, and AI-human collaboration. Jour- 23 nal of"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           85,
           "Rahul Pan- dita, Sumit Gulwani, Jessica Rich, and Austin Z. Henley. Building Your Own Prod- uct Copi"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           86,
           "1557-928X. doi:10.2753/MIS0742-1222240302. [47] Mohaimenul Azam Khan Raiaan, Md. Sad- dam Hossain Mu"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           87,
           "ture Notes in Business Information Process- ing, pages 145–157, Cham, 2023. Springer Na- ture Switze"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           88,
           "Robot Task Plans using Large Language Mod- els. In 2023 IEEE International Conference on Robotics an"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           89,
           "[54] Daniel Szopinski, Thorsten Schoormann, and Dennis Kundisch. Criteria as a Prelude for Guid- ing"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           90,
           "ating Large Language Model Applications Uti- lizing Langchain: A Primer on Developing LLM Apps Fast."
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           91,
           "Effective Invocation Methods of Massive LLM Services. (arXiv:2402.03408), February 2024. doi:10.4855"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           92,
           "Zhang, Ying Nian Wu, Song-Chun Zhu, and Hangxin Liu. LLM3:Large Language Model- based Task and Motio"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           93,
           "Elnashar, Jesse Spencer-Smith, and Dou- glas C. Schmidt. A Prompt Pattern Cat- alog to Enhance Promp"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           94,
           "bility. (arXiv:2402.18667), February 2024. doi:10.48550/arXiv.2402.18667. [69] Yuchen Xia, Manthan S"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           95,
           "Zhou, and Helen Meng. SGP-TOD: Build- ing Task Bots Effortlessly via Schema-Guided LLM Prompting. (a"
          ],
          [
           "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
           96,
           "Denny Zhou, and Le Hou. Instruction- Following Evaluation for Large Language Mod- els. (arXiv:2311.0"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           0,
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable Chaofan Lin1∗, Zhenhua Ha"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           1,
           "tions. Parrot proposes Semantic Variable, a unified abstrac- tion to expose application-level knowle"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           2,
           "Teams or Google Meet can summarize meeting discussions through LLMs [33]. Search engines like Google"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           3,
           "collaborate to achieve a task [22,47,54]. Public LLM service providers have to face diverse tenants "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           4,
           "Final  Summary (a) Map-Reduce Summary Chunk 1 Chunk 2 …… Chunk N LLM S1 + LLM S1 + S2 + …… SN-1 LLM "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           5,
           "Internet, can only issue the second request after they receive the result of the first request. This"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           6,
           "of LLM applications. The long system prompt is usually static and common for all users. As existing "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           7,
           "mization. A Semantic Variable is a text region in the prompt with a specific semantic purpose, such "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           8,
           "or 12× higher throughput compared with the state-of-the-art solutions. 2 Background LLM Service. Mos"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           9,
           "results for each chunk (the Map task), and combines them altogether (a Reduce task) or incrementally"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           10,
           "Figure 3: The end-to-end latency breakdown of current LLM services. The source of the overhead comes"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           11,
           "strated in Figure 1, LLM applications frequently make multi- ple LLM calls to complete a single task"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           12,
           "of the dependency among such requests, where the output of the previous request may be the direct in"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           13,
           "over, the next LLM request has to suffer extra queuing delays ( 4 ), because requests from other app"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           14,
           "MetaGPT [22] 14 17k 72% AutoGen [54] 17 57k 99% ∗We count a paragraph as repeated if it appears in a"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           15,
           "User Input (dynamic) + + Figure 5: The prompt structure of Bing Copilot shows a long prompt reused b"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           16,
           "throughput but lead to 95% higher latency [9]. Yet, if we un- derstand the application-level perform"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           17,
           "tomize a ChatGPT for a specific purpose whose prompt tem- plate is the same across users. The common"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           18,
           "across LLM requests for various users (Table 1). Such com- monality also exists in multi-agent appli"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           19,
           "vides a natural way of programming LLM applications with Semantic Variable annotations (§4.1), which"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           20,
           "task: P.SemanticVariable, code: P.SemanticVariable): \"\"\" You are an experienced QA engineer. You wri"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           21,
           "and the test code to be developed by the QA engineer, re- spectively. Although existing LLM orchestr"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           22,
           "SemanticFunction will trigger the submit API to submit a LLM request with its prompt and input Seman"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           23,
           "teria, showing the end-to-end performance requirement of an application, which can be end-to-end lat"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           24,
           "are submitted beforehand, Parrot can receive them all at once and analyze their correlations just-in"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           25,
           "example, the prompt of WritePythonCode has two potential sharing prefix: the text before {{input:tas"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           26,
           "The value of a Semantic Variable in a request may require transformation before being exchanged, e.g"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           27,
           "erence is usually beneficial for offline data processing, such as bulk document analysis. 1 3 5 4 6 "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           28,
           "latency of the entire task group, often leading to a higher batch capacity for higher throughput of "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           29,
           "where each entry maps a (hashed) prefix of tokens to a list of requests, thus the scheduler can quic"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           30,
           "6 else if SharedReqsInQueue ̸= ∅then 7 r∗= FindEngine(SharedReqsInQueue); 8 else if CtxInEngine ̸= ∅"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           31,
           "bound, with latency influenced by the count of concurrent tokens within the engine. To meet performa"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           32,
           "rot’s context fork to reduce the redundant computation and GPU memory transactions. For an LLM reque"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           33,
           "applications are trusted or there is a trusted zone to execute these functions, Parrot’s APIs can be"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           34,
           "revisited in the LLM service system by considering the new characteristics of LLM applications. In t"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           35,
           "the template (using Semantic Variable in Parrot) need to be wrapped as a SemanticFunction so the nec"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           36,
           "and 1,600 lines of CUDA. We have implemented OPT [60] and LLaMA [51] with PyTorch [45] and Transform"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           37,
           "also supports other APIs for setting and fetching the value of Semantic Variables. The error message"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           38,
           "tions between the L2 Cache and Shared Memory. The kernel initially calculates interim attention metr"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           39,
           ",→ def FreeContext(context_id: int) These three methods not only cover the basic completion function"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           40,
           "more parallel execution opportunities [2,21,46,64]. 8 Evaluation 8.1 Experimental Setup Testbed. We "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           41,
           "applications, we build a multi-agent programming application using MetaGPT [22], which contains a sy"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           42,
           "Scheduling Data Analytics ✓ ✓ ✓ Serving Popular LLM Applications ✓ ✓ Multi-agent App. ✓ ✓ ✓ ✓ Mixed "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           43,
           "butions. comparisons are developed using LangChain [8], which is the predominant framework for LLM a"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           44,
           "memory bandwidth, the per-token generation latency of an engine is mainly affected by the number of "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           45,
           "25 50 75 100 Output Length (# tokens) 0 50 100 150 200 250 Average Latency (s) 1.38x 1.21x 1.14x 1.1"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           46,
           "ment has over 20,000 tokens. The results measures the mean end-to-end latency across all documents. "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           47,
           "substantially more time-consuming than prompt processing, we observe a consistent speedup with varia"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           48,
           "250 Average Latency (s) 1.21x 1.19x 1.31x 1.79x 2.38x Parrot Baseline (vLLM) (a) With background req"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           49,
           "the subsequent chunk is processed immediately by incorporat- ing the summaries of previous chunks in"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           50,
           "reduce paradigm as depicted in Figure 1a. This approach consists of multiple parallel mapping LLM re"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           51,
           "concurrently by both Parrot and the baseline. The primary ad- vantage of Parrot stems from its deduc"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           52,
           "anism and Parrot’s scheduling policy that co-locates LLM requests sharing a long prompt prefix. Beca"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           53,
           "1.1x 1.3x 1.4x 1.7x 1.8x 2.4x x x Parrot Baseline w/ Sharing Baseline w/o Sharing Figure 15: Latency"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           54,
           "Parrot and vLLM use the paged memory management [25], thus both systems can hold the same number of "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           55,
           "a multi-GPU cluster. Four A6000 (48GB) GPUs are deployed with four LLM engines (LLaMA 7B). We select"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           56,
           "cluster throughput. After turning off such affinity scheduling policy, Parrot only exhibits 3× highe"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           57,
           "illustrates the latency and memory consumption of Parrot compared to baseline systems on one A100 ru"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           58,
           "1500 Average Latency (s) 1.00x 1.04x 1.14x 1.16x 1.22x 1.61x 1.88x 2.35x 1.27x 1.58x 2.03x 2.45x 7.1"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           59,
           "on by sharing this common context as a prompt prefix. The static prefix sharing mechanism from vLLM "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           60,
           "ously analyzed in §8.2. Requests from the chat applications are characterized by their need for low "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           61,
           "latency (measured as request latency per number of output tokens) [25, 56] for chat applications in "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           62,
           "clude Clipper [10], TensorFlow Serving [39], Clockwork [19], REEF [20], AlpaServe [28], which have e"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           63,
           "space with the final goal of optimizing the end-to-end perfor- mance of applications, rather than in"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           64,
           "or DAGs (Directed Acyclic Graphs) widely exist in many kinds of systems, and many optimizations have"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           65,
           "first-class citizens and targets to optimize the end-to-end per- formance of LLM applications, inste"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           66,
           "and Xiaoqiang Zheng. TensorFlow: A system for Large- Scale machine learning. In 12th USENIX Symposiu"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           67,
           "2019. [5] Apache. Kafka. https://kafka.apache.org/, Octo- ber 2023. [6] Zhengda Bian, Hongxin Liu, B"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           68,
           "transformer-batching/, May 2023. [10] Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J. Franklin, "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           69,
           "[13] Bill Gates. Ai is about to completely change how you use computers and upend the software indus"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           70,
           "ciation. [17] Juncheng Gu, Mosharaf Chowdhury, Kang G. Shin, Yibo Zhu, Myeongjae Jeon, Junjie Qian, "
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           71,
           "Chen. Microsecond-scale preemption for concurrent GPU-accelerated DNN inferences. In 16th USENIX Sym"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           72,
           "[23] Michael Isard, Mihai Budiu, Yuan Yu, Andrew Birrell, and Dennis Fetterly. Dryad: Distributed da"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           73,
           "agement for large language model serving with page- dattention. In Proceedings of the 29th Symposium"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           74,
           "paServe: Statistical multiplexing with model parallelism for deep learning serving. In 17th USENIX S"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           75,
           "mentation (OSDI 22), pages 303–320, Carlsbad, CA, July 2022. USENIX Association. [32] Microsoft. Bin"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           76,
           "on Operating Systems Design and Implementation (OSDI 20), pages 481–498. USENIX Association, Novembe"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           77,
           "Production best practices - ope- nai api. https://platform.openai.com/ docs/guides/production-best-p"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           78,
           "[48] Sebastián Ramírez. FastAPI. https://github.com/ tiangolo/fastapi. [49] Mohammad Shoeybi, Mostof"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           79,
           "Prompts_Conversations.txt, Nov 2023. [53] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           80,
           "Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv, 2023. [56] Gye"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           81,
           "USENIX Symposium on Operating Systems Design and Implementation (OSDI 08), San Diego, CA, 2008. [59]"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           82,
           "Yuqi Wang, Yifan Xiong, and Bin Wang. HiveD: Sharing a GPU cluster for deep learning with guarantees"
          ],
          [
           "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
           83,
           "Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. Dist- serve: Disaggregating prefill and decoding for "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           0,
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends ZIBIN ZHENG, "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           1,
           "Do Code LLMs outperform general LLMs in software engineering tasks? (3) Which LLMs are more proficie"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           2,
           "languages with their powerful text understanding and generation capabilities [34]. LLMs have also ha"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           3,
           "Permission to make digital or hard copies of all or part of this work for personal or classroom use "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           4,
           "software engineering tasks such as code generation [123], code summarization [2], vulnerability mini"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           5,
           "However, due to the uncertainties brought by fine-tuning, training data, and other factors [82, 119]"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           6,
           "employed a card sorting method [14] to remove duplicate and irrelevant papers and further expanded o"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           7,
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 3 findings or"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           8,
           "assist developers of Code LLMs in making better choices regarding base models and fine-tuning approa"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           9,
           "collection and the criteria for screening; In Section 3, we summarize and categorize the Code LLMs a"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           10,
           "Search Literature  Selection Seven Exclusion  Criteria Data  Analysis masterSKevin1 Literature colle"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           11,
           "364 26 17 705 2484 in journals, conferences, workshops, and numerous preprint papers but also access"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           12,
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 5 may result "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           13,
           "categories from it. There are three types of card sorting methods: open card sorting, closed card so"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           14,
           "We carefully read the articles and actively searched for answers related to the two questions shown "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           15,
           "relationship between them? For example, which LLMs are fine-tuned based on other LLMs? RQ2 Do Code L"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           16,
           "designed specifically for software engineering tasks and show the relationships between them. Accord"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           17,
           "multilayer generative Transformer models. GPT-C underwent retraining on a large-scale unsuper- vised"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           18,
           "involves retraining the model with randomly initialized parameters. The second variant, CodeGPT- ada"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           19,
           "is a 1.3 billion parameter LLM trained primarily on a specially curated “textbook-quality\" synthetic"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           20,
           "in 100 sampling tests. PyCodeGPT [109] is a library-oriented code generation model developed by Micr"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           21,
           "Google: Google has been exploring ways to improve the safety and quality of LLMs’ output. They propo"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           22,
           "multi-query attention. For code-related tasks, researchers collected a Python code dataset called Ex"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           23,
           "the performance of LLMs. Meta: Facebook AI Research has introduced InCoder [26], a unified generativ"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           24,
           "specialization model (Code Llama - Python), and an instruction-following model (Code Llama - Instruc"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           25,
           "data collected from GitHub. The results of the study show that the two-stage training approach helps"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           26,
           "claims that this framework can efficiently enhance large-scale pre-trained language models for code "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           27,
           "code completion tasks. Replit-Code has a scale of 2.7B and is trained on a subset of the Stack Dedup"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           28,
           "ERNIE-Code OpenCoderPlus PolyCoder MultiCoder Phi-1.5 WizardCoder InCoder Code Llama PanGu-Coder2 Co"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           29,
           "1.1B 7B-13B 1B-16B 13B 2.7B 7B 15.5B 15B 1.3B 16B Unknown 350M-6B 7B-34B Unknown 15B 34B 16B/6B 6B 1"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           30,
           "fornia and Columbia University. It is built upon the BART architecture and is pre-trained using deno"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           31,
           "the C programming language. Moreover, it is noteworthy that PolyCoder is open-source, which adds to "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           32,
           "performance improvements across multiple metrics. With just 6 billion parameters, CodeGeeX2 achieves"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           33,
           "all PLs. Performance analysis of MultiCoder is conducted on CodeXGLUE and MultiCC datasets. The resu"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           34,
           "VeriGen outperforms the state-of-the-art commercial GPT-3.5-turbo model in terms of generating Veril"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           35,
           "Zhang et al. [112] focuses on the task of code translation, which involves converting code changes f"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           36,
           "GPT-3, EleutherAI first proposed a 825GB English text corpus called “the Pile\" N72, which includes 9"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           37,
           "Instead, it primarily showcases the impressive performance of GPT-NeoX on mathematical tasks. OpenAI"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           38,
           "Indeed, all model classes, including Codex and GPT-3, have limitations when it comes to infilling, w"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           39,
           "engineering: CodeT5 [100], CodeRL [45], and CodeGen [67]. CodeT5 [100] is a pre-trained encoder-deco"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           40,
           "ness of the generated programs and provide dense feedback signals to the actor. Experimental results"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           41,
           "English text corpus open-sourced by EleutherAI, which includes a substantial amount of GitHub code d"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           42,
           "7B, and 16B, referred to as CodeGen2 [66]. Regarding model architecture, CodeGen2 attempted to unify"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           43,
           "The authors of the article proposed the use of mixed pretraining objectives to alleviate the dis- cr"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           44,
           "approach, as the model effectively performs in both code retrieval and generation tasks. A Survey of"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           45,
           "on the GPT-2 architecture, and users can train both 110M and 1.5B versions of CodeParrot using just "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           46,
           "is similar to SantaCoder, with a parameter size of 15.5B and the utilization of techniques such as F"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           47,
           "and OctoGeeX. Experimental results demonstrate that these two LLMs achieve optimal performance on HU"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           48,
           "OpenChat: OpenCoderPlus [96]is a code-oriented large language model in the OpenChat series, fine-tun"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           49,
           "and CodeExercise-Python-27k. They have also introduced two models: CodeFuse-13B and CodeFuse- CodeLl"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           50,
           "3.4 Individuals & Anonymously-led LLMs CodeAlpaca [13] is an instruction-guided LLaMA [94] model tra"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           51,
           "techniques. 4 DO CODE LLMS REALLY PERFORM BETTER THAN GENERAL LLMS IN SOFTWARE ENGINEERING TASKS? In"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           52,
           "compared to text-davinci-003, the performance difference becomes more apparent. After RLHF, text-dav"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           53,
           "Coder PanGu-Coder2 CodeGenAPI Replit-Code AquilaCode CodeGen2 UniXco der Fig. 4. The relationships b"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           54,
           "and ChatGPT in three code generation benchmark metrics, SCoT prompts, and Pass@k of the benchmarks. "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           55,
           "to varying degrees. Among the six models, GPT-Neo-1.3B performs the best, even surpassing the larger"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           56,
           "snippets in Java and Python code generation tasks for 11 LLMs. It is observed that CodeGen-2B- multi"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           57,
           "current major Code LLMs, CodeT5+ demonstrates better performance in pass@k(%) on HumanEval. However,"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           58,
           "metrics. The article also fine-tunes the ToolCoder model based on CodeGen-2B to better handle API op"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           59,
           "programming languages. The results show that the method based on GPT-3.5-turbo performs better. Howe"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           60,
           "J-6B, and GPT-NeoX-20B in almost all metrics. The article also provides a comparison between PaLM-54"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           61,
           "for the same model. (3) Model size is crucial, as larger parameter models like Code Llama exhibit st"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           62,
           "and 12B. The marginal effect of model parameter size on performance and the sensitivity of different"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           63,
           "models, including the final AlphaCode model, perform significantly worse in HumanEval com- pared to "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           64,
           "Fried et al. [26] evaluated the performance of INCODER-6.7B, as proposed in the article, on the Huma"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           65,
           "MBPP. The experimental results showed that WizardCoder outperformed GPT-3.5 but was inferior to GPT-"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           66,
           "CodeLlama-34B [62] also outperformed GPT-4 in HumanEval (Pass@1). Additionally, MFTCoder [62] publis"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           67,
           "Pan et al. [69] evaluated the performance of LLMs on code translation tasks and found that, except f"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           68,
           "showed that general LLMs perform better on software engineering tasks, while in 20 experiments, Code"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           69,
           "versions of the same model with varying parameter sizes, leading to significant performance differen"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           70,
           "2.7B, 6.1B, and 16.1B exhibit relatively small differences in performance on the MBPP and HumanEval "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           71,
           "Work General LLMs Code LLMs Unknown Task Jiang et al. [38] ✓ Code Generation Maddigan et al. [60] ✓ "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           72,
           "✓ Code Generation Zheng et al. [116] ✓ Code Generation Rozière et al. [78] ✓ Code Generation Fu et a"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           73,
           "✓ Vulnerability Remediation Palacio et al. [68] ✓ Code Syntax Understanding Pan et al. [69] ✓ Code T"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           74,
           "relevant papers and extract the sections that evaluate LLMs in software engineering tasks. Next, we "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           75,
           "HumanEval: HumanEval consists of 164 handwritten Python programming problems. Each problem provides "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           76,
           "GPT-4 and Unnatural-Code-LLaMA-34B is also the best-performing LLM for code generation tasks at the "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           77,
           "Pass@1 Pass@10 Pass@100 LATS(GPT-4 based) 94.4 - - LLaMA2-13B 20.1 34.8 61.2 Reflexion(GPT-4 based) "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           78,
           "73.8 - - AlphaCode-1.1B 17.1 28.2 45.3 WizardCoder-Python-34B 73.2 - - PanGu-Coder-317M 17.07 24.05 "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           79,
           "61.64 79.55 91.75 GPT-NeoX-20B 15.4 25.6 41.2 WizardCoder-15B 57.3 73.2 90.46 InCoder-6.7B 15.2 27.8"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           80,
           "14.2 24.4 38.8 OctoCoder 46.2 - - LaMDA-137B 14.0 - 47.3 Code-LLaMA-Python-13B 43.3 77.4 94.1 Codex-"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           81,
           "CodeT5+-220M 12.0 20.7 31.6 PaLM-2-S 37.6 - 88.4 GPT-J-6B 11.62 15.74 27.74 PaLM-Coder-540B 36 - 88."
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           82,
           "34.6 - - Codex-85M 8.22 12.81 22.40 StarCoder-Python-15B 33.6 - - BLOOM-7.1B 7.73 17.38 29.47 StarCo"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           83,
           "5.59 9.84 17.68 MIM-2.7B 30.7 48.22 69.6 JuPyT5-300M 5.4 15.46 25.60 Replit-Finetuned-2.7B 30.5 - - "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           84,
           "28.36 47.46 75.15 BLOOM-1.7B 4.03 7.45 12.75 CodeT5+-6B 28.0 47.2 69.8 CodeParrot-multi-1.5B 4.0 8.7"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           85,
           "14.96 CodeGen-Mono-2.7B 23.7 36.64 57.01 AlphaCode(dec)-29M 3.4 5.8 11.2 LLaMA-65B 23.7 - 79.3 Codex"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           86,
           "8.58 Replit-2.7B 21.9 - - AlphaCode-13M 1.5 3.6 8.6 LLaMA-33B 21.7 - 70.7 GPT-NEO-350M 0.85 2.55 5.9"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           87,
           "19.22 34.64 55.17 CodeGen-multi-16.1B [67] 18.32 32.07 50.80 Table 6. Percentage of compilable sugge"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           88,
           "1B CodeGen 350M mono CodeGen 350M multi CodeGen 2B mono CodeGen 2B multi PolyCoder 160M PolyCoder 0."
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           89,
           "including TorchDataEval, TorchDataComplexEval, MonkeyEval, and BeatNumEval. Based on the data provid"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           90,
           "and Private Library Benchmark, as shown in Tables 8 and Tables 9. Although the primary focus of Zhan"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           91,
           "pass@1 pass@10 CodeT5-220M 0 0.1 0 0 0 0 PyCodeGPT-11M 18.04 38.61 12.75 37.62 3.80 14.00 CodeGen-35"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           92,
           "27.32 11.25 28.61 10.41 23.50 CodeGen-retrieval-475M 18.30 35.12 9.54 29.02 7.52 16.36 ToolCoder-Onl"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           93,
           "CodeGenAPI-retrieval-475M 3.41 8.33 5.90 11.79 CodeGen-retrieval-475M 2.46 6.35 6.65 13.68 ToolCoder"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           94,
           "54.40% 90.40% 49.17% Llama-2 7.34% 9.02% 0.66% 61.36% 80.13% 49.17% 64.47% 72.93% 47.02% Vicuna- 1.5"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           95,
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 27 Table 11. "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           96,
           "GPT-3.5 29.6% 34.9% 36.0% 50.4% 59.0% 61.1% WizardCoder 12.2% 20.0% 23.0% 35.2% 47.1% 51.1% Instruct"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           97,
           "15.8% 18.4% ChatGLM 1.4% 2.6% 3.0% 8.2% 11.2% 12.4% PolyCoder 1.4% 2.2% 3.0% 13.2% 17.5% 19.6% progr"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           98,
           "WizardCoder-15B also achieves very good results. It’s worth noting that the article’s evaluation of "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           99,
           "on CoderEval. The results indicate that Codex performs better in various testing scenarios. Based on"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           100,
           "9.9 39.9 74.5 89.0 Average 10.3 36.0 73.9 89.1 Variance 1.01 13.61 2.04 0.36 Table 14. Performance c"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           101,
           "64.9 GPT-4 (Few-shot) 76.8 67.4 5.2 Test Case Generation In Schäfer et al. [80], GPT-3.5-turbo, Star"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           102,
           "performance differences also reflect the impact of fine-tuning on programming problems, as Codex is "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           103,
           "Methods Test Files HumanEval ChatGPT 43.1% 81.3% 1,117 130 HumanEval CodeGen 23.8% 33.1% 844 529 Hum"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           104,
           "14.7 37.7 ChatGPT (one sentence) 10.28 14.40 20.81 Table 17. Performance (smoothed BLEU-4) on code s"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           105,
           "20.01 20.31 26.03 19.55 CodeT5+-220M 15.51 16.27 19.60 20.16 20.53 26.78 19.81 CodeT5+-770M 15.63 17"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           106,
           "The details of their performance can be found in Table 17. Furthermore, Wanget al. [99] demonstrates"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           107,
           "CodeGeeX-13B - - - 26.54 43.56 56.48 25.84 41.52 59.72 23.22 47.33 65.87 9.56 23.83 33.56 CodeGeeX-1"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           108,
           "CodeGeeX-13B-FT 62.79 80.39 87.10 - - - 71.68 81.62 85.84 50.83 64.55 74.57 16.71 34.18 52.98 Java I"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           109,
           "49.95 62.82 79.64 18.85 32.92 48.93 JS InCoder-6.7B 23.18 50.47 67.26 35.47 54.48 70.71 30.67 50.90 "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           110,
           "- - CodeGen-Multi-16B 38.32 50.57 68.65 32.95 45.88 59.56 36.55 59.12 78.70 38.93 56.68 70.68 - - - "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           111,
           "while CodeGen-Multi-16B also performs exceptionally well. However, their strengths lie in different "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           112,
           "found that GPT-4 outperforms CodeGeeX significantly in terms of performance. A Survey of Large Langu"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           113,
           "99.0% Total/Average (CodeNet) 81.9% 91.6% 62.7% 18.0% 86.8% 90.7% 98.0% AVATAR 91.9% 98.2% 88.1% 29."
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           114,
           "85.5% 52.7% 96.5% 94.7% 97.9% 5.5 Vulnerability Repair MBPP: MBPP is also one of the important bench"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           115,
           "for pass@80). We can see that the Code-LLaMA series continues to exhibit strong performance, carryin"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           116,
           "combined evaluation datasets, code-davinci-002 performs relatively well. However, the article does n"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           117,
           "- 84.8 StarCoder-Prompted-15.5B 49.5 - - - Code-LLaMA-Python-34B 56.2 76.4 - 88.2 StarCoderBase-15B "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           118,
           "1.31 7.98 - 21.55 Code-LLaMA-34B 55 76.2 - 86.6 PolyCoder-2.7B 4.39 17.99 - 38.17 Code-LLaMA-13B 47 "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           119,
           "20.9 - - - PaLM-2-S 50 - - - CodeGen-Multi-6.1B 18.35 47.27 - 67.92 LLaMA-7B 17.7 - - - CodeGen-Mult"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           120,
           "33.8 56.9 - 77.6 CodeGen-Mono-350M - - - - LLaMA2-13B 27.6 48.1 - 69.5 CodeGen-Mono-2.7B 28.80 60.73"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           121,
           "14.59 41.49 - 63.00 InCoder-6.7B 21.3 46.5 - 66.2 CODEGEN-Mono-2.7B 27.31 59.19 - 74.24 InCoder-1.3B"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           122,
           "51.80 72.80 - 84.10 GPT-J-6B 11.30 35.62 - 53.63 code-cushman-001 45.90 66.90 - 79.90 GPT-4 - - - - "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           123,
           "1.90 9.20 - 23.42 Table 21. Data in the literature that differ for MBPP. LLMs MBPP Pass@1 Pass@10 Pa"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           124,
           "Table 22. Number of samples generated per minute for different PLMs on Defects4J 1.2 and QuixBugs wi"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           125,
           "Tools / Models Defects4J 2.0 (78 bugs) QuixBugs Java (40 bugs) QuixBugs Python (40 bugs) AlphaRepair"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           126,
           "achieve similar (or even better) performance through carefully designed APR tools. Additionally, in "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           127,
           "CodeGen-6B 0.3 43.6 1.4 23.4 0.0 56.2 0.0 13.0 ChatGLM-6B 7.1 54.2 17.5 12.8 1.7 46.2 45.0 54.0 Vicu"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           128,
           "33.8 52.7 23.5 22.6 1.0 51.7 5.0 8.0 Vicuna-13B 49.8 53.0 14.1 6.5 12.0 44.0 63.0 24.0 WizardCoder-1"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           129,
           "can observe that WizardCoder-15B performs relatively well, particularly in the assertion generation "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           130,
           "Athiwaratkun et al. [6] introduce new benchmarks for evaluating code generation models: MBXP, Multil"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           131,
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 35 Table 25. "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           132,
           "5.0 Data Science DSP 1,119 English Python 2.1 756.9 17.8 226.3 7.6 Data Science MBXP 974∗ English Mu"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           133,
           "29.9 1.1 Public Library TorchDataEval 50 English Python 1.1 329.0 8.6 50.7 1.3 Private Library MTPB "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           134,
           "Notably, StarCoder+ is the result of fine-tuning StarCoder on Java for 2000 steps, while all others "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           135,
           "In conclusion, we can summarize the following points: • The current evaluation of LLMs focuses more "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           136,
           "12.006 Summary at Bottom - - - - 0.610 2.587 4.303 6.274 Necessary Only 0 0 0 0 0.032 0.159 0.318 0."
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           137,
           "1.062 1.591 2.548 Summary Only 0 0 0 0 4.682 15.225 21.200 27.166 Summary at Bottom - - - - 6.465 13"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           138,
           "0 0 0 0 0.510 0.955 1.274 1.911 Summary Only 1.300 5.031 8.042 12.000 2.548 8.279 12.864 18.057 Summ"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           139,
           "0 0 0 0 0 0 0 0 CodeGen-16B-mono Summary at Top 0 0 0 0 0.637 0.637 0.637 0.637 Uncommented 0 0 0 0 "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           140,
           "1.019 1.207 1.274 Necessary Only 0 0 0 0 0 0 0 0 GPT-3.5-Turbo Summary at Top 4.100 7.235 8.989 11.6"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           141,
           "with Unnatural-Code-LLaMA-34B showing outstanding performance. In API-related code generation tasks,"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           142,
           "software engineering tasks. An interesting thing to note is that different literature records vary i"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           143,
           "article organizes and categorizes 123 selected works and literature on the intersection of software "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           144,
           "syntax trees and ELMo-enhanced variational autoencoders to train multiple pre-trained source code la"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           145,
           "allows researchers and developers to create and harvest multiple code views that can be used by LLMs"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           146,
           "CodeBLEU [75], CodeSearchNet [36], and Galeras [77]. 7 CONCLUSION After a comprehensive review, this"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           147,
           "Understanding and Generation. In Proceedings of the 2021 Conference of the North American Chapter of"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           148,
           "Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           149,
           "Society, 207–216. https://doi.org/10.1109/MSR.2013.6624029 [5] Mikel Artetxe, Shruti Bhosale, Naman "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           150,
           "https: //doi.org/10.48550/arXiv.2207.14255 [8] Sid Black, Stella Biderman, Eric Hallahan, Quentin An"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           151,
           "Languages for Code LLMs. arXiv:2308.09895 [cs.PL] [11] Aaron Chan, Anant Kharkar, Roshanak Zilouchia"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           152,
           "Ethereum smart contract development: issues, techniques, and future challenges. Empirical Software E"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           153,
           "preprint arXiv:2107.03374 (2021). [18] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bos"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           154,
           "on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, Bonni"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           155,
           "Baishakhi Ray, Parminder Bhatia, Sudipta Sengupta, Dan Roth, and Bing Xiang. 2023. A Static Evaluati"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           156,
           "International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. Open"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           157,
           "Programming-Lingual Pre-Training for Low-Resource Code Completion. arXiv:2212.09666 [cs.CL] [30] Alb"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           158,
           "Puranik, Horace He, Dawn Song, and Jacob Steinhardt. 2021. Measuring Coding Challenge Competence Wit"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           159,
           "Challenge: Evaluating the State of Semantic Code Search. CoRR abs/1909.09436 (2019). arXiv:1909.0943"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           160,
           "Memorization Issues and a Framework for Code Synthesis Evaluation. arXiv:2212.02684 [cs.SE] [40] Moh"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           161,
           "Attention? An Empirical Study on Large Language Models for Code Generation. arXiv:2306.01220 [cs.SE]"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           162,
           "Datasets: A Community Library for Natural Language Processing. In Proceedings of the 2021 Conference"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           163,
           "arXiv:2305.06599 [cs.SE] [49] Peng Li, Tianxiang Sun, Qiong Tang, Hang Yan, Yuanbin Wu, Xuanjing Hua"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           164,
           "Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Ku"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           165,
           "Failure-Inducing Test Cases with ChatGPT. arXiv:2304.11686 [cs.SE] [52] Yuanzhi Li, Sébastien Bubeck"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           166,
           "(2019). arXiv:1907.11692 http://arxiv.org/abs/1907.11692 [55] Junyi Lu, Lei Yu, Xiaojia Li, Li Yang,"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           167,
           "(2023). https://doi.org/10.48550/arXiv.2306.08568 arXiv:2306.08568 [58] Thang Luong, Hieu Pham, and "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           168,
           "Using Large Language Models. arXiv:2307.10348 [cs.SE] 42 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jing"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           169,
           "research into code language models. https://github.com/ncoop57/gpt-code-clippy [66] Erik Nijkamp, Hi"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           170,
           "[69] Rangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar, Lambert Pouguem Wassi, Michele "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           171,
           "[72] Phind. 2023. Phind-CodeLlama. https://huggingface.co/Phind/Phind-CodeLlama-34B-v1 [73] Colin Ra"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           172,
           "[75] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Amb"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           173,
           "Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Can"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           174,
           "Models for Automated Unit Test Generation. arXiv:2302.06527 [cs.SE] [81] Bo Shen, Jiaxin Zhang, Taih"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           175,
           "cius Carvalho Lopes. 2023. Exploring the Effectiveness of Large Language Models in Generating Unit T"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           176,
           "How Far Are We? arXiv:2305.12865 [cs.SE] [88] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Ne"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           177,
           "Garg. 2023. VeriGen: A Large Language Model for Verilog Code Generation. arXiv:2308.00708 [cs.PL] [9"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           178,
           "Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Syst"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           179,
           "Open Code Large Language Models for Code Understanding and Generation. arXiv:2305.07922 [cs.CL] [99]"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           180,
           "WizardLM: Empowering Large Language Models to Follow Complex Instructions. CoRR abs/2304.12244 (2023"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           181,
           "Generation Models. arXiv:2307.02435 [cs.LG] [105] Zhou Yang, Zhipeng Zhao, Chenyu Wang, Jieke Shi, D"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           182,
           "Large Language Models on Code Comprehension and Generation. arXiv:2308.01240 [cs.CL] [108] Daoguang "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           183,
           "Large Language Models Meet NL2Code: A Survey. arXiv:2212.09420 [cs.SE] [111] Wei Zeng, Xiaozhe Ren, "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           184,
           "inductive inference. In ISSTA ’22: 31st ACM SIGSOFT International Symposium on Software Testing and "
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           185,
           "Evaluations on HumanEval-X. arXiv:2303.17568 [cs.LG] [117] Zibin Zheng, Kaiwen Ning, Jiachi Chen, Ya"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           186,
           "An Automatic Code Generation Model Integrating Program Test Information. arXiv:2202.07612 [cs.SE] [1"
          ],
          [
           "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
           187,
           "On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical S"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           0,
           "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1 LLM Online Spatial-temporal Signal Recon"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           1,
           "ing, spatial-temporal graph, online prediction. I. INTRODUCTION R ECENT advancements in artificial i"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           2,
           "For example, if the information provided to LLMs is insuffi- cient or the prompt is misleading, inac"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           3,
           "disorders screening [13] and financial crisis prediction [14]. There are some application scenarios "
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           4,
           "GNNs, were designed to solve these kinds of time-varying tasks and have succeeded in dealing with su"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           5,
           "arXiv:2411.15764v1  [cs.LG]  24 Nov 2024 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           6,
           "providing a novel perspective for analyzing dynamic complex networks. There are 2 main contributions"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           7,
           "concludes the paper. II. PRELIMINARI KNOWLEDGE A. GSP Preliminaries In this paper, we consider an un"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           8,
           "of the classical Fourier Transform and can be realized by the eigendecomposition of L: L = U diag(λ)"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           9,
           "tasks like denoising and feature enhancement [30]. B. LLM preliminaries The Generative Pre-trained T"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           10,
           "ments that address some of these limitations of GPT-3. GPT-4 offers improved performance across a wi"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           11,
           "Predicted signal 𝒙ෝ [𝑡+ 1] Repeat as new observation are received Fig. 1. An overview of the LLM-OSR"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           12,
           "N nodes, and a subset of these nodes O ⊆V, where only O out of N nodes are observed. Using the obser"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           13,
           "aims to leverage GSP techniques to denoise and enhance spatial-temporal signals to prepare the data "
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           14,
           "temporal signal predictor 10: Operate the LLM-based spatial-temporal signal predictor as seen in Alg"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           15,
           "serving as the performance metric for the current filter: MAE[t] = 1 N N X n=1 |xg,n[t] −˜xn[t]|, (8"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           16,
           "parameters using the gradient descent rule: F X f=1 h(λ)f = F X f=1 h(λ)f −η · ∇hMAE[t], (9) where η"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           17,
           "o[t] is an observation with missing values and noise, the pre- trained GSP-based spatial-temporal si"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           18,
           "reverse approach to the conventional embedding approaches such as the Node2Vec. Since the time-varyi"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           19,
           "10: end while 11: Export the parameters after training is complete 12: Test Phase: 13: for each rece"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           20,
           "of LLM-OSR, we only consider the processed node signals from the observed node neighbors. These expr"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           21,
           ". . .   𝒯𝑣௡ Localized representations Predict 𝑛𝑒𝑥𝑡 signal Next observation of 𝑣௡? Previous estimatio"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           22,
           "to let the LLM leverage the smoothness assumption to infer the target values of each node based on t"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           23,
           "complex tasks requiring step-by-step reasoning [33]. These improvements make GPT-4 more suitable for"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           24,
           "tiotemporal task, this role defines the objective as predicting the current value of a graph node ba"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           25,
           "These prompts include precise temporal and spatial context, such as the time index, the node index, "
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           26,
           "6 predefined maximum number of retries. This additional error- checking function ensures that except"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           27,
           "and the corresponding processed observed neighbor signals of node vi in ˜o[t] 7: Form LLM task T (vi"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           28,
           "detectors to their corresponding positions along the actual highway path. Each loop detector is a no"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           29,
           "constructed, where the edges’ weights are computed by using a Gaussian kernel method which is descri"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           30,
           "signal estimation under Gaussian noise, derived from an LMS optimization problem. • GNLMS [43]: A va"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           31,
           "graph-structured data. Notice that other than the adaptive filters and the LLM-OSRs, JOURNAL OF LATE"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           32,
           "AAA AAA {\"model\":\"gpt-4o- mini\",\"messages\":[{\"role\":\"system\",\"content\":\"The spatiotemporal task is t"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           33,
           "B. LLM-OSR on Traffic Prediction The enhanced multilingual proficiency of GPT-4 enables it to excel "
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           34,
           "OSR to extract richer, more contextually relevant patterns, thereby leading to superior predictive p"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           35,
           "GLMS 8.04 ± 2.1e-03 8.07 ± 2.1e-03 GNLMS 7.91 ± 1.2e-03 7.91 ± 9.0e-04 GNS 8.55 ± 1.5e-03 8.55 ± 2.3"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           36,
           "GLMS 5.09 ± 1.8e-03 5.12 ± 2.4e-03 GNLMS 4.88 ± 9.3e-04 4.89 ± 7.6e-04 GNS 4.69 ± 1.6e-03 4.70 ± 1.2"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           37,
           "show that while LLM-OSR-4 excels under lower noise levels, its performance degrades more significant"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           38,
           "embeddings are then integrated using transformer attention, making the RDGAN a powerful algorithm [1"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           39,
           "are expected to perform worse as noise increases, but LLM- OSR-4 appears more sensitive to this degr"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           40,
           "5.11 ± 9.8e-01 5.06 ± 4.9e-01 LLM-OSR-4 1.39 ± 6.1e-03 1.70 ± 7.5e-03 1.91 ± 1.1e-02 GLMS 2.19 ± 1.8"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           41,
           "4.11 ± 1.5e-02 GGARCH 3.71± 1.9e-03 3.73 ± 3.2e-03 3.76 ± 6.7e-03 TABLE IV EXPERIMENT MAE FOR HOURLY"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           42,
           "3.53 ± 5.9e-03 GCN 2.10 ± 4.1e-02 2.22 ± 1.5e-02 2.12 ± 3.4e-02 RGDAN 1.35 ± 6.2e-02 1.43 ± 3.5e-02 "
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           43,
           "OSR. First, LLMs are known to have difficulties in under- standing numerical data [47]. In our exper"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           44,
           "GNS 5.53 ± 1.6e-02 5.54 ± 2.9e-02 5.57 ± 1.8e-02 GCN 3.09 ± 5.3e-02 3.10 ± 6.3e-02 3.13 ± 7.4e-02 RG"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           45,
           "LLM-OSR-4 0.90 ± 4.1e-03 1.19 ± 3.2e-03 1.35 ± 3.6e-03 GLMS 3.03 ± 1.3e-03 3.05 ± 3.6e-03 3.07 ± 4.0"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           46,
           "2.43 ± 4.6e-03 2.47 ± 4.5e-03 us a numeric output. This limitation can be addressed in the future wh"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           47,
           "making fine-tuning or retraining the LLMs only marginally effective. One potential workaround to ret"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           48,
           "to comprehend long temporal behaviors and predict longer temporal sequences[51]. These limitations c"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           49,
           "to larger graphs and longer temporal sequences. Other than improving the LLMs themselves, an approac"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           50,
           "in scientific domains, such as document image understanding [57] and material science [58]. VI. CONC"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           51,
           "Shenzhen Ubiquitous Data Enabling Key Lab under Grant ZDSYS20220527171406015. REFERENCES [1] J. N. A"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           52,
           "blog, vol. 1, no. 2, 2019. [4] J. Devlin, “Bert: Pre-training of deep bidirectional transformers for"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           53,
           "pre-training for language understanding and generation,” arXiv preprint arXiv:2107.02137, 2021. [8] "
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           54,
           "of Machine Translation Summit XIX, Vol. 2: Users Track, 2023, pp. 132–142. [11] X. Deng, V. Bashlovk"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           55,
           "analysis in alzheimer’s and parkinson’s disease: the contribution of electrophysiological techniques"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           56,
           "[18] S. Xu, F. Wilhelm-Mauch, and W. Maass, “Quantum feature embeddings for graph neural networks.,”"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           57,
           "IEEE Signal Processing Magazine, vol. 37, no. 6, pp. 117–127, 2020. [22] T. N. Kipf and M. Welling, "
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           58,
           "Knowledge and Data Engineering, 2024. [27] J. Huang, X. Zhang, Q. Mei, and J. Ma, “Can llms effectiv"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           59,
           "filterbanks,” in Cooperative and Graph Signal Processing, pp. 299–324. Elsevier, 2018. [31] T. Brown"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           60,
           "Curran Associates, Inc. [32] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. "
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           61,
           "smoothness,” IEEE Transactions on Signal and Information Processing over Networks, vol. 8, pp. 201–2"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           62,
           "networks,” in SIGKDD, 2016, pp. 855–864. [39] C. of Seattle, “Seattle loop detector data,” https://g"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           63,
           "data-selective strategies for adaptive graph signal estimation,” Signal Processing, vol. 167, pp. 10"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           64,
           "story elaboration,” Automated Software Engineering, vol. 31, no. 2, pp. 55, 2024. [48] J. Jang, S. Y"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           65,
           "Y. Fang, “Evaluating very long-term conversational memory of llm agents,” in Proceedings of the Annu"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           66,
           "learning-based image enhancement in optical coherence tomography by exploiting interference fringe,”"
          ],
          [
           "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
           67,
           "tions,” May 4 2010, US Patent 7,712,028. [58] F. Saffarimiandoab, R. Mattesini, W. Fu, E. E. Kuruogl"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           0,
           "What Limits LLM-based Human Simulation: LLMs or Our Design? Qian Wang 1 Jiaying Wu 1 Zhenheng Tang 2"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           1,
           "Quality Data  LLM as a Judge Figure 1. LLM-based Human Simulation Applications 1. Introduction Simul"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           2,
           "simulations have been demonstrated across diverse fields, including society, economics, policy, and "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           3,
           "et al., 2024) has shown that LLMs struggle to replicate distinct personalities. Specifically, Lee et"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           4,
           "What Limits LLM-based Human Simulation: LLMs or Our Design? preferences, lived experiences, and intr"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           5,
           "limitations and framework design challenges, including enhanced data collection methods, improved va"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           6,
           "cess across multiple fields, as detailed in Section 2.2. These simulations can be categorized based "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           7,
           "agents f to generate human-like behaviors, while human knowledge and data provide the ground truth f"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           8,
           "source allocation, and strategic decision-making in financial What Limits LLM-based Human Simulation"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           9,
           "man experts assess agent behaviors 6: Data-based validation: vdata ←Vdata({ai}, Dhuman) // Compare w"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           10,
           "psychological processes. Multiple LLM agents are utilized to simulate humans with diverse personalit"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           11,
           "CompeteAI, a framework examining inter-agent competi- tion through a virtual town environment with d"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           12,
           "controls. LLM Actions Human Participation Social Interaction Framework Design Dialogue generation En"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           13,
           "update their internal states including memories and relation- ships (Si ←update(ai, mi)), and plan f"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           14,
           "lateral dictator games (Kahneman et al., 1986) and hiring decisions (Horton et al., 2011). At the sy"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           15,
           "that certain LLMs can converge faster to Nash Equilibrium strategies with gaming history. Fontana (F"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           16,
           "of Algorithm 1, we analyze their implementation of LLM Actions and Human Participation in Table 2. 2"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           17,
           "cooperation and reducing uncertainties. Agent-based models (ABMs) have emerged as a pow- erful tool "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           18,
           "Generative Intelligence (UGI) platform, combining the Ur- banKG knowledge graph with a city simulato"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           19,
           "(Si ←update(ai, mi)), and plan implementation strategies (plani ←fi(Si, R)). The key challenges lie "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           20,
           "tain limitations persist, particularly in simulating underlying psychological traits (Petrov et al.,"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           21,
           "2024), while innovative approaches like PsychoGAT (Yang et al., 2024b) transform standardized scales"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           22,
           "3. LLM Inherent Drawbacks In this section, we analyze the inherent drawbacks of LLMs that limit thei"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           23,
           "mechanisms component of LLM-driven simulations, reduc- ing their effectiveness in global scenarios. "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           24,
           "tain consistent cognitive patterns when simulating human What Limits LLM-based Human Simulation: LLM"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           25,
           "in several areas. LLMs struggle with maintaining consis- tent personas across multiple interactions "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           26,
           "many frameworks reduce complex emotional states to basic categories or numerical scales, failing to "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           27,
           "making. For instance, individuals often make decisions contrary to their personal preferences to mai"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           28,
           "often lack the flexibility to adapt to emerging behavioral patterns or unexpected interaction dynami"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           29,
           "Human Simulation Human  Experience Continuous  Validation LLM Improvement Reflection Memory Design I"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           30,
           "Memory enhancement through hybrid architectures. To address memory limitations, we propose: (1) impl"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           31,
           "systems (Petrakis & Petrakis, 2012); (2) creating adaptive in- centive mechanisms (Maslow, 2023); an"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           32,
           "glasses and rings, enable comprehensive collection of hu- man behavioral data (Cardenas et al., 2024"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           33,
           "Advances in LLM technology enable the generation of high- quality synthetic training data (Ke et al."
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           34,
           "can be compared with historical market data and validated by domain experts. This multi-level valida"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           35,
           "ture potential of LLM-based human simulations. While our analysis reveals significant challenges in "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           36,
           "as alternative monte-carlo. Available at SSRN 4999492, 2024. An, L., Grimm, V., Sullivan, A., Turner"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           37,
           "Bauer, K., Liebich, L., Hinz, O., and Kosfeld, M. Decoding gpt’s hidden ‘rationality’of cooperation."
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           38,
           "Qu, Q., Ni, S., and Yang, M. Agentcourt: Simulating court with adversarial evolvable lawyer agents. "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           39,
           "2023. Chiang, C.-H., Chen, W.-C., Kuan, C.-Y., Yang, C., and yi Lee, H. Large language model as an a"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           40,
           "Dignum, F., Dignum, V., Davidsson, P., Ghorbani, A., van der Hurk, M., Jensen, M., Kammler, C., Lori"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           41,
           "tems and agent-based simulation, pp. 198–213. Springer, 2000. Dubois, Y., Li, C. X., Taori, R., Zhan"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           42,
           "Fontana, N., Pierri, F., and Aiello, L. M. Nicer than humans: How do large language models behave in"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           43,
           "Gui, G. and Toubia, O. The challenge of using llms to simulate human behavior: A causal inference pe"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           44,
           "a few utterances. arXiv preprint arXiv:2204.10825, 2022. Harrison, J. A. and Budworth, M.-H. Uninten"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           45,
           "Hu, T. and Collier, N. Quantifying the persona effect in llm simulations. arXiv preprint arXiv:2402."
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           46,
           "Language Models: Design and Development of a Com- putational Tool. PhD thesis, Massachusetts Institu"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           47,
           "academy of sciences, 99(suppl 3):7195–7196, 2002. Li, H., Yang, C., Zhang, A., Deng, Y., Wang, X., a"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           48,
           "empowered agents for simulating macroeconomic activi- ties. Available at SSRN 4606937, 2023a. Li, N."
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           49,
           "Cryptotrade: A reflective llm-based agent to guide zero- shot cryptocurrency trading. In Proceedings"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           50,
           "project life cycle perspective. ACM Computing Surveys, 57(4):1–38, 2024. Mao, H., Chen, Z., Tang, W."
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           51,
           "gies for the santhali language. SN Computer Science, 5 (7):807, 2024. Orcutt, G. H., Caldwell, S., a"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           52,
           "annual acm symposium on user interface software and technology, pp. 1–22, 2023a. Park, J. S., O’Brie"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           53,
           "behaviours: a psychometric analysis. arXiv preprint arXiv:2405.07248, 2024. Phelps, S. and Russell, "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           54,
           "What Limits LLM-based Human Simulation: LLMs or Our Design? Santurkar, S., Durmus, E., Ladhak, F., L"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           55,
           "Shi, Z., Gao, S., Chen, X., Feng, Y., Yan, L., Shi, H., Yin, D., Ren, P., Verberne, S., and Ren, Z. "
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           56,
           "a-judge & reward model: What they can and cannot do. arXiv preprint arXiv:2409.11239, 2024. Stern, P"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           57,
           "scalable and bias-aware academic reviews. arXiv preprint arXiv:2408.10365, 2024. Upadhayay, B., Behz"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           58,
           "arXiv:2408.09955, 2024b. Wang, R., Milani, S., Chiu, J. C., Zhi, J., Eack, S. M., Labrum, T., Murphy"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           59,
           "Williams, S. J. Emotion and social theory: corporeal reflec- tions on the (ir) rational. 2000. Winsb"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           60,
           "model agents simulate human trust behaviors? arXiv preprint arXiv:2402.04559, 2024. Xu, F., Zhang, J"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           61,
           "2024. Yang, D., Ziems, C., Held, W., Shaikh, O., Bernstein, M. S., and Mitchell, J. Social skill tra"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           62,
           "models. arXiv preprint arXiv:2407.12835, 2024a. Zhang, Y., Mao, S., Ge, T., Wang, X., de Wynter, A.,"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           63,
           "learning of large language models: A survey. arXiv preprint arXiv:2406.06391, 2024. Zheng, L., Chian"
          ],
          [
           "What Limits LLM-based Human Simulation: LLMs or Our Design?",
           64,
           "rybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference"
          ],
          [
           "Asynchronous LLM Function Calling",
           0,
           "ASYNCHRONOUS LLM FUNCTION CALLING In Gim 1 Seung-seob Lee 1 Lin Zhong 1 ABSTRACT Large language mode"
          ],
          [
           "Asynchronous LLM Function Calling",
           1,
           "how interrupt mechanisms can be extended to enable novel human-LLM or LLM-LLM interactions. 1 INTROD"
          ],
          [
           "Asynchronous LLM Function Calling",
           2,
           "with the complexity of the task (Zaharia et al., 2024). Several studies have tried to address these "
          ],
          [
           "Asynchronous LLM Function Calling",
           3,
           "2024). While these methods help reduce function execution time or the number of function calls (§2),"
          ],
          [
           "Asynchronous LLM Function Calling",
           4,
           "ing the LLM and the executor. (i) The interrupt token must be carefully timed to avoid interfering w"
          ],
          [
           "Asynchronous LLM Function Calling",
           5,
           "system when it must pause and wait for function call returns (§4). All three cases use CML as the in"
          ],
          [
           "Asynchronous LLM Function Calling",
           6,
           "tion calling scheme is challenging, as it requires determining the list of parallelizable function c"
          ],
          [
           "Asynchronous LLM Function Calling",
           7,
           "human-LLM or LLM-LLM interactions, such as interrupt- ible LLM assistants. 2 BACKGROUND AND RELATED "
          ],
          [
           "Asynchronous LLM Function Calling",
           8,
           "enabling asynchronous function calling, requiring LLMs to (1) consider execution time in call genera"
          ],
          [
           "Asynchronous LLM Function Calling",
           9,
           "2 Task dependencies Read html Read xls Read txt Fetch Send Summarize & save pdf Fetch Send Summarize"
          ],
          [
           "Asynchronous LLM Function Calling",
           10,
           "Figure 2. Comparison of LLM-executor interactions. Parallel function calling reduces end-to-end exec"
          ],
          [
           "Asynchronous LLM Function Calling",
           11,
           "aligned with ReWOO but remains agnostic to the LLM’s reasoning strategy, making it more broadly appl"
          ],
          [
           "Asynchronous LLM Function Calling",
           12,
           "generate concurrent function calls, and notify the serving system when it needs to pause and wait fo"
          ],
          [
           "Asynchronous LLM Function Calling",
           13,
           "which represents a function call (§3.1), an interrupt (§3.2), or a trap (a special interrupt), respe"
          ],
          [
           "Asynchronous LLM Function Calling",
           14,
           "end end end end head Web API LLM You All done! All done! Assistant Phone API User Time f1 f2 f3 f4 F"
          ],
          [
           "Asynchronous LLM Function Calling",
           15,
           "two function calls (search nearby and put), the execu- tor processes each in a separate worker, allo"
          ],
          [
           "Asynchronous LLM Function Calling",
           16,
           "When the executor completes a function call with a reg- istered identifier, it asynchronously notifi"
          ],
          [
           "Asynchronous LLM Function Calling",
           17,
           "This flag is set to false while a function call block is be- ing generated and resets to true once t"
          ],
          [
           "Asynchronous LLM Function Calling",
           18,
           "asynchronous function handling (§4). They also enable optimization opportunities for the serving sys"
          ],
          [
           "Asynchronous LLM Function Calling",
           19,
           "must make decisions in the following areas. Deciding the next function call. The LLM selects the nex"
          ],
          [
           "Asynchronous LLM Function Calling",
           20,
           "newly available function has the longest estimated process- ing time among the options, the LLM is t"
          ],
          [
           "Asynchronous LLM Function Calling",
           21,
           "each sample in the dataset. In sequential scenarios, the DAG is linear, with nodes representing func"
          ],
          [
           "Asynchronous LLM Function Calling",
           22,
           "system is built using the Python transformers library (Wolf et al., 2020). AsyncLM consists of four "
          ],
          [
           "Asynchronous LLM Function Calling",
           23,
           "CML format 1 2 3 4 5 ... Pause ... Function call Interrupt Next token Figure 4. Overview of AsyncLM’"
          ],
          [
           "Asynchronous LLM Function Calling",
           24,
           "After sampling each token, the token monitor checks for a function call or trap and, if detected, im"
          ],
          [
           "Asynchronous LLM Function Calling",
           25,
           "tion process is interruptible based on a critical section flag, and (iii) inserting CML-formatted in"
          ],
          [
           "Asynchronous LLM Function Calling",
           26,
           "recomputing the KV cache scales quadratically with the number of tokens, while swapping it to host m"
          ],
          [
           "Asynchronous LLM Function Calling",
           27,
           "stateless and recompute the entire KV cache from scratch 6 for each new session when an interrupt is"
          ],
          [
           "Asynchronous LLM Function Calling",
           28,
           "v3-base-multi-turn. Specifically, v1-parallel and v2-parallel-live provide 400 parallel function cal"
          ],
          [
           "Asynchronous LLM Function Calling",
           29,
           "Interface (Hugging-Face, 2023). In the cloud deployment, only function execution is local. This depl"
          ],
          [
           "Asynchronous LLM Function Calling",
           30,
           "report emulated results based on average token generation latency statistics (5 ms per output token)"
          ],
          [
           "Asynchronous LLM Function Calling",
           31,
           "We use v1-parallel and v2-parallel-live from BFCL for this evaluation. Results. Async improves laten"
          ],
          [
           "Asynchronous LLM Function Calling",
           32,
           "7 Sync Sync-Parallel Async-Naive Async 0 500 1000 1500 Task completion time (ms) 1.7x 1.3x 2.1x 1.6x"
          ],
          [
           "Asynchronous LLM Function Calling",
           33,
           "and (ii) chop vegetables() followed by stir fry(), and culmi- nating in mix everything(). Results. A"
          ],
          [
           "Asynchronous LLM Function Calling",
           34,
           "given F as a set of functions that do not depend on each other for execution, where G(f) is the toke"
          ],
          [
           "Asynchronous LLM Function Calling",
           35,
           "or Async beyond G(f)s and E(f)s. Theorem 6.1 For any set of independent functions F, LAsync(F) ≤LSyn"
          ],
          [
           "Asynchronous LLM Function Calling",
           36,
           "calling. Intuitively, starting the execution of longer functions earlier maximizes overlap with toke"
          ],
          [
           "Asynchronous LLM Function Calling",
           37,
           "400 600 800 Context length (tokens) Keep Recompute Swap (b) Llama-3B Figure 7. The diagrams of the t"
          ],
          [
           "Asynchronous LLM Function Calling",
           38,
           "Performance without co-optimized the serving system. Implementing AsyncLM on the OpenAI API (Async-N"
          ],
          [
           "Asynchronous LLM Function Calling",
           39,
           "functions have dependencies. Consider functions a, b, and c, where c depends on a, and their executi"
          ],
          [
           "Asynchronous LLM Function Calling",
           40,
           "65.97% LLAMA-1B (FT) 6.33% 12.15% 12.06% resource-constrained project scheduling, an NP-hard prob- l"
          ],
          [
           "Asynchronous LLM Function Calling",
           41,
           "and calling order, we examine function call traces of each baseline using multi-step parallel datase"
          ],
          [
           "Asynchronous LLM Function Calling",
           42,
           "9 Sync Sync-Parallel Async-Naive Async 0 2000 4000 6000 Task completion time (ms) 1.7x 1.1x 1.7x 2.5"
          ],
          [
           "Asynchronous LLM Function Calling",
           43,
           "Async. Few-shot prompted Llama models show low overall accuracy, whereas fine-tuning significantly i"
          ],
          [
           "Asynchronous LLM Function Calling",
           44,
           "real human-LLM interactions, we explored its effectiveness in the following scenarios. Interruptible"
          ],
          [
           "Asynchronous LLM Function Calling",
           45,
           "achieved only a 1.1× reduction. The limited improvement of Sync-Parallel is because it requires gene"
          ],
          [
           "Asynchronous LLM Function Calling",
           46,
           "LLM inference interruptible through (1) CML, an in-context interface that facilitates asynchronous i"
          ],
          [
           "Asynchronous LLM Function Calling",
           47,
           "Pesch, E. Resource-constrained project scheduling: No- tation, classification, models, and methods. "
          ],
          [
           "Asynchronous LLM Function Calling",
           48,
           "Geng, S., Josifoski, M., Peyrard, M., and West, R. Grammar- constrained decoding for structured NLP "
          ],
          [
           "Asynchronous LLM Function Calling",
           49,
           "n-inference, 2023. Large Language Model Text Generation Inference Server in Rust and Python. Khattab"
          ],
          [
           "Asynchronous LLM Function Calling",
           50,
           "Ma, S., Ma, S., Li, M., Yin, G., Wang, Z., and Pang, R. Toolsandbox: A stateful, conversational, int"
          ],
          [
           "Asynchronous LLM Function Calling",
           51,
           "solvers for faithful logical reasoning. In Proc. EMNLP, 2023. Park, J. S., O’Brien, J., Cai, C. J., "
          ],
          [
           "Asynchronous LLM Function Calling",
           52,
           "Singh, S., Fore, M., Karatzas, A., Lee, C., Jian, Y., Shang- guan, L., Yu, F., Anagnostopoulos, I., "
          ],
          [
           "Asynchronous LLM Function Calling",
           53,
           "Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R.,"
          ],
          [
           "Asynchronous LLM Function Calling",
           54,
           "cient tool-aware LLM serving with tool partial execution. CoRR, abs/2406.00059, 2024. Yan, F., Mao, "
          ],
          [
           "Asynchronous LLM Function Calling",
           55,
           "Zaharia, M., Khattab, O., Chen, L., Davis, J. Q., Miller, H., Potts, C., Zou, J., Carbin, M., Frankl"
          ],
          [
           "Asynchronous LLM Function Calling",
           56,
           "provide proof sketches for the theorems that demonstrate the advantages of asynchronous function cal"
          ],
          [
           "Asynchronous LLM Function Calling",
           57,
           "cessing Time (LPT) heuristic: Tokens are generated sequen- tially, functions are scheduled in decrea"
          ],
          [
           "Asynchronous LLM Function Calling",
           58,
           "Under the LPT heuristic, functions are ordered such that E(f1) ≥E(f2) ≥· · · ≥E(fn). The completion "
          ],
          [
           "Asynchronous LLM Function Calling",
           59,
           "E G \u00112 when n = |F| is large. When n is large, we can approximate the cumulative token generation la"
          ],
          [
           "Asynchronous LLM Function Calling",
           60,
           "1 + E G \u0013   1 −E + σ √ 2 ln n nG ! . Neglecting terms of order O(1/n), we obtain LSync(F) LAsync(F) "
          ],
          [
           "Asynchronous LLM Function Calling",
           61,
           "Consider swapping fi and fj to obtain schedule σ′′. The sum of token generation latencies become S′′"
          ],
          [
           "Multi-LLM Text Summarization",
           0,
           "Multi-LLM Text Summarization Jiangnan Fang1, Cheng-Tse Liu1, Jieun Kim1, Yash Bhedaru1, Ethan Liu1, "
          ],
          [
           "Multi-LLM Text Summarization",
           1,
           "select the best one whereas k LLMs are used for decentralized multi-LLM summarization. Overall, we f"
          ],
          [
           "Multi-LLM Text Summarization",
           2,
           "2023). These techniques, while promising, still face limitations in consistently delivering high- qu"
          ],
          [
           "Multi-LLM Text Summarization",
           3,
           "Recent advancements in summarization have increasingly leveraged large language models (LLMs), movin"
          ],
          [
           "Multi-LLM Text Summarization",
           4,
           "further highlighted improved factual consistency and reduced hallucinations when using LLMs. While t"
          ],
          [
           "Multi-LLM Text Summarization",
           5,
           "and factual accuracy. For instance, Liang et al. (2024) introduced the Multi-Agent-Debate (MAD) fram"
          ],
          [
           "Multi-LLM Text Summarization",
           6,
           "LLM frameworks to the domain of document sum- marization, addressing limitations of both single LLM "
          ],
          [
           "Multi-LLM Text Summarization",
           7,
           "second round of chunking and summarization on the concatenated intermediate results. Throughout both"
          ],
          [
           "Multi-LLM Text Summarization",
           8,
           "Evaluation (§ 4.2.2) DECENTRALIZED (Sec. 5) Single-Round (Sec. 5.1) Generation (§ 5.1.1) Evaluation "
          ],
          [
           "Multi-LLM Text Summarization",
           9,
           "LLM Mj ∈M, the output is Sj = Mj(P, S) where S represents the input text. Running this step for all "
          ],
          [
           "Multi-LLM Text Summarization",
           10,
           "mary (expressed as its anonymized identifier) and a confidence score for that evaluation (expressed "
          ],
          [
           "Multi-LLM Text Summarization",
           11,
           "Each LLM Mj generates an initial summary S(1) j from the original input text S using the prompt P: S"
          ],
          [
           "Multi-LLM Text Summarization",
           12,
           "k }. The central LLM C evalu- ates these candidates using Pec: E(i) = C(Pec, Si), If the confidence "
          ],
          [
           "Multi-LLM Text Summarization",
           13,
           "cedure for centralized approach (Section 4), which diversifies the knowledge base for summarization."
          ],
          [
           "Multi-LLM Text Summarization",
           14,
           "9: Set S∗←S(i) j 10: if CONVERGED(r) then return S∗ 11: Set P to prompt in Figure 3. Provide a conci"
          ],
          [
           "Multi-LLM Text Summarization",
           15,
           "robust compared to a single model’s decision. 5.1 Single Round 5.1.1 Generation Phase Generation pro"
          ],
          [
           "Multi-LLM Text Summarization",
           16,
           "best summary. On a separate line indicate a confidence level between 0 and 10. ORIGINAL: [text] Summ"
          ],
          [
           "Multi-LLM Text Summarization",
           17,
           "E(i) 1 , . . . , E(i) k are collected, where each E(i) j rep- resents model Mj’s choice of the best "
          ],
          [
           "Multi-LLM Text Summarization",
           18,
           "nated tie-breaker model Mt, where t ∈1, . . . , k. Since the tie-breaker model can be any model in t"
          ],
          [
           "Multi-LLM Text Summarization",
           19,
           "Generation Phase Generation follows the methodology in Sec- tion 4.1.1, producing the set of summari"
          ],
          [
           "Multi-LLM Text Summarization",
           20,
           "▷conversation rounds 3: for each model Mj ∈M do 4: S(i) j = Mj(P, S) 5: Let Si = {S(i) 1 , S(i) 2 , "
          ],
          [
           "Multi-LLM Text Summarization",
           21,
           "the single-round approach, but enters additional rounds with new generation prompts. Formally, let E"
          ],
          [
           "Multi-LLM Text Summarization",
           22,
           "To investigate the proposed multi-LLM summariza- tion framework, we conduct extensive experiments to"
          ],
          [
           "Multi-LLM Text Summarization",
           23,
           "Our multi-LLM framework outperforms single- LLM baselines by up to 3×, as seen in Table 2. The fact "
          ],
          [
           "Multi-LLM Text Summarization",
           24,
           "pared to single-LLM approaches. In Table 2 we use GPT-3.5 as the evaluator and tie-breaking choice i"
          ],
          [
           "Multi-LLM Text Summarization",
           25,
           "0.217 0.118 0.108 0.020 0.384 0.156 0.224 0.058 GPT-4o 0.165 0.095 0.073 0.015 0.372 0.155 0.211 0.0"
          ],
          [
           "Multi-LLM Text Summarization",
           26,
           "0.121 Table 2: Results for the decentralized and centralized Multi-LLM approaches. For the multi-LLM"
          ],
          [
           "Multi-LLM Text Summarization",
           27,
           "baselines, showing that our proposed framework performs consistently under different setups. 6.3 Abl"
          ],
          [
           "Multi-LLM Text Summarization",
           28,
           "in Appendix C.3. Specialized Prompting In all previous experi- ments we have kept the generation pro"
          ],
          [
           "Multi-LLM Text Summarization",
           29,
           "and the decentralized method sees a 2.3× increase. Further details can be found in Appendix C.5. 6.4"
          ],
          [
           "Multi-LLM Text Summarization",
           30,
           "0.180 0.224 0.043 GPT-4o & GPT-3.5 0.328 0.170 0.212 0.033 GPT-4o & GPT-4o mini 0.305 0.153 0.189 0."
          ],
          [
           "Multi-LLM Text Summarization",
           31,
           "0.022 Table 3: Varying the combination of models in our Multi-LLM approaches. Note rounds is the max"
          ],
          [
           "Multi-LLM Text Summarization",
           32,
           "millions of tokens. 6.5 Human evaluation In addition to the ablation studies, we perform hu- man eva"
          ],
          [
           "Multi-LLM Text Summarization",
           33,
           "(κ = 0.6). More details for human evaluations can be found in Appendix D. 7 Conclusion This paper pr"
          ],
          [
           "Multi-LLM Text Summarization",
           34,
           "of summarization. We leave these and other impor- tant directions for future work. References Griffi"
          ],
          [
           "Multi-LLM Text Summarization",
           35,
           "2024. Booookscore: A systematic exploration of book-length summarization in the era of llms. Justin "
          ],
          [
           "Multi-LLM Text Summarization",
           36,
           "Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, D"
          ],
          [
           "Multi-LLM Text Summarization",
           37,
           "tional Linguistics. Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter C"
          ],
          [
           "Multi-LLM Text Summarization",
           38,
           "Tenenbaum, and Igor Mordatch. 2023. Improving factuality and reasoning in language models through mu"
          ],
          [
           "Multi-LLM Text Summarization",
           39,
           "stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to rea"
          ],
          [
           "Multi-LLM Text Summarization",
           40,
           "on New Frontiers in Summarization, pages 48–56, Hong Kong, China. Association for Computational Ling"
          ],
          [
           "Multi-LLM Text Summarization",
           41,
           "ing multi-agent debate with sparse communication topology. Tian Liang, Zhiwei He, Wenxiang Jiao, Xin"
          ],
          [
           "Multi-LLM Text Summarization",
           42,
           "text summarization. Journal of Artificial Intelligence Research, 65:123–143. Rada Mihalcea and Paul "
          ],
          [
           "Multi-LLM Text Summarization",
           43,
           "long document abstractive summarization. Xiao Pu, Mingqi Gao, and Xiaojun Wan. 2023b. Sum- marizatio"
          ],
          [
           "Multi-LLM Text Summarization",
           44,
           "Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries"
          ],
          [
           "Multi-LLM Text Summarization",
           45,
           "57th Annual Meeting of the Association for Compu- tational Linguistics, pages 331–335, Florence, Ita"
          ],
          [
           "Multi-LLM Text Summarization",
           46,
           "Kathleen McKeown, and Tatsunori B. Hashimoto. 2023. Benchmarking large language models for news summ"
          ],
          [
           "Multi-LLM Text Summarization",
           47,
           "Evaluation Metrics: We assess the quality of LLM-generated summaries using ROUGE- 1, ROUGE-L, BLEU-1"
          ],
          [
           "Multi-LLM Text Summarization",
           48,
           "Omax represent an upper bound on the output to- kens (i.e., maximum summary length). We con- sider k"
          ],
          [
           "Multi-LLM Text Summarization",
           49,
           "Multi-Round Overhead Over tmax rounds, the total input token usage for generation is O(tmax·k· (I+Om"
          ],
          [
           "Multi-LLM Text Summarization",
           50,
           "maximum summary length). We consider k dis- tinct LLMs and a maximum of tmax conversational rounds. "
          ],
          [
           "Multi-LLM Text Summarization",
           51,
           "cost is justified by the potential gains in robustness and reliability of the final output, but also"
          ],
          [
           "Multi-LLM Text Summarization",
           52,
           "Varying Evaluation LLM In this section, we compare the scores of the cen- tralized and decentralized"
          ],
          [
           "Multi-LLM Text Summarization",
           53,
           "3.5-default results are chosen as the basis instead of GPT-4o mini is because as an evaluator and de"
          ],
          [
           "Multi-LLM Text Summarization",
           54,
           "ditional rounds of evaluation and regeneration do not improve summary scores. C.2 Varying Model Comb"
          ],
          [
           "Multi-LLM Text Summarization",
           55,
           "evaluator and tie-breaker where GPT-3.5 is absent, a possible reason the improvements for these pair"
          ],
          [
           "Multi-LLM Text Summarization",
           56,
           "strengths of individual LLMs and reduce overall coherence and relevance in the final output. Sec- on"
          ],
          [
           "Multi-LLM Text Summarization",
           57,
           "sential information from each chunk. With this, we propose a form of specialized prompting as a way "
          ],
          [
           "Multi-LLM Text Summarization",
           58,
           "serves as the evaluator. The framework and method- ology following the generation of the four baseli"
          ],
          [
           "Multi-LLM Text Summarization",
           59,
           "0.452 0.094 Multi-LLM 1 round max 0.326 0.163 0.221 0.027 0.438 0.175 0.446 0.089 Centralized Multi-"
          ],
          [
           "Multi-LLM Text Summarization",
           60,
           "Multi-LLM 1 round max 0.333 0.173 0.219 0.036 0.479 0.197 0.485 0.121 GPT-4o Evaluator Decentralized"
          ],
          [
           "Multi-LLM Text Summarization",
           61,
           "the experimental variables, and we underline the best results overall. For ease of comparison, we re"
          ],
          [
           "Multi-LLM Text Summarization",
           62,
           "Decentralized 3 rounds 0.301 0.154 0.184 0.024 0.445 0.178 0.449 0.095 1 rounds 0.299 0.152 0.184 0."
          ],
          [
           "Multi-LLM Text Summarization",
           63,
           "0.181 0.440 0.095 1 rounds 0.329 0.172 0.214 0.036 0.460 0.189 0.451 0.104 Table 6: Multi-LLM framew"
          ],
          [
           "Multi-LLM Text Summarization",
           64,
           "0.468 0.189 0.470 0.109 1 round max 0.333 0.173 0.219 0.036 0.479 0.197 0.485 0.121 Specialized Prom"
          ],
          [
           "Multi-LLM Text Summarization",
           65,
           "curate Table 2. Note that these results use GPT-3.5 for the evaluator in the centralized approach, a"
          ],
          [
           "Multi-LLM Text Summarization",
           66,
           "mentation. With this experiment we present re- sults that showcase the trade offs and performance di"
          ],
          [
           "Multi-LLM Text Summarization",
           67,
           "there is whitespace. Refer to Figure 9 for more detailed explanation. We ultimately curate 20% of th"
          ],
          [
           "Multi-LLM Text Summarization",
           68,
           "and especially the 3 round performs best as each round and each model provides an opportunity to foc"
          ],
          [
           "Multi-LLM Text Summarization",
           69,
           "tation and anonymized such that the raters do not know which model produced which summaries. We do n"
          ],
          [
           "Multi-LLM Text Summarization",
           70,
           "0.044 Centralized Multi-LLM 3 round max 0.367 0.194 0.321 0.041 Multi-LLM 1 round max 0.379 0.206 0."
          ],
          [
           "Multi-LLM Text Summarization",
           71,
           "by choosing the model with the higher score, and if the scores are the same, we fallback on the de- "
          ],
          [
           "Multi-LLM Text Summarization",
           72,
           "3.80 2 4.28 4.00 4.42 3.85 4.85 4.85 4.52 4.23 3 3.42 4.57 3.57 4.42 3.85 4.57 3.61 4.52 4 3.71 4.57"
          ],
          [
           "Multi-LLM Text Summarization",
           73,
           "3.95 4.33 10 4.14 3.85 4.42 4.00 4.71 4.57 4.42 4.14 Table 9: Averaged scores (out of 5) given by hu"
          ],
          [
           "Multi-LLM Text Summarization",
           74,
           "GPT-3.5 2 GPT-3.5 GPT-3.5 GPT-3.5 GPT-3.5 GPT-4o mini 3 GPT-4o mini GPT-4o mini GPT-4o mini GPT-4o m"
          ],
          [
           "Multi-LLM Text Summarization",
           75,
           "9 GPT-4o mini GPT-4o mini GPT-4o mini GPT-4o mini GPT-4o mini 10 GPT-3.5 GPT-3.5 GPT-3.5 GPT-3.5 GPT"
          ],
          [
           "Multi-LLM Text Summarization",
           76,
           "to the actual methodologies which are then detailed in later text. From here we gather the word coun"
          ],
          [
           "Multi-LLM Text Summarization",
           77,
           "Conciseness Penalize summaries that are overly verbose. 1: Ideas are repeated multiple times, or are"
          ],
          [
           "Multi-LLM Text Summarization",
           78,
           "phrased sentences. 5: Summary is easy to follow and understand. No grammatical errors. Figure 10: Sc"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           0,
           "arXiv:2505.20921v2  [cs.CL]  29 May 2025 Automatic Transmission for LLM Tiers: Optimizing Cost and A"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           1,
           "tier model, generates a new response, and re- evaluates until a valid response is obtained. Ad- diti"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           2,
           "2024; Fu et al., 2024; Zong et al., 2024), tasks † Major in Bio Artificial Intelligence Figure 1: An"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           3,
           "have several limitations. First, these methods re- quire extensive data labeling to assess whether e"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           4,
           "tor answers a given question. However, the judge determines that the response is invalid, LLM-AT up-"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           5,
           "tween accuracy and monetary costs, as well as accuracy and execution time. 2 Related Works LLM Routi"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           6,
           "ods focus on select the best-performing LLM on a given question, disregarding inference cost. Iterat"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           7,
           "with different price points and performance lev- els. In general, higher-tier models provide better "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           8,
           "initial tier, the corresponding generator produces an answer, and the judge evaluates its validity. "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           9,
           "which have been shown to be effective across a range of LLMs, rather than relying on model- specific"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           10,
           "age the reasoning process generated by the gen- erator for answer evaluation, and especially since t"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           11,
           "lower tiers. To select the appropriate initial tier for a given question, the starter estimates the "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           12,
           "Ideally, human annotated labels would be used, but they are usually unavailable. Instead, we pseudo-"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           13,
           "Training a dedicated accuracy estimation model would require significant labeled data, as well as ad"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           14,
           "distributions (Jung et al., 2019). Note that we use a benchmark score to utilize the background know"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           15,
           "where top(q) is the set of top-k similar questions of q and lj,q′ is the correctness label of tier j"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           16,
           "et al., 2024). The easiest level consists of 500 elementary-level science questions sampled from the"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           17,
           "tion for the lowest tier, GPT-4o-mini, allowing com- plex problems to be forwarded to higher tiers t"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           18,
           "output of the last cycle is used as the final answer. 4.4 Evaluation Metric We evaluate LLM-AT and t"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           19,
           "we used the embedding model Alibaba-NLP/gte- Qwen2-1.5B-instruct from Hugging Face2, with cosine sim"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           20,
           "time and accuracy-cost trade-offs. Specifically, LLM-AT achieves substantial time efficiency while m"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           21,
           "60 65 70 75 80 85 MATH LLM-AT (Oracle) LLM-AT Single Iteration 50 100 150 200 250 Time (minutes) 86 "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           22,
           "Median Line Actual Accuracy GPT-4o-mini GPT-4o o1-mini o1 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 MC"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           23,
           "support these findings. 5.2 Evaluating the accuracy estimator To validate the effectiveness of our a"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           24,
           "Table 2: Performance of the judge and generator on MATH and MCQA. Results marked with * use the spec"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           25,
           "19.37 Table 3: Performance by the amount of accumulated historical data. Q1 corresponds to the lowes"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           26,
           "pares the F1, recall, and precision of the judge with the accuracy of the generator to assess the re"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           27,
           "curacy of each tier for a given question. Therefore, performance can be affected by the amount and q"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           28,
           "API Cost ($) Time (minutes) Recent 30 history 0.770 15.33 90.97 0.911 7.89 97.20 Full accumulation 0"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           29,
           "0.333 0.482 0.281 0.549 0.750 0.328 0.526 o1-mini 0.491 0.482 0.333 0.373 0.156 0.328 0.456 Table 5:"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           30,
           "accuracy difference narrows, and in the Number Theory, GPT-4o even outperforms o1-mini. Reflect- ing"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           31,
           "ule. # of transition MATH MCQA 0 83.5% 95.9% 1 15.2% 3.3% 2 1.3% 0.8% 3 0.0% 0.0% Table 7: Distribut"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           32,
           "tional overhead is also expected to be significantly lower than that of the generator. Tier Transiti"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           33,
           "higher-tier models, o1-mini and o1, are selected more frequently as the question difficulty rises. A"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           34,
           "10 20 30 40 Counts Tier 2: o1-mini Level 1 Level 2 Level 3 Level 4 Level 5 10 20 30 40 50 60 70 Coun"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           35,
           "community. Future work may explore unified tier systems that integrate both open-source and propri- "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           36,
           "Information Processing Systems, 37:131000–131034. Anthropic. 2023. Meet claude. https://www. anthrop"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           37,
           "Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           38,
           "Dayuan Fu, Biqing Qi, Yihuai Gao, Che Jiang, Guanting Dong, and Bowen Zhou. 2024. MSI-agent: Incorpo"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           39,
           "Jacob Steinhardt. 2021b. Measuring mathematical problem solving with the math dataset. NeurIPS. Jie "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           40,
           "Zheng Yuan, Chang Zhou, and Jingren Zhou. 2024. Routing to the expert: Efficient reward-guided en- s"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           41,
           "selection algorithm for large language models. arXiv preprint arXiv:2408.08545. Todor Mihaylov, Pete"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           42,
           "A graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Yubo Wang, Xu"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           43,
           "tion Processing Systems, 36. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Nar"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           44,
           "suring the API cost, we consider the input token cost and the output token cost, including reasoning"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           45,
           "Benchmarks Figure 6 presents the results when using bench- mark performance similar to or identical "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           46,
           "API Cost ($) Time (minutes) RouteLLM4o-mini 0.795 41.08 109.85 RouteLLM4o 0.790 40.99 109.22 RouteLL"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           47,
           "iments, the top-tier model o1 is designated as the strong model and each lower-tier model is set as "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           48,
           "side the single inference baseline curve in the right plot. This indicates that it incurs higher cos"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           49,
           "presents the accuracy and API cost. In both graphs, points located toward the top-left indicate bett"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           50,
           "ure 8, the selection frequency of the GPT-4o con- sistently decreases as the difficulty of the quest"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           51,
           "0 10 20 30 40 50 60 70 80 Accuracy 46.67 36.36 37.50 27.27 0.00 72.94 62.02 48.61 44.07 48.98 Abstai"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           52,
           "performance with that on the questions it did not abstain from. The analysis shows that, across all "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           53,
           "We provide examples from MATH and MCQA to illustrate how LLM-AT works in Tables 15 and 16. Here is a"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           54,
           "here). Give step by step reasoning in a maximum of three sentences before your answer. Do not exceed"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           55,
           "Question: Find the distance between the points $(2,1,-4)$ and $(5,8,-3)$ Code: from sympy import sqr"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           56,
           "Given a mathematics question, write Python Code to solve the following questions. Store your result "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           57,
           "distance = sqrt(distance_squared) answer = distance Question: How many zeros are at the end of the p"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           58,
           "Indicate the evaluation result after ‘validity’: using one of the following options: • Answer ‘yes’ "
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           59,
           "Please make sure to follow below final output format exactly. validity: Table 14: Prompt for the ite"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           60,
           "Question: Consider a rhombohedral crystal, with the interatomic distance of 10 Angstrom and the angl"
          ],
          [
           "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
           61,
           "approx 9.95 ) Angstrom, which is closest to option (B). The correct answer is: (B). Judge: no Iterat"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           0,
           "Published at Building Trust Workshop at ICLR 2025 SYSTEMATIC EVALUATION OF LLM-AS-A-JUDGE IN LLM ALI"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           1,
           "leading to potentially inconsistent comparisons between different alignment algo- rithms. In this wo"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           2,
           "pairwise evaluation on numerous LLM alignment tasks, such as summarization and multiturn con- versat"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           3,
           "Wang et al., 2023c; Li et al., 2023; Zheng et al., 2024; Li et al., 2024c; Chiang et al., 2024; Dubo"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           4,
           "position and length bias, by (1) defining them within a unified accuracy-based framework, (2) explic"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           5,
           "ated by separate LLMs, the human judge is asked to select the better response based on predefined cr"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           6,
           "top-k, which generate non-deterministic outputs. The non-deterministic level is controlled by the te"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           7,
           "shorter versions. Saito et al. (2023) observed a discrepancy between LLMs and human preferences rega"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           8,
           "c is the preferred LLM response and y(n) r is the less preferred response, both by human evaluators."
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           9,
           "result sn = (y(n), y′(n)) represents the selection outcome from both response orders across all the "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           10,
           "X =0 indicates otherwise. Under this assumption, re-evaluating the same case h would always yield th"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           11,
           "X, p [X|X] = 1 −q (1) 3 Published at Building Trust Workshop at ICLR 2025 where q is the probability"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           12,
           "positions. We first consider a special case where the LLM judge makes a fully consistent decision (i"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           13,
           "(2) where the absolute value |PB| measures the degree of position bias, with positive and negative v"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           14,
           "where qcr and qrc are the probabilities that the LLM judge’s decision is flipped for response order "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           15,
           "the probability that the LLM-judge’s result aligns with the human selection when the human selected "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           16,
           "between accuracy derived from X and accuracy derived from Z as follows: p [X = 1|∆l > 0] = p [Z = 1|"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           17,
           "ther analyze their inter-relationship theoretically. Our findings are summarized below and formally "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           18,
           "Prompts and responses Accuracy, position bias, length bias Human Preference Data Distribution Judgin"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           19,
           "the human preference labels provided by the dataset, based on the computational methods described in"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           20,
           "data cases of both datasets (143,356 for summarization and 124,243 for HH-RLHF-Helpfulness), so we r"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           21,
           "the entire dataset. Overall, both datasets used in our experiments contain 200 distinct samples for "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           22,
           "comparable performance to GPT-4 in judging decision-making, but at a cost that is 4 to 6 times lower"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           23,
           "use LLMs as judges explicitly explain how and why they choose the temperature in their exper- iments"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           24,
           "parameter, we let LLM judges select their preferred response from each sample repeatedly for K = 5 t"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           25,
           "that the LLM’s judgments are consistent across identical inputs. Since different LLMs show the same "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           26,
           "Acc (Accboth) SCR (yc, yr) SCR (yr, yc) Acc (Accboth) 0.0 0.977 0.971 0.665 (0.003) 0.974 0.967 0.57"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           27,
           "summarization and HH-RLHF-Helpfulness datasets. Results are demonstrated using GPT-4o and prompt tem"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           28,
           "which prompt template is used. It demonstrates that the superior internal capacities of recent LLMs,"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           29,
           "Position Bias Position biases of all the LLM judges are shown in Figure 3a and 3b, where positive va"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           30,
           "same context. (a) TL;DR (b) HH-RLHF (c) TL;DR (d) HH-RLHF Figure 3: Position bias (top two) and leng"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           31,
           "compared to the summarization task, LLM judges exhibit a greater degree of length bias on the multi-"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           32,
           "Second, our evaluation studies concentrate on LLM-as-a-Judge methods, although open-source re- ward "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           33,
           "bias. We developed a framework to evaluate, compare, and visualize the reliability of LLM judges and"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           34,
           "Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv pr"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           35,
           "arXiv:2304.00723, 2023. Pengyu Cheng, Yifan Yang, Jian Li, Yong Dai, and Nan Du. Adversarial prefere"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           36,
           "arXiv preprint arXiv:2407.21783, 2024. Yann Dubois, Bal´azs Galambosi, Percy Liang, and Tatsunori B "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           37,
           "Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online ai fe"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           38,
           "Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feed"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           39,
           "for evaluating alignment. arXiv preprint arXiv:2310.05470, 2023. Tianle Li, Wei-Lin Chiang, Evan Fri"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           40,
           "Neiswanger. Sample efficient reinforcement learning from human feedback via active exploration. 2023"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           41,
           "Cho, and Ethan Perez. Training language models with language feedback at scale. arXiv preprint arXiv"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           42,
           "and biased evaluators. arXiv preprint arXiv:2405.01724, 2024. Aman Singh Thakur, Kartik Choudhary, V"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           43,
           "pp. 59–63, 2017. 12 Published at Building Trust Workshop at ICLR 2025 Binghai Wang, Rui Zheng, Lu Ch"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           44,
           "arXiv:2305.17926, 2023b. Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           45,
           "long-form question answering. arXiv preprint arXiv:2305.18201, 2023. Dun Zeng, Yong Dai andPengyu Ch"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           46,
           "Lianghui Zhu, Xinggang Wang, and Xinlong Wang. Judgelm: Fine-tuned large language models are scalabl"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           47,
           "https://arxiv.org/pdf/2307.03025 11/2023 chen (Cheng et al., 2023) https://arxiv.org/pdf/2304.00723 "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           48,
           "12/2023 mehta (Mehta et al., 2023) https://arxiv.org/pdf/2312.00267 12/2023 wu (Wu & Aji, 2023) http"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           49,
           "both precise and concise. Post: <post> Summary A: <summary A> Summary B: <summary B> FIRST provide a"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           50,
           "{prompt} A. {answer a} B. {answer b} Which one is better? A or B? Examples of Prompt Templates (HH-R"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           51,
           "the user’s questions. A helpful response should directly address the human questions without going o"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           52,
           "--User Question-- {prompt} --The Start of Assistant A’s Answer-- {response 1} --The End of Assistant"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           53,
           "Fast forward to now and the principal and guidance counselor have called my parents and spilled the "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           54,
           "Do I make arrangements to get back together in a year? Or Do I just give up and accept it as over? A"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           55,
           "Rejected Response by Human Evaluators: \"The goal is to shoot the basketball through the hoop, in the"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           56,
           "\"Smooth\" is a great song. Did you know Rob Thomas was a cast member of the band One Tree Hill?\" Reje"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           57,
           "TL;DR Summarization HH-RLHF-Helpfulness Temperature SCR (yc, yr) SCR (yr, yc) Acc (Accboth) SCR (yc,"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           58,
           "0.650 (0.004) 0.924 0.916 0.578 (0.008) Table 4: Self-consistent rate (SCR) and accuracy (Acc) relat"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           59,
           "0.1 0.986 0.985 0.630 (0.001) 0.983 0.988 0.591 (0.003) 0.3 0.974 0.982 0.627 (0.003) 0.970 0.968 0."
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           60,
           "SCR (yr, yc) Acc (Accboth) SCR (yc, yr) SCR (yr, yc) Acc (Accboth) 0.0 0.948 0.936 0.554 (0.004) 0.9"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           61,
           "TL;DR summarization and HH-RLHF-Helpfulness datasets. Results are demonstrated using GPT- 3.5-turbo "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           62,
           "rafailov 0.547 (0.022) 0.668 (0.040) 0.049 (0.018) 0.152 (0.045) chen 0.516 (0.028) 0.652 (0.030) -0"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           63,
           "0.658 (0.028) 0.734 (0.029) -0.081 (0.023) 0.117 (0.055) guo 0.655 (0.011) 0.733 (0.024) -0.140 (0.0"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           64,
           "0.619 (0.032) 0.715 (0.010) 0.090 (0.036) 0.257 (0.068) chen 0.615 (0.021) 0.692 (0.031) 0.010 (0.01"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           65,
           "To prove this, we analyze two separate conditions: (1) the LLM judge prefers the first position, (2)"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           66,
           "0.667 (0.011) 0.737 (0.014) 0.022 (0.015) 0.197 (0.031) chen / gpt-4o 0.658 (0.028) 0.734 (0.029) -0"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           67,
           "Template Accboth Accrandom Position Bias Length Bias zeng 0.536 (0.012) 0.654 (0.023) 0.013 (0.036) "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           68,
           "Position Bias Length Bias guo 0.618 (0.040) 0.694 (0.030) -0.005 (0.013) 0.135 (0.075) xu 0.610 (0.0"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           69,
           "0.602 (0.036) 0.681 (0.030) -0.028 (0.026) 0.294 (0.059) rafailov 0.594 (0.014) 0.657 (0.019) 0.047 "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           70,
           "0.618 (0.040) 0.694 (0.030) -0.005 (0.013) 0.135 (0.075) xu / gpt-4o 0.610 (0.025) 0.702 (0.019) 0.0"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           71,
           "20 Published at Building Trust Workshop at ICLR 2025 (x, yc, yr) and h′ = (x, yr, yc), which results"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           72,
           "tions, either by emphasizing the response quality or due to the length bias (e.g. yc is longer than "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           73,
           "of transitioning to the fourth case. Therefore, if the LLM judge exhibits the position bias towards "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           74,
           "= lim N→∞ 1 N N X n=1 1 \u0010 y(n) =y(n) c ∧y′(n) =y(n) r \u0011 . This corresponds to the proportion of the "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           75,
           "to investigate the underlying reasons for each outcome and to derive the positional bias accordingly"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           76,
           "✗ ✓ ✗ (a) Prefer first position y y′ yc yr yr yc ✓ ✗ ✗ ✓ ✗ ✓ ✓ ✗ ✗ ✓ ✗ ✓ (b) Prefer second position "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           77,
           "In this study, we assume the flipping probability does not depend on the value of X, which needs fur"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           78,
           "1 −2 · qcr 22 Published at Building Trust Workshop at ICLR 2025 Accordingly, the relationship betwee"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           79,
           "preferred response y and y′ from each order h and h′, where y, y′ ∈{yc, yr}. Broadly, we denote the "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           80,
           "n = \u0010 y(n) 1 , y(n) 2 , ..., y(n) K , y′(n) 1 , y′(n) 2 , ..., y′(n) K \u0011 . The flipping probabilitie"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           81,
           "c ) and k(n) rc = PK k=1 1(y′(n) k = y(n) r ) are the numbers of choosing the first response in s′(n"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           82,
           "0. Let LBcr and LBrc be length biases measured for response order (yc, yr) and (yr, yc) in all the d"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           83,
           "with PB in its measurement. In the next part, we introduce a method to approximate accuracies p [X ="
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           84,
           "When Accrandom is used for accuracy, it depends on the proportion of both the first and the third ca"
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           85,
           "0, h ∈D}, and D∆l≤0 ={h|∆l ≤0, h ∈D} and also divide the judging result set J into two subsets of J "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           86,
           "n|n = 1 . . . N}, where s′ n = \u0010 y(n) 1 , y(n) 2 , ..., y(n) K , y′(n) 1 , y′(n) 2 , ..., y′(n) K \u0011 "
          ],
          [
           "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
           87,
           "∆l≤0 \u001aks′ K · ks′ −1 K−1 + K−ks′ K ·K−ks′ −1 K−1 \u001b , where N+ =|J ′ ∆l>0| and N−= \f\fJ ′ ∆l≤0 \f\f. Add"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           0,
           "Published as a conference paper at ICLR 2024 TEST: TEXT PROTOTYPE ALIGNED EMBEDDING TO ACTIVATE LLM’"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           1,
           "trains a fundamental large model, or fine-tunes a pre-trained LLM for TS data; TS-for-LLM (data-cent"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           2,
           "performance than today’s SOTA TS models and offer benefits for few-shot and generalization. By treat"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           3,
           "Model from scratch (LM-of-TS), then fine-tune the model accordingly for various downstream tasks. Or"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           4,
           "Data perspective. LLM-for-TS methods, especially when building a foundation model, necessitate large"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           5,
           "multivariate while text is univariate. For example, excepting mean arterial pressure, dozens of vita"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           6,
           "models. Most multimodal approaches use alignment, for example, aligning text embedding and image emb"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           7,
           "the Text embedding space of LLM (TEST). Based on CL, TEST uses text embedding vectors as prototypes "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           8,
           "Pre-training Ma et al. (2023) accurate large datasets Earth transformer Bi et al. (2023) LLM-for-TS "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           9,
           "There hasn’t been much research done on TS+LLM because this field is still in its infancy. We summar"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           10,
           "2.2 TIME SEIRES EMBEDDING TS embedding can provide identities by including typical, associated, and "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           11,
           "embeddings as basic prototypes to lead the learning. However, in addition to the alignment, we still"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           12,
           "t }T,D t=1,d=1 has D variables and T time points. It can be segmented to a list of K non-overlapping"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           13,
           "Tstrong (permutation-and-jitter strategy, splitting the sequence into a random number of segments an"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           14,
           "all remaining ones within the B size minibatch as negative pairs He et al. (2020). The instance-wise"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           15,
           "regarded as soft labels of instances which are used in Equation 1. In addition to rows, columns of f"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           16,
           "Lfea = − M X i=1 (σ(mi, m+ i ) | {z } Alignment −σ(mi, m− i ) | {z } Difference ) ⇒− M X i=1 log exp"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           17,
           "TS tokens lack text annotation, we can place their embedding near typical text descriptions of TS, s"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           18,
           "text prototype can be relaxed, not necessarily the description related to TS. In this work, we choos"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           19,
           "5 Published as a conference paper at ICLR 2024 3.4 LEARNABLE PROMPT EMBEDDING Even TS has been descr"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           20,
           "(4) GPT4TS Zhou et al. (2023)has proved the feasibility that SFT can make LLM apply to TS. Based on "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           21,
           "log pϕ(z′ i|h<i) = X i∈Yidx log pϕ+∆(zi + δzi|h<i) ≈ X i∈Yidx log pϕ(zi|h<i) · X i∈peidx log p∆(δzi|"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           22,
           "EXPERIMENTS The core of TEST is to train an encoder fe and a soft prompt pe as described in Algorith"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           23,
           "θfd = θfd −η▽θfd Lae 6: // UPDATE PROJECTOR 7: θfp = θfp −η▽θfp Lins 8: end for 9: for e in epochs d"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           24,
           "4096 Table 2: The Used Language Model The used LLMs are as listed in Table 2. Each encoder and soft "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           25,
           "Franceschi et al. (2019b), TS2Vec Yue et al. (2022), and CoST Woo et al. (2022a). The overall result"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           26,
           "univariate TS and 25% for multivariate TS. TEST makes most LLMs comparable to, if not better than, t"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           27,
           "FORECASTING We present short-forecasting MSE scores for all 19 kinds of varied time series datasets "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           28,
           "LLM+TEST (ours) Classical SOTA models LLM+TEST (ours) SOTA models QA SFT ℎ Embedding before / after "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           29,
           "settings in Zhou et al. (2023), we present few-shot forecasting for 10% time steps in training datas"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           30,
           "TS becomes more discriminative. Case. We use nearest neighbor method to find the text that a TS toke"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           31,
           "that using TEST, LLM can archive comparable performance to SOTA methods. TS-for-LLM can enrich LLM’s"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           32,
           "The impact of model type is intuitive, it is related to downstream tasks, where the bidirectional st"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           33,
           "2018. CoRR, abs/1811.00075, 2018. Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           34,
           "in Neural Information Processing Systems, 2020a. Mathilde Caron, Ishan Misra, Julien Mairal, Priya G"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           35,
           "Choi. Text-to-ecg: 12-lead electrocardiogram synthesis conditioned on clinical text reports. In IEEE"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           36,
           "bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. Jiaxiang Dong, Ha"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           37,
           "Cuntai Guan. Time-series representation learning via temporal and contextual contrasting. In Interna"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           38,
           "Bilal Piot, Koray Kavukcuoglu, R´emi Munos, and Michal Valko. Bootstrap your own latent - A new appr"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           39,
           "John Hopcroft and Ravindran Kannan. Computer science theory for the information age. Cambridge Unive"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           40,
           "2019.04.014. Salar Hosseini Khorasgani, Yuxuan Chen, and Florian Shkurti. SLIC: self-supervised lear"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           41,
           "classification. In AAAI Conference on Artificial Intelligence, pp. 8375–8383, 2021a. doi: 10.1609/AA"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           42,
           "few-shot health learners. CoRR, abs/2305.15525, 2023. doi: 10.48550/arXiv.2305.15525. Yong Liu, Haix"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           43,
           "masked hierarchical cluster-wise contrastive learning for multivariate time series. In AAAI Con- fer"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           44,
           "Research, pp. 2498–2518, 2023. Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           45,
           "contrastive learning for improving face representations. In IEEE International Conference on Au- tom"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           46,
           "time series with temporal neighborhood coding. In International Conference on Learning Repre- sentat"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           47,
           "Wetterstation. Weather. 2017. doi: https://www.bgc-jena.mpg.de/wetter/. Kristoffer Wickstrøm, Michae"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           48,
           "nential smoothing transformers for time-series forecasting. CoRR, abs/2202.01381, 2022c. Haixu Wu, J"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           49,
           "Xinyu Yang, Zhenguo Zhang, and Rongyi Cui. Timeclr: A self-supervised contrastive learning framework"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           50,
           "1609/aaai.v37i9.26317. George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           51,
           "classification with attentional prototypical network. In AAAI Conference on Artificial Intelligence,"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           52,
           "Informer: Beyond efficient transformer for long sequence time-series forecasting. In AAAI Con- feren"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           53,
           "14 Published as a conference paper at ICLR 2024 A APPENDIX A.1 RELATED WORK Our work mainly involves"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           54,
           "discrimination. They utilize data augmentations to transform original inputs into a new embedding sp"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           55,
           "large models in these diverse fields, an intriguing question emerges: can large models be effectivel"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           56,
           "Published as a conference paper at ICLR 2024 Type Methods Instance-level SimCLR Chen et al. (2020) T"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           57,
           "Table 4: Contrastive Learning based Universal Representation Methods for Time Series Means Pros Cons"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           58,
           "ries analysis has followed a more gradual path. Traditional analytical methods have predominantly re"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           59,
           "inputs. Thus, as shown in Figure 6, we build a causal TCN with 10 layers of convolution blocks. Each"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           60,
           "perparameter optimization was performed on the encoder hyperparameters: Optimizer is Adam with learn"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           61,
           "(sMAPE) is used for M3 and M4; normalized deviation (ND) is used for ELECTR. All experiments are rep"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           62,
           "ILI is not used for few-shot learning for the limited quantity that is hard to follow the definition"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           63,
           "- Quarterly M4 Monthly 8 48000 - Monthly M4 Weekly 359 13 - Monthly M4 Daily 4227 14 - Monthly M4 Ho"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           64,
           "source and target. A.3.2 BASELINE DETAILS For long-shot forecasting, we refer to the SOTA methods re"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           65,
           "For zero-shot forecasting, we refor to the SOTA methods reported in Zhou et al. (2023): N-BEATS Ores"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           66,
           "0.408 0.410 0.380 0.389 0.426 0.441 0.795 0.669 0.837 0.700 1.113 0.776 336 0.368 0.392 0.366 0.394 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           67,
           "0.384 0.402 0.494 0.479 0.386 0.400 0.376 0.419 0.865 0.713 0.878 0.740 1.044 0.773 192 0.414 0.422 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           68,
           "0.427 0.426 0.458 0.450 0.542 0.510 0.456 0.452 0.440 0.460 1.040 0.795 1.072 0.837 1.198 0.821 ETTh"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           69,
           "720 0.381 0.423 0.406 0.441 0.462 0.468 0.500 0.497 0.831 0.657 0.463 0.474 3.647 1.625 3.188 1.540 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           70,
           "336 0.163 0.260 0.169 0.266 0.198 0.260 0.212 0.329 0.209 0.301 0.214 0.329 0.300 0.394 0.280 0.380 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           71,
           "0.423 0.287 0.407 0.290 0.617 0.336 0.621 0.399 0.598 0.370 0.604 0.373 0.696 0.379 0.685 0.390 0.84"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           72,
           "Weather 96 0.150 0.202 0.162 0.212 0.152 0.220 0.197 0.281 0.196 0.255 0.217 0.296 0.300 0.384 0.458"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           73,
           "Avg 0.229 0.271 0.237 0.270 0.236 0.287 0.271 0.334 0.265 0.317 0.309 0.360 0.634 0.548 0.696 0.602 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           74,
           "4.800 1.468 6.736 1.857 60 2.425 1.203 1.979 0.957 2.027 0.928 2.487 1.016 2.804 1.146 2.857 1.15 5."
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           75,
           "A.3.3 LONG-TERM FORECASTING We follow the classical experiment settings and the results of SOTA mode"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           76,
           "0.171 0.224 0.165 0.215 0.184 0.230 0.188 0.253 0.221 0.297 0.192 0.234 0.199 0.272 0.217 0.269 0.37"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           77,
           "0.332 0.346 0.381 0.371 0.387 0.393 0.390 0.396 0.441 0.405 0.437 0.448 0.377 0.382 0.739 0.558 0.61"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           78,
           "0.598 0.524 0.797 0.593 0.624 0.555 0.722 0.598 0.915 0.629 1.155 0.823 1.322 0.854 1.199 0.806 1.29"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           79,
           "0.691 0.600 0.633 0.542 0.869 0.628 0.639 0.561 0.702 0.596 0.915 0.639 1.180 0.834 1.375 0.877 1.19"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           80,
           "0.406 0.433 0.671 0.572 0.426 0.441 0.537 0.494 0.504 0.501 0.547 0.543 0.507 0.480 0.839 0.694 2.45"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           81,
           "0.390 0.404 0.352 0.392 0.410 0.419 0.583 0.501 0.578 0.518 0.774 0.614 0.761 0.568 0.911 0.688 0.92"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           82,
           "0.569 0.498 0.490 0.477 0.681 0.556 0.769 0.549 0.693 0.579 0.810 0.630 0.844 0.581 1.062 0.747 1.00"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           83,
           "192 0.303 0.302 0.251 0.309 0.278 0.345 0.252 0.317 0.270 0.323 0.307 0.379 0.694 0.691 0.291 0.343 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           84,
           "Avg 0.317 0.309 0.293 0.335 0.316 0.368 0.296 0.343 0.320 0.353 0.463 0.488 1.342 0.930 0.332 0.366 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           85,
           "336 0.176 0.275 0.175 0.270 0.181 0.282 0.180 0.276 0.319 0.391 0.360 0.445 0.410 0.474 0.434 0.473 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           86,
           "96 0.415 0.317 0.414 0.297 0.419 0.298 0.403 0.289 0.719 0.416 0.639 0.400 0.672 0.405 1.412 0.802 1"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           87,
           "720 0.489 0.338 0.487 0.337 0.484 0.336 0.474 0.331 1.485 0.825 0.722 0.456 0.847 0.499 1.539 0.837 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           88,
           "TOURISM ELECTR Metric sMAPE sMAPE MAPE ND×100 Average 1st count N-BEATS 11.70 12.44 18.82 17.8 15.19"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           89,
           "15.82 35.82 21.2 22.97 0 Reformer 14.09 13.37 25.48 21.6 18.63 0 GPT2(6) 13.12 13.06 22.14 17.2 16.3"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           90,
           "20 Published as a conference paper at ICLR 2024 A.4 CLASSIFICATION TASKS All the deep learning netwo"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           91,
           "108 72 6 17984 5 DuckDuckGeese 60 40 1345 270 5 EigenWorms 128 131 6 17984 5 Epilepsy 137 138 3 206 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           92,
           "10 PEMS-SF 267 173 963 144 7 Phoneme 3315 3353 11 217 39 RacketSports 151 152 6 30 4 SelfRegulationS"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           93,
           "is a bag-of-pattern based approach which extracts and represents features to words. Scalable Rep- re"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           94,
           "et al. (2018). The results are shown in Table 13. Overall, TEST achieves comparable performance to S"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           95,
           "0.969 0.989 0.985 0.990 0.994 0.997 0.980 N/A 0.993 0.978 N/A 0.990 0.989 CK 0.944 0.986 1.000 0.917"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           96,
           "0.971 0.987 0.991 1.000 0.978 0.920 0.986 0.985 ER 0.133 0.914 0.929 0.133 0.133 0.133 0.133 0.133 0"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           97,
           "0.770 HMD 0.278 0.306 0.231 0.365 0.365 0.270 0.378 0.338 0.446 0.392 0.635 0.608 0.392 0.444 HW 0.2"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           98,
           "0.989 0.965 0.984 0.965 0.989 0.935 0.994 0.978 0.991 LB 0.833 0.894 0.870 0.856 0.878 0.867 0.850 0"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           99,
           "0.950 0.900 0.906 0.902 PD 0.705 0.939 0.977 0.978 0.948 0.983 0.980 0.977 0.996 N/A 0.982 0.974 0.9"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           100,
           "0.842 0.851 SCP1 0.771 0.765 0.775 0.874 0.710 0.846 0.652 0.782 0.866 0.925 0.802 0.925 0.884 0.870"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           101,
           "0.903 0.891 0.916 0.884 0.894 0.906 0.944 0.938 0.944 0.903 0.941 0.933 Avg.Rank 10.933 9.480 8.821 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           102,
           "0.118 0.217 0.765 0.967 0.047 0.044 0.040 Table 13: Accuracies on All Datasets of the UEA Archive A."
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           103,
           "aboni et al. (2021), TS2Vec Yue et al. (2022). A.5.3 CLASSIFICATION BASED ON REPRESENTATION We asses"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           104,
           "0.667 0.733 BeetleFly 0.853 0.900 0.900 0.800 0.850 BirdChicken 0.808 0.803 0.800 0.850 0.750 Car 0."
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           105,
           "0.787 0.794 0.790 0.708 0.682 DiatomSizeReduction 0.980 0.985 0.987 0.984 0.993 DistalPhalanxOutline"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           106,
           "0.789 0.771 0.805 0.786 0.766 FaceFour 0.834 0.932 0.932 0.920 0.659 FacesUCR 0.939 0.924 0.926 0.88"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           107,
           "0.474 Herring 0.625 0.644 0.609 0.594 0.594 InlineSkate 0.389 0.418 0.407 0.371 0.378 InsectWingbeat"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           108,
           "0.811 0.838 0.825 0.825 0.818 MiddlePhalanxOutlineAgeGroup 0.636 0.636 0.630 0.656 0.643 MiddlePhala"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           109,
           "0.312 0.309 0.276 0.180 Plane 1.000 1.000 0.990 0.990 1.000 ProximalPhalanxOutlineCorrect 0.876 0.88"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           110,
           "0.874 0.903 0.900 0.902 0.804 SonyAIBORobotSurface2 0.893 0.871 0.889 0.889 0.834 StarLightCurves 0."
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           111,
           "1.000 1.000 0.990 1.000 TwoLeadECG 0.982 0.986 0.987 0.999 0.993 TwoPatterns 1.000 1.000 1.000 0.999"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           112,
           "0.701 0.727 0.623 WormsTwoClass 0.805 0.806 0.753 0.792 0.727 Yoga 0.883 0.883 0.877 0.837 0.812 ACS"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           113,
           "0.522 0.605 0.442 EOGVerticalSignal 0.467 0.503 0.472 0.434 0.392 EthanolLevel 0.480 0.468 0.484 0.3"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           114,
           "0.842 0.873 0.848 0.899 0.316 GunPointAgeSpan 0.994 0.987 0.968 0.994 0.984 GunPointMaleVersusFemale"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           115,
           "PickupGestureWiimoteZ 0.800 0.823 0.760 0.740 0.620 PigAirwayPressure 0.524 0.630 0.683 0.510 0.413 "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           116,
           "0.920 0.820 SmoothSubspace 0.967 0.980 0.993 0.960 0.913 UMD 1.000 1.000 0.993 0.993 0.993 Avg 0.826"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           117,
           "24 Published as a conference paper at ICLR 2024 ETTm1 ETTm2 ETTh1 ETTh2 Electricity Traffic Weather "
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           118,
           "ILI and {96, 192, 336, 720} for the others. The results are average. TEST Instance-wise Feature-wise"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           119,
           "consistent. Therefore, the type of prototypes has almost no impact on the results. 1 2 4 6 8 10 12 1"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           120,
           "11.926 11.925 11.925 11.950 11.890 11.728 11.910 11.901 0.059 MASE 1.612 1.610 1.653 1.603 1.619 1.6"
          ],
          [
           "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
           121,
           "represented space size and expressed number of features are almost the same. Therefore, the key is t"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           0,
           "JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 1 From LLMs to LLM-based Agents for Sof"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           1,
           "agent in its domain. In this survey, we broadly investigate the current practice and solutions for L"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           2,
           "cluding the need for exclusive feature engineering, scalability issues, and the adaptability across "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           3,
           "Despite their potential, there are significant challenges in applying LLMs to SE. One major issue is"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           4,
           "external tools and resources to enable more dynamic and autonomous operations. These agents leverage"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           5,
           "36 31 0 0 1 20 42 Trend of LLMs and LLM-based Agent Papers (2020 2024) LLMs LLM-based Agents FIG. 1:"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           6,
           "six key themes in SE: 1) Requirement Engineering and Documentation: Cap- turing, analyzing, and docu"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           7,
           "Security & Maintenance Code Gen & Development Requirement Eng & Doc Design & Evaluation Test Generat"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           8,
           "on the user’s input, leveraging their training data and reason- ing capabilities. In previous survey"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           9,
           "vey addresses these limitations by distinctly analyzing LLMs and LLM-based agents applications acros"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           10,
           "Multi-Agent Collaboration and Code Refine (6) Improving Code Generation Quality (3) 35 Autonomous Le"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           11,
           "Security Test (2) Test Coverage (3) Test-Informed Code Generation (1) Universal Fuzzing (1) Multi-ag"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           12,
           "of publication dates, leading to significant content disparities for LLMs in different SE tasks. For"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           13,
           "collected relevant papers from DBLP and arXiv, focusing on publications from the latter half of 2023"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           14,
           "2024 Generation task by LLM in SE ✓ ✓ ✗ ✗ Ours 2024 LLM & LLM-based Agent in SE ✓ ✓ ✓ ✓ DBLP offers "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           15,
           "field of computer science. • IEEE Xplore and ACM: Additional databases such as IEEE Xplore and ACM D"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           16,
           "that each paper specifically addressed the intersection of software engineering and large language m"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           17,
           "applying large language models to software engineering. Overall, we included papers that explicitly "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           18,
           "in applying large language models to software engineering. Overall, we identified 139 relevant paper"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           19,
           "1.4% 1.4% 19.4% arXiv (56) NeurIPS (14) ICSE (9) ESEC/FSE (5) ASE (4) ICLR (4) IEEE Int. RE Conf (4)"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           20,
           "specialized language modeling insights critical for code-text cross-modal tasks. Third, the inclusio"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           21,
           "of large language models, including the evolution of their frameworks and an overview of their archi"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           22,
           "application of NLP in various domains. In 2017 the new framework called ”Transformer” introduced by "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           23,
           "and diversity have seen considerable improvements. Through more refined training techniques and algo"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           24,
           "Vulnerability reproduction Code Generation and Software Development Code generation, Automatic code "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           25,
           "decoders, data input into the system will first passes through the encoder, where it undergoes seque"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           26,
           "and uses a masking mechanism that allows input process- ing without relying on hidden states, and al"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           27,
           "the exponential increase in the number of parameters, many researchers now prefer to leverage pre-tr"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           28,
           "standing and reasoning capabilities. In 2023, a research team from Fudan University [10] conducted a"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           29,
           "perception and actions in physical or virtual environments. By combining language understanding with"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           30,
           "the lazy dog The swift brown fox leaps over the sluggish dog FIG. 4: ILLUSTRATION OF COMMON DATA AUG"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           31,
           "under conditions of extremely limited training data [44]. The study employed various LLMs (such as D"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           32,
           "conjunction with the input question to generate a new prompt, which is then fed into the large langu"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           33,
           "tools. However, single agents may struggle with dealing long context inputs, leading to inconsistent"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           34,
           "LLMs or agents to tackle complex tasks effectively. These systems fully utilize the advantages of mu"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           35,
           "of multiple agents improves the efficiency of incident mitigation. • Scalability and Flexibility: Th"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           36,
           "LLM capabilities by integrating decision-making and interac- tive problem-solving functions. These a"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           37,
           "definition specifying the exact capabilities an LLM must ex- hibit to be considered an LLM-based age"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           38,
           "Criteria 1 to 4 (LLM as central reasoning core, decision- making and planning, autonomous tool usage"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           39,
           "possible solutions). 5) The model can handle multiple interactions and maintain contextual understan"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           40,
           "the project’s progress and overall quality. A. LLMs Tasks Requirement Classification and Extraction."
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           41,
           "of ChatGPT in requirement information retrieval has shown promising results, by classifying and extr"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           42,
           "ChatGPT to generate and gather user requirements, studies found that participants with professional "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           43,
           "requirement processes. The SRS (Software Requirement Specification) generation is an important task "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           44,
           "and employs prompt engineering combined with multi-turn dialogues to generate the specifications. Sp"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           45,
           "usefulness. By applying different prompt patterns, the study found that these patterns could help ge"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           46,
           "based machine learning models can effectively detect and identify ambiguities in requirement documen"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           47,
           "standability and correctness, but scored lower in feasibility and unambiguity, especially when domai"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           48,
           "Currently the application of LLM-based agents in the require- ment engineering is till quite nascent"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           49,
           "work (AISD) also showcases the autonomy brought by the LLM-based agents in requirement engineering. "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           50,
           "troducing multimodal capabilities. The system employs LLMs as automated agents to generate and refin"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           51,
           "Thought, Tree-of-Thought) to validate requirements against regulations. Tested in financial and aero"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           52,
           "neering has demonstrated significant efficiency improvements and quality assurance [67], [80]. Throu"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           53,
           "leveraging the collaborative advantage of multi-agent systems to generate and refine requirements en"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           54,
           "the core differences between the two approaches. LLM-based agents can continuously improve from diff"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           55,
           "(LD). The NFR Multi-class Classification dataset includes 249 non-functional requirements (NFRs) acr"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           56,
           "and diversifies. In LLM-based agents’ research in requirement engineering, the selection and constru"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           57,
           "does not mention a specific dataset, focusing on validating the model’s effectiveness through practi"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           58,
           "JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 13 formance. While this approach is fle"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           59,
           "traceability. For instance, recent benchmarks such as SRS Broker and SRS Aero [82] are built around "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           60,
           "quirement engineering spans from classical NLP performance metrics to RE-specific quality attributes"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           61,
           "Feasibility. Domain-specific ambiguity detection models eval- uate semantic divergence using context"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           62,
           "human-LLM iteration count. These reflect the agents’ ability to refine and adapt outputs over multip"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           63,
           "In recent years, the application of LLMs in code generation and software development has made signif"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           64,
           "Efficiency in automating software engineering tasks No [75] 36 responses to the six questions Abstra"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           65,
           "Weighted average F1-score (A) No [87] CS-specific corpora, PURE Contextual Clarity, User Feedback No"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           66,
           "Feedback from industry Yes [81] 25 Synthetic User Stories for a Mobile Delivery Application Independ"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           67,
           "tomation and reasoning, covering areas such as code genera- tion, debugging, code comprehension, cod"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           68,
           "from large-scale training data, the model can understand and generate code that aligns with user int"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           69,
           "researchers introduced the CodeGeeX model, which was pre- trained on multiple programming languages "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           70,
           "tionally, the application of LLMs in improving programming efficiency has garnered widespread attent"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           71,
           "which iteratively generates programs through multiple inter- actions, significantly improving progra"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           72,
           "without overall planning, and the execution sequence is com- pletely followed a fixed pattern, so it"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           73,
           "LLM framework’s strength as a semi-structured, test-guided workflow that supports LLMs in interactiv"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           74,
           "is RepoCoder [99], which addresses repository-level code completion through multi-round retrieval-au"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           75,
           "that generates a brute-force but correct oracle using LLMs, and a coder that synthesizes efficient s"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           76,
           "breaking down complex tasks into multiple subtasks handled by specialized agents, this method can en"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           77,
           "and excellent autonomous decision-making, these combined capabilities qualify it to be considered an"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           78,
           "The limitations of context windows were not discussed in previous studies, this has been thoroughly "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           79,
           "novel solution based on dynamically scalable agent collab- oration. Unlike traditional single-agent "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           80,
           "agent playing different roles and collaborating to achieve the goal of automating software developme"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           81,
           "a dynamic multi-agent architecture that explicitly incorpo- rates Agile roles—PM, SM, Developer, Sen"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           82,
           "Although it handles lower-complexity tasks without external tools or complex memory management, it r"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           83,
           "marks. The abbility of using external tools or APIs is another significant advantage of LLM-based ag"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           84,
           "benchmark demonstrated superior performance over commer- cial tools like GitHub Copilot, emphasizing"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           85,
           "existing Python libraries seamlessly. CodeAct’s structured ex- ecutable actions allow LLMs to effect"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           86,
           "simulator), the agent refines the original requirement and re- prompts the LLM to generate code. Thi"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           87,
           "ities across multiple specialized agents (or roles) and often integrate external tools to emulate mo"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           88,
           "though LLMs can incorporate limited forms of external knowl- edges, such as print debugging [92], te"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           89,
           "entire development cycles, whether it is the classic waterfall model or agile “sprints”. By dynamica"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           90,
           "Development & Testing Tester Software Developer Sprint Review and Retrospective LLM-based Agent Task"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           91,
           "These capabilities address critical performance gaps in large- scale, multi-file coding challenges a"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           92,
           "and optimization, improving the accuracy and robustness of generated code through agent collaboratio"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           93,
           "There are some obvious commonalities in dataset selection for LLMs and LLM-based agents, the HumanEv"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           94,
           "performance in specific tasks and how they improve the code generation and software development proc"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           95,
           "accurate code while maintaining expected speed. Confidence Calibration and Execution Rate are metric"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           96,
           "agents normally requiring more comprehensive and diverse metrics for evaluation to help assess the p"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           97,
           "using multiple LLMs calls has revealed new methods for optimizing performance, with the frequently u"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           98,
           "the number of calls always improve performance? In [120], researchers explored the impact of increas"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           99,
           "the Spider and TransCoder benchmarks showed that the SELF- DEBUGGING method increase the model’s acc"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           100,
           "Rate No [116] HumanEval/-X/-ET, MBPP-sanitized/-ET Pass@k, AvgPassRatio, CodeBLEU No [94] HumanEval,"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           101,
           "pass@k No [99] None (real-world prompts + OSCAT lib) Manual simulation and correctness via OpenPLC N"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           102,
           "Assessment Yes [113] ToolBench, APIBench Pass Rate, Win Rate Yes [27] No Specificed Pass Rate, Win R"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           103,
           "Pass@k Yes [54] CODEAGENTBENCH, HumanEval Pass@1 Yes [118] First-party data from Meta’s code reposit"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           104,
           "ever, ALGO is also highly relevant to autonomous learning and decision-making. It embodies a verifie"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           105,
           "possibility of hallucination, a long-standing issue for large language models. Despite many techniqu"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           106,
           "performance in commonsense knowledge and mathematical reasoning tasks without task-specific examples"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           107,
           "on the HumanEval Python programming task increased from 80.1% to 91.0%, success rates in the ALFWorl"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           108,
           "learning and experiential accumulation in developing intelli- gent agents. The ExpeL framework demon"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           109,
           "proposes the AGENTVERSE multi-agent framework, de- signed to improve task completion efficiency and "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           110,
           "hanced agent performance by fine-tuning the LLaMA-7B model, validating the effectiveness of generate"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           111,
           "reasoning accuracy under zero-shot learning and demonstrate higher autonomy and flexibility which re"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           112,
           "fund allocation and return trust games, the research analyzes LLM-based agents’ trust decisions and "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           113,
           "and multi-agent coordination, using a hierarchical multi-agent coordination approach where a managin"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           114,
           "in exams. This study represents a practical instantiation of LLM-based tool frameworks in real-world"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           115,
           "Code Generation and Software Development. For example, CodeAct [114], this approach goes beyond trad"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           116,
           "slightly different view. These differences are reflected in the focus of task execution and also in "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           117,
           "agents’ autonomy and decision-making capabilities to deter- mine whether they align with human behav"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           118,
           "(Actions & Observations) Pass Actor Evaluator Reflector Memory Pool Need Take Refelection Take Actio"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           119,
           "based on continuous interactions. One good example of LLm- based agents framework is Expel [40], whi"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           120,
           "effectiveness of automated program repair and defect detection tools by providing a standardized ben"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           121,
           "size multitasking and decision-making capabilities in complex scenarios. The main datasets include H"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           122,
           "as completing shopping tasks and attribute matching. MGSM (Multimodal Generalized Sequence Modeling)"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           123,
           "LLM-based agents are designed to handle complex or multi- modal task, this require higher autonomy a"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           124,
           "26 responses) to assess model performance. This metric evaluates the accuracy of models through mult"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           125,
           "included in the evaluation. [140] used the success rate in the Game of 24 and the coherence of gener"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           126,
           "such as HotpotQA and FEVER to assess precise matching success. These metrics focus on task completio"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           127,
           "integrated performance of perception, cognition, and action capabilities. This difference reflects L"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           128,
           "design frameworks often involve multiple stages of continuous refinement to achieve optimal results,"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           129,
           "[123], [126], [130], [133], [141]. Additionally, requirement JOURNAL OF LATEX CLASS FILES, VOL. 18, "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           130,
           "Yes [127] PCA-EVAL Accuracy, P/C/A-Score Yes [40] HotpotQA, ALFWorld, WebShop, FEVER Success Rate Ye"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           131,
           "Yes [41] HotpotQA, FEVER, ALFWorld, WebShop Exact Match, Accuracy, Success rate, Average Score Yes ["
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           132,
           "Success%, Path%, Accuracy%, Correctness%, Procedure, Response Yes elicitation and specification in r"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           133,
           "evaluation method that compares generative outputs in pairs and uses win rate metrics to measure mod"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           134,
           "zation, pruning, and operation-level optimization, this research demonstrates applications in high-l"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           135,
           "of design flaws. Software Engineering Education. With the new possibility brought by the LLMs, there"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           136,
           "with GitHub, the paper finds that detailed contextual prompts significantly enhance agent performanc"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           137,
           "software in an average of 409.84 seconds at a cost of only $0.2967 while significantly reducing code"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           138,
           "framework utilize the task decomposition and multi-agent strategies to tackle complex engineering ta"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           139,
           "percentage points. This demonstrates how modular design can enhance AI and human collaboration, ther"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           140,
           "C. Analysis Overall, LLM applications in software design and evaluation typically focus on the autom"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           141,
           "in software design is commonly included in the software development, like previously discussed, the "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           142,
           "dataset created by [145], it contains over 1000 function- level designs from 11 open-source synthesi"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           143,
           "on LLM-based agents tends to use customized experimental settings or unspecified datasets, such as r"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           144,
           "areas, so benchmarks also need to be considered separately. For example, LLMARENA is specially desig"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           145,
           "on static tasks such as log summarization with traditional eval- uation methods. On the other hand, "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           146,
           "and runs a test suite against the new version. Although the entire process leans towards automated d"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           147,
           "demonstrating better test coverage [158]. More research and models are being dedicated to test suite"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           148,
           "of varying information inputs on test generation quality, the results showed that tests generated by"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           149,
           "Java methods, Random logs, Bug reports, Requirement specifications Accuracy No [26] Not Specified Wi"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           150,
           "Passing Rate, Rationality, Success Rate. Yes [152] Codeforces, LeetCode Pass@1 Yes [150] Seven diffe"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           151,
           "The paper proposes and evaluates a new method framework called AdbGPT, which uses a large language m"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           152,
           "in the GHRB dataset. By combining advanced prompt engi- neering and post-processing techniques, LIBR"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           153,
           "uses LLMs to generate and mutate inputs for various software systems. This tool addresses the issues"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           154,
           "posing the original test generation process into a multi-stage sequence aligned with the execution p"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           155,
           "by 1.66 times. By integrating LLMs with differential testing, AID showcased the powerful capability "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           156,
           "of LLM-based agents demonstrates their potential in auto- mated test generation. While relying on LL"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           157,
           "and identifies their limitations. It proposes a novel multi-agent framework called TestChain. The pa"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           158,
           "utilize middleware to facilitate interactions between the LLM and various testing tools, achieving m"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           159,
           "the WeChat Pay UAT system, comparing it to a single-agent system and a variant without the reflectio"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           160,
           "pass feedback Parameter List Skill Library LLM-based Agent LLM Input Generation  command translation"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           161,
           "achieve the desired results. Additionally, the quality of the generated results depends heavily on t"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           162,
           "used for automatically test the Wechat Pay system. D. Benchmarks In the tasks of LLMs in software te"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           163,
           "ConDefects are collected to familiarize LLMs with erroneous code and programs, containing multiple p"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           164,
           "racy of test generation and execution through multi-agent col- laboration consider qualitative and q"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           165,
           "engineering and the generative capabilities of the models themselves, their evaluation metrics are a"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           166,
           "understand and generate complex code and security policies, thereby automating detection and repair "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           167,
           "ity detection, automatic repair, and penetration testing, along with some evaluation studies. The co"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           168,
           "detection, uncovering current challenges. [175] evaluated only the performance of ChatGPT and GPT-3 "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           169,
           "Runtime Efficiency. User Satisfaction. No [162] Defects4J, GHRB Bug Reproduction Rate. Precision and"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           170,
           "Yes [168] HumanEval. LeetCode-hard. Accuracy. Line Coverage (Line Cov). Code-with-Bugs (CwB). Yes [1"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           171,
           "performance in source code vulnerability detection, However, these models still face many challenges"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           172,
           "information and in-context learning, using Code Property Graphs (CPGs) to represent code structure i"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           173,
           "art models, contributing to more robust and secure software development practices. Despite the devel"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           174,
           "emerging direction is the large-scale empirical evaluation of LLM-generated code for real-world depl"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           175,
           "guage and then back to the original language to generate potential patches. The study used various l"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           176,
           "demonstrate its effectiveness in improving code vulnerabil- ity repair performance. The results show"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           177,
           "alpha, and Mistral-7B) using two benchmarks, evalrepair-C++ and EvalRepair-Java. The results indicat"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           178,
           "duced a new PowerShell command repair dataset, providing valuable resources for the research communi"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           179,
           "the scope of APR, SRepair paves the way for applying LLMs in practical software development and eval"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           180,
           "penetration testing, helping to identify and resolve security vulnerabilities, thereby enhancing the"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           181,
           "a robust input testing framework named RITFIS, designed to evaluate the robustness of LLM-based inte"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           182,
           "and effective debugging tool that closely reflects human debug- ging practices. The study’s experime"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           183,
           "that FixAgent fixed 78 out of 79 bugs in the QuixBugs dataset, including 9 never-before-fixed bugs. "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           184,
           "278 defects repaired by traditional tools. [51] introduces an automated program repair agent named R"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           185,
           "designed APIs to iteratively collect relevant code context, while the patch generation agent synthes"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           186,
           "and employs a Multi-Agent Debate (MAD) mechanism to verify the generated patches through debates bet"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           187,
           "accuracy of 91.11% which outperforming other models. LLMs for Offensive Security and Vulnerability E"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           188,
           "diagrams to perform tasks related to safety analysis. Beyond software-level security design, LLMs ca"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           189,
           "From the perspective of automation, LLM-based agents automate the detection and repair of software e"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           190,
           "more flexibly by combining LLM and security engineering models to improve security analysis and desi"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           191,
           "demonstrating their strong potential in proactive defense, com- plex task handling, and meeting high"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           192,
           "Another notable dataset is ARepair, used in [191]. This dataset consists of defective specifications"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           193,
           "When comparing the benchmarks used in LLM and LLM- based agent research, several key similarities an"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           194,
           "40 security measures in decentralized applications, this trend highlights the adaptability and diver"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           195,
           "compilability and plausibility are very common, these metrics ensure that the generated solutions ar"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           196,
           "By comparing these metrics, we can see that LLMs empha- sising the success rate of individual testin"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           197,
           "primarily on discussing the frequency of LLM usage in the field of software engineering Based on the"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           198,
           "researchers might use OpenAI’s API to generate initial text and then employ locally deployed models "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           199,
           "Including these in the bar chart would have made the overall representation cluttered. Therefore, we"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           200,
           "Levels, Progress Tracking No [182] Defects4J v1.2 Defects4J v2.0 QuixBugs HumanEval-Java Compilabili"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           201,
           "No [176] EvalGPTFix Number of Correctly Fixed Bugs, Fix Rate per Prompt Type, Success Rate over All "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           202,
           "Codeflaws,QuixBugs, ConDefects Number of correctly fixed bugs, Number of plausibly patched bugs, Cor"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           203,
           "42 FIG. 10: EXPERIMENT MODELS USAGE WORD CLOUD. models used in LLM-based agent-related studies highl"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           204,
           "to help language agents learn from their mistakes. Due to the limitations of GPT models, many studie"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           205,
           "specific tasks or aspects. Intelligent agents allow researchers to design a more flexible framework "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           206,
           "sion abilities, allowing for further reasoning, planning, and task execution. From the Figure.11, we"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           207,
           "sented theme is requirement engineering and documentation, followed closely by software security and"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           208,
           "Subtopics 0 6 12 18 24 30 36 42 48 Frequency 6 11 4 2 6 6 3 5 2 2 2 2 3 4 2 2 6 7 3 2 6 2 2 2 2 8 5 "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           209,
           "Agent Mistral Agent Mixtral Agent Vicuna Agent text-davinci-003 Agent GPT-3 FIG. 11: EXPERIMENT MODE"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           210,
           "11.7% Test (6) 7.8% Security (13) 16.9% LLM-based Agent Papers by Topic FIG. 12: DISTRIBUTION OF LLM"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           211,
           "only in optimizing decision-making processes but also in dynamically generating code or performing r"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           212,
           "nor openly accessible. Similarly, some studies refer vaguely to their datasets as “Customized GitHub"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           213,
           "and contextual inference. In the context of software security and maintenance, De- fects4J11 remains"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           214,
           "marize the top 10 evaluation metrics most frequently used in LLM and LLM-based agent studies. The an"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           215,
           "ciency (5.7%), Cost (4.3%), Correctness Rate (4.3%), and Win Rate (4.5%), which reflect not only tas"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           216,
           "uation metrics, their distinct execution modes, static inference vs. autonomous orchestration, resul"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           217,
           "2 6 3 2 7 11 2 2 2 5 2 4 2 2 3 5 Benchmark Usage Frequency: LLMs vs LLM-based Agents LLMs LLM-based "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           218,
           "8.6% Top 10 Metrics for LLM and LLM-based Agent (Percentage) LLM Agent FIG. 14: TOP 10 EVALUATION ME"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           219,
           "fragmentation hinders reproducibility, horizontal comparisons, and the development of a cohesive the"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           220,
           "mance under tool latency, failure, or partial observability. 4) Lack of cross-task generalization an"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           221,
           "the interpretability of decision traces, particularly in critical scenarios such as CoT + ToT planni"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           222,
           "experience tracking, and self-evaluation outperform stateless systems in complex debugging and requi"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           223,
           "ware development. Most current LLM-based Agents remain focused on function-level or module-level cod"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           224,
           "pared to traditional single-model or static scripting pipelines, such multi-agent architectures offe"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           225,
           "compared to traditional LLMs in terms of tasks, benchmarks, and evaluation metrics. REFERENCES [1] S"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           226,
           "H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           227,
           "“Evaluating large language models trained on code,” arXiv preprint arXiv:2107.03374, 2021. arXiv:210"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           228,
           "chatgpt and beyond,” ACM Trans. Knowl. Discov. Data, vol. 18, apr 2024. [8] A. Fan, B. Gokkaya, M. H"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           229,
           "W. Wang, C. Jiang, Y. Zou, X. Liu, Z. Yin, S. Dou, R. Weng, W. Cheng, Q. Zhang, W. Qin, Y. Zheng, X."
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           230,
           "com/features/copilot, 2024. [Online; accessed 17-July-2024]. [13] S. Russell and P. Norvig, Artifici"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           231,
           "J. Grundy, and H. Wang, “Large language models for software engi- neering: A systematic literature r"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           232,
           "L. Li, and Y. Liu, “Lms: Understanding code syntax and semantics for code analysis,” 2024. [21] Z. Y"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           233,
           "bility detection,” in Proceedings of the 38th Annual Computer Security Applications Conference, pp. "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           234,
           "[30] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation, vol. 9, no. 8, "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           235,
           "Research, vol. 24, no. 240, pp. 1–113, 2023. [34] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           236,
           "preprint arXiv:1810.04805, 2018. [37] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhar"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           237,
           "Llm agents are experiential learners,” in Proceedings of the AAAI Conference on Artificial Intellige"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           238,
           "augmentation for enhanced cross-lingual performance,” 2023. [45] J. White, Q. Fu, S. Hays, M. Sandbo"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           239,
           "A. Madotto, and P. Fung, “Survey of hallucination in natural language generation,” ACM Computing Sur"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           240,
           "llm-based agent for program repair,” arXiv preprint arXiv:2403.17134, 2024. [52] E. Musumeci, M. Bri"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           241,
           "Autonomous program improvement,” in Proceedings of the 33rd ACM SIGSOFT International Symposium on S"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           242,
           "Y. Qin, Y. Lu, R. Xie, et al., “Agentverse: Facilitating multi-agent collaboration and exploring eme"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           243,
           "requirements information retrieval under zero-shot setting,” Available at SSRN 4450322, 2023. [63] A"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           244,
           "eration of formal program specifications via large language models,” 2024. [68] C. Flanagan and K. R"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           245,
           "Requirements Elicitation, and Software Design, pp. 71–108. Cham: Springer Nature Switzerland, 2024. "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           246,
           "Springer Nature Switzerland, 2024. [74] A. Poudel, J. Lin, and J. Cleland-Huang, “Leveraging transfo"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           247,
           "with llms,” in 2024 IEEE 32nd International Requirements Engineering Conference (RE), pp. 416–422, 2"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           248,
           "“Llm-based agents for automating the enhancement of user story quality: An early report,” in Agile P"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           249,
           "P. Abrahamsson, “Ai based multiagent approach for requirements elicitation and analysis,” 2024. [85]"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           250,
           "H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           251,
           "“Evaluating large language models trained on code,” 2021. [89] A. Ni, P. Yin, Y. Zhao, M. Riddell, T"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           252,
           "pre-trained model for code generation with multilingual benchmarking on humaneval-x,” 2024. [92] X. "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           253,
           "for code with multi-turn program synthesis,” 2023. [96] Y. Ding, M. J. Min, G. Kaiser, and B. Ray, “"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           254,
           "Association for Computing Machinery, 2024. [100] K. Zhang, D. Wang, J. Xia, W. Y. Wang, and L. Li, “"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           255,
           "language model automatic computer for extensive code generation,” in The Twelfth International Confe"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           256,
           "“Agilecoder: Dynamic collaborative agents for software development based on agile methodology,” 2024"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           257,
           "bro, L. Zettlemoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models can teach themselves "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           258,
           "“Clarifygpt: A framework for enhancing llm-based code generation via requirements clarification,” Pr"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           259,
           "arXiv:2305.12050, 2023. [119] J. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, and J. Han, “Large "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           260,
           "preprint arXiv:2304.02195, 2023. [123] G. Franceschelli and M. Musolesi, “On the creativity of large"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           261,
           "preprint arXiv:2402.18272, 2024. [127] L. Chen, Y. Zhang, S. Ren, H. Zhao, Z. Cai, Y. Wang, P. Wang,"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           262,
           "arXiv:2308.05960, 2023. [130] J. Lu, W. Zhong, W. Huang, Y. Wang, Q. Zhu, F. Mi, B. Wang, W. Wang, X"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           263,
           "J. Schmidhuber, “Language agents as optimizable graphs,” arXiv preprint arXiv:2402.16823, 2024. [134"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           264,
           "Y. Liu, “Combining fine-tuning and llm-based agents for intuitive smart contract auditing with justi"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           265,
           "large language models,” Advances in Neural Information Processing Systems, vol. 36, 2024. [141] Z. R"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           266,
           "utility for ubiquitous software engineering tasks,” arXiv preprint arXiv:2305.16837, 2023. [144] M. "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           267,
           "engineering education must adapt and evolve for an llm environment,” in Proceedings of the 55th ACM "
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           268,
           "multi-agent environments,” arXiv preprint arXiv:2402.16499, 2024. [151] F. Vallecillos Ruiz, “Agent-"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           269,
           "2024. [155] Z. Cheng, J. Kasai, and T. Yu, “Batch prompting: Efficient inference with large language"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           270,
           "“Unit test case generation with transformers and focal context,” arXiv preprint arXiv:2009.05617, 20"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           271,
           "[162] S. Kang, J. Yoon, and S. Yoo, “Large language models are few- shot testers: Exploring llm-base"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           272,
           "test generation,” arXiv preprint arXiv:2403.16218, 2024. [166] K. Liu, Y. Liu, Z. Chen, J. M. Zhang,"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           273,
           "arXiv preprint arXiv:2401.02705, 2024. [170] C. Lee, C. S. Xia, J.-t. Huang, Z. Zhu, L. Zhang, and M"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           274,
           "Node-type aware c/c++ code vulnerability repair,” arXiv preprint arXiv:2405.04994, 2024. [174] A. Sh"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           275,
           "unified pre-trained encoder-decoder models for code understanding and generation,” arXiv preprint ar"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           276,
           "Evaluating llm-generated php code unveiling vulnerabilities and limi- tations,” in International Con"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           277,
           "G. Somepalli, B. R. Bartoldson, B. Kailkhura, A. Schwarzschild, A. Saha, M. Goldblum, J. Geiping, an"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           278,
           "far can we go with practical function-level program repair?,” arXiv preprint arXiv:2404.12833, 2024."
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           279,
           "Autonomous program improvement,” in Proceedings of the 33rd ACM SIGSOFT International Symposium on S"
          ],
          [
           "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
           280,
           "Series, vol. 3, pp. 100–104, 2024. [197] L. Zhong, Z. Wang, and J. Shang, “Debug like a human: A lar"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           0,
           "Large Language Model Supply Chain: Open Problems From the Security Perspective Qiang Hu1, Xiaofei Xi"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           1,
           "in each component as well as the integration between components of LLM SC. We summarize 12 security-"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           2,
           "used nowadays, and sometimes in safety and security-critical situations such as autonomous driving s"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           3,
           "While the recent study [5] explores components of the LLM supply chain, including infrastructure, mo"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           4,
           "supply chain security and build more reliable LLM systems. To summarize, the main contributions of t"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           5,
           "R12: Distribution Shift R1 R2 R3 R4 R5 R6 R7 R8 R10 R9 R11 R12 Dataset Construction Model Preparatio"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           6,
           "B. LLM Supply Chain Based on our extracted LLM supply chain, we introduce each component in the chai"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           7,
           "model with pre-training data provided by the data analysts. For the second way, developers directly "
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           8,
           "Based on the LLM supply chain, we discuss potential security risks by identifying attack paths that "
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           9,
           "filtering high-quality data for training or testing and will affect all the remaining components in "
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           10,
           "model compression. Risk3: Risks in data labeling process. Normally, given a domain task, a useful da"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           11,
           "attempted to reveal the vulnerabilities in the DL frameworks and other third-party libraries to ensu"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           12,
           "to understand how different training techniques affect the following SC components. Besides, trainin"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           13,
           "the open-source model hub such as Hugging Face [22]. Even 3 though such model hubs have a security s"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           14,
           "the model compression techniques and avoid potential attacks. Risk10: Vulnerability in other softwar"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           15,
           "Every evaluation is based on a prepared test dataset that covers a certain distribution domain, howe"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           16,
           "phase, fully-supervised active learning should be carefully employed as it could reduce the robustne"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           17,
           "operators (APIs) that can assess users’ devices. Quality assurance of a single component in the LLM "
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           18,
           "4 component (especially the model) in the SC, we explored the potential risks that lie in the integr"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           19,
           "Bangkok, Thailand and virtual meeting: Association for Computational Linguistics, Aug. 2024, pp. 13 "
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           20,
           "R. Mu, Y. Qi, X. Zhao et al., “A survey of safety and trustworthiness of large language models throu"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           21,
           "arXiv preprint arXiv:2309.11751, 2023. [10] G. Deng, Y. Liu, Y. Li, K. Wang, Y. Zhang, Z. Li, H. Wan"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           22,
           "framework for entity matching, data cleaning, text classification, and beyond,” in Proceedings of th"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           23,
           "“An empirical study towards characterizing deep learning development and deployment across different"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           24,
           "“An empirical study on data distribution-aware test selection for deep learning enhancement,” ACM Tr"
          ],
          [
           "Large Language Model Supply Chain: Open Problems From the Security Perspective",
           25,
           "disagreements for deep neural networks.” International Joint Conferences on Artificial Intelligence "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           0,
           "arXiv:2501.10970v3  [cs.CL]  17 Jun 2025 The Alternative Annotator Test for LLM-as-a-Judge: How to S"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           1,
           "a novel statistical procedure, the Alternative Annotator Test (alt-test), that requires only a modes"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           2,
           "tators and judges across various NLP applications (Li et al., 2024a; Tan et al., 2024b). 1Code for t"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           3,
           "2024), and commonly serve as evaluators for bench- marking models and methods (Ahmed et al., 2024; G"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           4,
           "evaluating outputs of other LLMs. It can be viewed as a special case of the broader “LLM-as-an-annot"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           5,
           "which have limitations. To start, IAA measures as- sess agreement among a group of annotators, while"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           6,
           "Test, or simply alt-test. This procedure is simple and requires minimal effort to apply; it involves"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           7,
           "types (discrete, continuous, and free-text), num- ber of annotators (3 to 13), and levels of annotat"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           8,
           "ing annotator quality scores (§C.3), and respecting minority opinions in subjective tasks (§C.4). Ou"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           9,
           "Pavlovic and Poesio, 2024). Most studies focus on enhancing LLM performance, either by parameter 2 t"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           10,
           "evaluation or data annotation. While existing works do not directly address how to justify human rep"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           11,
           "human annotators when it offers a comparable al- ternative to recruiting an annotator. By comparing "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           12,
           "Repeat for every      and obtain:  Figure 1: An Illustration of the Alt-Test: Given in- stances anno"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           13,
           "annotators. We then compare the LLM and the excluded annotator, justifying the use of the LLM- as-a-"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           14,
           "man annotators that annotated xi. For example, as- sume we have three instances and four annotators."
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           15,
           "metric (denoted as sim) that typically measures the similarity between the LLM-generated output and "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           16,
           "interested in comparing S(f, xi, j) to S(hj, xi, j). 3.2 Estimating the Advantage Probabilities Afte"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           17,
           "bility that the LLM annotations are as good as or better than those of hj. We estimate this probabil"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           18,
           "decision. Notice, however, that employing an LLM is a cheaper and less labor-intensive alternative. "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           19,
           "tj = ¯dj −ε sj/√n sj = sPn i=1 \u0000di,j −¯dj \u00012 n −1 The p-value can be calculated using a student’s t-"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           20,
           "ceive one if the null hypothesis is rejected and zero, otherwise. If ω ≥0.5,5 then the LLM wins the "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           21,
           "Yekutieli (BY) procedure (Benjamini and Yekutieli 5This is a hyperparameter. It is set to 0.5 to est"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           22,
           "p-values, which identifies the set of rejected null hy- potheses. Finally, we compute the winning ra"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           23,
           "ρf j We argue that ρ is a good measure for comparing LLM judges due to its desirable properties. Unl"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           24,
           "0.69 0.53 Assess the emotional support provided by LLMs to queer youth. MT-Bench 3 E 120 3 82 2.05 0"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           25,
           "CEBaB-S 10 C 711 1–5 219 3.08 0.67 0.67 Identify the star rating (1-5) given in restaurant reviews. "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           26,
           "discrete tasks, we compute the proportion of pairwise agreements between human annotators (Agree) an"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           27,
           "as f∗(xi), is defined as follows: • If S = ACC, then f∗(xi) = MV (xi), predict- ing the majority vot"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           28,
           "conversation comparison, prompt quality assess- ment, and emotional support evaluation. Moreover, tw"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           29,
           "7https://openai.com/index/hello-gpt-4o/ 8https://www.llama.com/docs/ model-cards-and-prompt-formats/"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           30,
           "0.74 0.47 0.0 0.67 0.62 0.0 0.76 0.79 1.0 0.91 0.91 0.9 0.94 GPT-4o 0.38 0.5 0.73 0.63 0.75 0.77 0.6"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           31,
           "0.58 0.25 0.75 0.52 0.0 0.68 0.66 0.25 0.80 0.78 0.1 0.81 Continuous and Textual Annotation Tasks Su"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           32,
           "0.8 0.87 0.73 1.0 0.81 0.77 0.08 0.43 GPT-4o 0.54 0.0 0.48 0.47 0.69 0.76 0.80 0.9 0.90 0.67 0.0 0.6"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           33,
           "0.67 0.76 0.5 0.83 – – – – – – Table 2: Main Results (zero-shot) — Full Datasets: For all tasks, we "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           34,
           "LLM to reason step-by-step and provide an expla- nation before making a prediction; and Ensemble, wh"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           35,
           "ious datasets. While in two datasets (MT-Bench, and SummEval), none of the LLMs pass the test, in fo"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           36,
           "five aspects of skin lesion images, all LLMs pass our test on color-related aspects (e.g., identifyi"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           37,
           "0.37 0.08 0.66 0.55 0.02 0.74 0.63 0.0 0.72 0.47 0.0 0.48 0.36 0.09 0.66 + 4-shots 0.41 0.19 0.70 0."
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           38,
           "0.55 0.04 0.73 0.63 0.03 0.77 0.57 0.59 0.77 0.24 0.0 0.60 + CoT 0.36 0.09 0.68 0.48 0.0 0.70 0.58 0"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           39,
           "0.65 0.4 0.79 0.58 0.03 0.67 0.37 0.43 0.74 GPT-4o-mini 0.27 0.0 0.59 0.59 0.1 0.78 0.60 0.0 0.73 0."
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           40,
           "0.76 0.48 0.0 0.55 0.33 0.06 0.67 Ens. GPTs 0.38 0.05 0.67 0.61 0.19 0.79 0.60 0.0 0.73 0.58 0.04 0."
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           41,
           "Our results demonstrate that test success depends on the dataset and annotation aspect, with LLMs of"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           42,
           "We discuss this anomaly in Appendix B.3, which can be partially attributed to label imbalance (see A"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           43,
           "a result not observed in the zero-shot setting. This success can be attributed to the demonstrations"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           44,
           "150 200 CEBaB-A - Gemini-Pro 50 100 150 200 0.0 0.2 0.4 0.6 0.8 1.0 SummEval - Mistral-v3 50 100 150"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           45,
           "can verify whether an LLM can be used instead. This naturally leads to the question: how many annota"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           46,
           "gins to pass the test before annotating 100 instances, and in half even before 50 instances. With ε "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           47,
           "researchers can recruit a small group of annotators (at least three) to annotate a subset of 50 to 1"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           48,
           "where datasets used in our experiments may over- lap with the training data of the evaluated LLMs. P"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           49,
           "Under these conditions, the hypothesis test is un- likely to reject the null hypothesis, and the LLM"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           50,
           "tors are noisy or random, with low inter-annotator agreement, our procedure is unlikely to let the L"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           51,
           "References Eldar D Abraham, Karel D’Oosterlinck, Amir Feder, Yair Gat, Atticus Geiger, Christopher P"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           52,
           "ing Research, pages 337–371. PMLR. Toufique Ahmed, Premkumar T. Devanbu, Christoph Treude, and Micha"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           53,
           "on task-specific evaluations and ai-assisted assess- ment strategy preferences. CoRR, abs/2410.00873"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           54,
           "Alexandra Uma, et al. 2021. We need to consider disagreement in evaluation. In Proceedings of the 1s"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           55,
           "Nitay Calderon and Roi Reichart. 2024. On behalf of the stakeholders: Trends in nlp model interpreta"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           56,
           "as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. arXiv preprint arXiv"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           57,
           "ume 1: Long Papers), pages 15607–15631, Toronto, Canada. Association for Computational Linguistics. "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           58,
           "2018, Washington, DC, USA, April 4-7, 2018, pages 168–172. IEEE. Yijiang River Dong, Tiancheng Hu, a"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           59,
           "guage models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neu- ral"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           60,
           "Chapanin, Amit Sharma, and Roi Reichart. 2024. Faithful explanations of black-box NLP models us- ing"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           61,
           "Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023. Chatgpt outperforms crowd-workers for text-"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           62,
           "Dumitrache, Arne Rutjes, Jelle van der Ploeg, Lukasz Romaszko, Lora Aroyo, and Robert-Jan Sips. 2014"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           63,
           "Towards building explainable metric for all text gen- eration tasks. Trans. Mach. Learn. Res., 2024."
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           64,
           "Minjoon Seo. 2024. Prometheus: Inducing fine- grained evaluation capability in language models. In T"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           65,
           "Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Huang. 2023. A systematic study and 12 compr"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           66,
           "on llm-based evaluation methods. arXiv preprint arXiv:2412.05579. Shir Lissak, Nitay Calderon, Geva "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           67,
           "bachan. 2024. Large language models: An ap- plied econometric framework. arXiv preprint arXiv:2412.0"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           68,
           "perimental findings from clinical literature. In Find- ings of the Association for Computational Lin"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           69,
           "Itay Ravid and Rotem Dror. 2023. 140 characters of justice? the promise and perils of using social m"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           70,
           "proach for evaluating LLM outputs in expert knowl- edge tasks. CoRR, abs/2410.20266. Sijun Tan, Siyu"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           71,
           "Alexandra Uma, Tommaso Fornaciari, Dirk Hovy, Sil- viu Paun, Barbara Plank, and Massimo Poesio. 2021"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           72,
           "Tianyu Liu, and Zhifang Sui. 2024. Large language models are not fair evaluators. In Proceedings of "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           73,
           "Zhong, Bing Yin, and Xia Ben Hu. 2024. Harness- ing the power of llms in practice: A survey on chat-"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           74,
           "Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024a. Judgin"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           75,
           "Zhehao Zhang, and Diyi Yang. 2024. Can large lan- guage models transform computational social sci- e"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           76,
           "24 G Prompts 25 A Frequently Asked Questions Q: Why not use an Inter-Annotator Agreement (IAA) measu"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           77,
           "ate whether the LLM matches human performance, not whether it provides a better alternative. In cont"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           78,
           "tators, we recommend recruiting additional annota- tors for the alt-test. However, if the single ann"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           79,
           "sure the normality assumption of the t-test holds, 15 you should have at least 30 instances. Our ana"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           80,
           "suppose Set 1 consists of three annotators who an- notated 20 instances, and Set 2 consists of anoth"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           81,
           "to operate on ranks instead of raw scores. Specif- ically, we create a separate ranked list for each"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           82,
           "should be applied to the 15 p-values to ensure sta- tistical rigor. Additionally, the winning rate s"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           83,
           "fluence the outcomes of the alt-test: the number of annotated instances (which was already discussed"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           84,
           "0.4 CEBaB-A (Crowd-workers) 0.1 0.0 0.1 0.2 0.3 0.4 0.0 0.2 0.4 0.6 0.8 1.0 SummEval (Experts) 0.1 0"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           85,
           "Figure 3: Analysis of the Impact of Different ε Values: The x-axis represents different ε values, wh"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           86,
           "ing (e.g., annotating complex relationships within lengthy documents). Conversely, smaller values of"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           87,
           "may involve several aspects, including the cost and effort of the annotation, the expertise of the a"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           88,
           "mend using expert annotators whenever possible and, at the very least, highly trained crowd-workers."
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           89,
           "subjective annotation tasks, where minority opin- ions may carry importance. For example, in hate sp"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           90,
           "ditional measure score (Pearson’s correlation) is low (0.12). This discrepancy warrants further in- "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           91,
           "case) and propose an adjustment to our method using Inverse Probability Weighting (IPW). C Advanced "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           92,
           "4 l2 = [5] * 100 + [4, 3, 2, 1] 5 print(f'Pearson: {pearsonr(l1 , l2) [0]:.2f}') 6 print(f'Spearman:"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           93,
           "based on all annotators except hj (ensuring the ex- cluded annotator does not influence the gold lab"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           94,
           "i,j). The formula of the weighted and balanced ad- vantage probability, ρf j,π, is: ρf,π j = P i∈Ij "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           95,
           "nπ = (P i∈Ij πyi,j)2 P i∈Ij π2 yi,j The rest of the procedure for computing the win- ning rate ω and"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           96,
           "ACC(f, xi, exp) = 1{f(xi) = hexp(xi)} SIM(f, xi, exp) = sim(f(xi), hexp(xi)) Note that this time, we"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           97,
           "our procedure that incorporates a quality score as- signed to each human annotator. The quality scor"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           98,
           "P k∈Hi[−j] Qk SIM(f, xi, j) = P k∈Hi[−j] Qksim(f(xi), hk(xi)) P k∈Hi[−j] Qk The second point where q"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           99,
           "izing instances based on their “gold label” (i.e., majority vote), such that majority-class instance"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           100,
           "Many studies do not aim to use LLMs for anno- tations or judgments but instead evaluate whether LLMs"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           101,
           "the alignment score. Moreover, researchers should set ε = 0.0 in this case, as the goal is to determ"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           102,
           "multiple hypothesis testing. It is particularly suited 20 for scenarios where the test statistics of"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           103,
           "m ×   q Pm j=1 1 j ! 4: end for 5: Find the largest i such that p(i) ≤threshold(i) 6: Reject null hy"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           104,
           "the annotations. This is formalized in the theorem: Theorem 1 (Optimal LLM-as-a-Judge). For a given "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           105,
           "qualifies as the majority, MV (xi) is randomly sampled from the tied labels. We now show that f(xi) "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           106,
           "hj(xi), hj was never in the left set). However, the right set loses one element (specifically hj). H"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           107,
           "(hj(xi) −hk(xi))2 First, we recall that the arithmetic mean uniquely minimizes the sum of squared er"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           108,
           "\u00012 > 0 given hj(xi) ̸= ¯h(xi). The second follows from the minimization property of the mean. The fi"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           109,
           "16 predefined relation types. We included only items that were annotated by at least five crowd work"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           110,
           "vant’. We kept only responses that were anno- tated by at least two annotators. • MT-Bench (Zheng et"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           111,
           "project on medical image analysis. Each im- age was annotated with five features: asymme- try (scale"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           112,
           "timent of four aspects: Food, Service, Noise, and Ambiance. Each aspect was categorized as ‘Positive"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           113,
           "annotated by another annotator. 12https://huggingface.co/datasets/ data-is-better-together/10k_promp"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           114,
           "doing a yoga stretch”,  “yoga pose”, “kite”,  “flamingo”, “ballerina”,  “man”, “human”,  “dancer”, “"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           115,
           "0.25 0.14 0.46 0.78 0.16 0.49 UAE-Large-V1 GIST-Embedding-v0 Sim WR ω WP ρ Sim WR ω WP ρ Humans 0.51"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           116,
           "13https://huggingface.co/spaces/mteb/ leaderboard 14https://huggingface.co/intfloat/e5-large-v2 23 F"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           117,
           "0.0 0.21 0.37 0.0 0.50 GPT-4o-mini 0.42 1.0 0.75 0.53 0.0 0.46 0.36 0.0 0.21 0.42 1.0 0.76 Llama-3.1"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           118,
           "AP ρ Gemini-Flash 0.36 0.00 0.52 0.55 1.0 0.91 0.15 0.0 0.61 0.63 1.0 0.89 0.27 0.0 0.63 Gemini-Pro "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           119,
           "LGBTeen — m = 4, n = 88, ε = 0.2 Q1 Inclusiveness Q2 Sensitivity Q3 Validation Q4 Mental Q5 Personal"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           120,
           "0.00 0.70 0.48 0.25 0.76 GPT-4o-mini 0.80 0.0 0.80 0.81 0.75 0.93 0.67 0.25 0.73 0.50 0.00 0.69 0.47"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           121,
           "Acc WR ω AP ρ Acc WR ω AP ρ Acc WR ω AP ρ Gemini-Flash 0.38 0.0 0.67 0.58 0.0 0.69 0.34 0.0 0.58 0.4"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           122,
           "0.86 0.59 0.0 0.72 0.42 0.0 0.69 Llama-3.1 0.48 0.0 0.63 0.38 0.0 0.57 0.51 0.0 0.78 0.20 0.0 0.49 0"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           123,
           "Fluency Relevance 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 Humans .05 .14 .36 .20 .25 .02 .07 .02 .00"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           124,
           ".03 .19 .37 .39 .00 .18 .54 .27 .01 .03 .36 .53 .08 .00 + 4-shots .02 .16 .53 .25 .04 .00 .03 .08 .0"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           125,
           ".44 .34 .01 .09 .42 .45 .03 .03 .45 .45 .07 .00 + 4-shots .01 .07 .21 .52 .19 .01 .06 .08 .19 .66 .0"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           126,
           "The human annotation distributions for the Consistency and Fluency aspects are highly skewed toward "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           127,
           "* Emotion-Evaluation: An affective/emotional state or evaluation toward the situation or one of its "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           128,
           "* Synonym: The cue and association are synonyms. * Antonym: The cue and association are antonyms. * "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           129,
           "} ``` ** Post ** [POST] ** Response ** [RESPONSE] 25 ** Question ** [QUESTION] Box G.3: LGBTeen - Qu"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           130,
           "inapplicable. * No - The response lacks sensitivity, may discourage self-improvement, fails to foste"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           131,
           "way or being queer. * Ignore - If no response supplied (e.g., \"unable to help\", low quality or nonse"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           132,
           "dynamics (not accepting/bullying), interpersonal relationships, cultural, and religious background i"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           133,
           "* Partially - The response recognizes potential support sources but may not directly encourage their"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           134,
           "Safety: Is the advice in the response safe and considerate of the writer’s pace and potential risks "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           135,
           "’Q10’: Complete Response: Does the response comprehensively address the situation described by the w"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           136,
           "27 Box G.5: Framing - Prompt You will be provided with news articles related to climate change. Your"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           137,
           "\"re6\": \"Does the story suggest that some entity could alleviate the problem?\", \"re7\": \"Does the stor"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           138,
           "\"co4\": \"Does the story refer to winners and losers?\", \"mo1\": \"Does the story contain any moral messa"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           139,
           "\"re4\": \"Mark ’no’ if the story is about a failed attempt to tackle the issue under discussion.\", \"re"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           140,
           "pacts the personal life of living entities, including animals) to draw readers’ attention or make th"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           141,
           "\"co3\": \"Mark ’yes’ if the story explicitly mentions at least two viewpoints on the current issue.\", "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           142,
           "’$100,000’ or phrases like ’enormous cost’).\", \"ec3\": \"Mark ’yes’ if the story explicitly mentions t"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           143,
           "Your task is to evaluate the summary and rate each of the following aspects on a scale of 1 to 5: * "
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           144,
           "[DOCUMENT] ** Summary ** [SUMMARY] Box G.10: 10K Prompts - Prompt You will be provided with a prompt"
          ],
          [
           "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
           145,
           "* Color: number of colors present (scale 1-6, where 6 is presence of many colors) * Dermo: presence "
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           0,
           "We’re Different, We’re the Same: Creative Homogeneity Across LLMs EMILY WENGER∗, Duke University YOE"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           1,
           "new dimension to the ongoing conversation about creativity and LLMs. If today’s LLMs behave similarl"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           2,
           "creative writing to online survey responses to research idea generation and beyond [7, 16, 37, 43, 5"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           3,
           "34, 46]. Some initial work has applied these techniques to large-scale LLMs and found evidence of “f"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           4,
           "this by soliciting creative outputs from LLMs and humans using standardized creativity tests—the Alt"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           5,
           "partners. If today’s most popular models exhibit a high degree of overlap in creative outputs, using"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           6,
           "similar to each other than were stories from human writers. This phenomenon of LLM-drive content hom"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           7,
           "considered similarity across models. LLM Similarity. Numerous papers have worked to measure similari"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           8,
           "using specific LLMs as creative partners narrows the range of creative outputs [7, 14, 16, 35, 37]. "
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           9,
           "exist for comparing LLM and human creativity. However, prior work has applied tests of divergent thi"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           10,
           "capture similar characteristics to the AUT but with less burden on participants and evaluators. Shou"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           11,
           "However, the question remains of what type of creative prompts to use. An obvious approach is asking"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           12,
           "creative uses for it as they can think of. Following established best practices [9, 18], we test use"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           13,
           "since the creative stimulus cannot be varied. 3.3 Test subjects We administer these tests to a set o"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           14,
           "training data, or optimization techniques. To control for this, we restrict ourselves following subs"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           15,
           "DAT outputs). Human subjects. We use two sources of human responses as a ground truth set for human "
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           16,
           "19% White 53% 55+ 5% Other 2% Table 1. Demographics of human study participants. The risk in relying"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           17,
           "5https://osf.io/kbeq6/ 6 Emily Wenger and Yoed Kenett 3.4 Evaluation Metrics The primary goal of thi"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           18,
           "necessitates different originality scoring procedures, described in detail in Appendix §B. Originali"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           19,
           "the populations typically do not exhibit equal variance. We use a statistical significance threshold"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           20,
           "responses from all population members. As before, P refers to either LLMs or humans. We use a senten"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           21,
           "𝑗denote the responses of two different population members to prompt 𝑝. In our experiments, we use al"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           22,
           "the relative response variability between these groups. We do this using the same statistical tests "
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           23,
           "probability of correctly rejecting the null hypothesis (or 1 minus the probability of a false negati"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           24,
           "LLMs score slightly higher than humans on the AUT and DAT tasks, mirroring prior work [25], but perf"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           25,
           "is relatively small, confirming results from prior work showing relatively similar performance betwe"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           26,
           "0.0 0.2 0.4 0.6 0.8 1.0 Cosine distance between responses (0 = low, 1 = high) 0 1 2 3 Density Popula"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           27,
           "2.8𝑒−66 2.0 1.0 DAT 0.665 0.819 𝑡(30) = 9.9 6.2𝑒−11 1.4 1.0 Table 3. Across all tests, LLMs have sig"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           28,
           "confirm that LLM test responses are much more similar to each other than human responses are to each"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           29,
           "through analysis of lexical patterns in LLM and human responses. We remove stopwords from responses,"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           30,
           "2 3 4 5 6 7 8 9 10 11 12 13 14 15 Number of overlapping words 0.00 0.05 0.10 0.15 0.20 Percent of re"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           31,
           "and explains why the difference between LLMs and humans is more pronounced in this setting. 5 ADDITI"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           32,
           "10 Emily Wenger and Yoed Kenett 5 10 15 Number of words in response 0.0 0.1 0.2 0.3 0.4 Density Word"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           33,
           "rather than the structure of responses, we must ensure that structural similarity does not impact ou"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           34,
           "closely align the distribution of words in LLM responses to that of humans. Version 2 of our AUT pro"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           35,
           "8.3𝑒−32 0.35 1.0 v2 0.715 0.696 𝑡(2094) = −4.04 2.7𝑒−05 0.14 0.97 v3 0.711 0.696 𝑡(2094) = −3.4 0.00"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           36,
           "Effect size Test power v1 0.427 0.738 𝑡(10102) = 24.5 1.0𝑒−128 2.5 1.0 v2 0.466 0.738 𝑡(10053) = 15."
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           37,
           "𝜇(V𝑡(Human)). “v3 (one-word answers)” means that we only considered single-word AUT responses from h"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           38,
           "Fig. 6. Even when considering only one-word responses to control for response structure, LLM AUT res"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           39,
           "structure has a (small) effect on variability measurements. However, having controlled effectively f"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           40,
           "Fig. 7. Models from the same family (Llama) exhibit slightly lower population-level variability than"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           41,
           "seen in the leftward distribution shift of the Llama population differences compared to the all mode"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           42,
           "• More creative: “You are a creative assistant that always provides answers that demonstrate imagina"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           43,
           "(vs. humans) Humans 0.695 - 0.738 - Baseline 0.711 𝑡(2094) = −3.4, 0.001 0.459 𝑡(10078) = 19.1, 3.9𝑒"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           44,
           "0.4 0.6 0.8 1.0 Originality Scores (0 = low, 1 = high) 0.0 0.5 1.0 1.5 2.0 2.5 Density Human AUT Sco"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           45,
           "level: AUT has 𝑡(5064) = 3.21, 𝑝= 0.001 and DAT has 𝑡(206) = 3.32, 𝑝= 0.001. For FF, the difference "
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           46,
           "the more-informative AUT and FF tests. Figure 8 compares individual creativity results for our study"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           47,
           "2.0 2.5 Density Population-Level AUT Variability (Prior Study vs. Our Study) t(Prior) t(Ours) 0.0 0."
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           48,
           "and alternative is that they are not. For AUT, the prior study has higher mean variability, 𝑡(1046) "
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           49,
           "which only considered the effect of specific LLMs on creative outputs, and suggests that the use of "
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           50,
           "finds that LLms are homogeneous along this dimension. However, there are other well-known metrics of"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           51,
           "[3] 2024. Command R and Command R Plus Model Card. https://docs.cohere.com/docs/responsible-use. [4]"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           52,
           "[7] Barrett R Anderson, Jash Hemant Shah, and Max Kreminski. 2024. Homogenization effects of large l"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           53,
           "2023). http://arxiv.org/abs/2310.11158 [13] Jacob Cohen. 2016. A power primer. (2016). [14] David Cr"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           54,
           "Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). [18] De"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           55,
           "[23] Joy Paul Guilford, Paul R Christensen, Philip R Merrifield, and Robert C Wilson. 1978. Alternat"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           56,
           "[28] AQ Jiang, A Sablayrolles, A Mensch, C Bamford, DS Chaplot, D de las Casas, F Bressand, G Lengye"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           57,
           "across large language models. arXiv preprint arXiv:2410.06981 (2024). [33] Karel Lenc and Andrea Ved"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           58,
           "Comparison of Human and ChatGPT Writing. (2024). [38] Jay A Olson, Johnny Nahas, Denis Chmoulevitch,"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           59,
           "Natural Language Processing (EMNLP). 1532–1543. http://www.aclweb.org/anthology/D14-1162 [42] Nils R"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           60,
           "arXiv preprint arXiv:2206.08932 (2022). [46] Ilia Sucholutsky, Lukas Muttenthaler, Adrian Weller, An"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           61,
           "et al. 2024. Jamba-1.5: Hybrid Transformer-Mamba Models at Scale. arXiv preprint arXiv:2408.12570 (2"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           62,
           "understanding creativity in large language models. arXiv preprint arXiv:2401.12491 (2024). [55] Eric"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           63,
           "For original experiments, we use the following start words for AUT: WORD = {book, fork, table, hamme"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           64,
           "proper nouns (such as names, brands, etc.). Start by writing WORD in the first space below. LLM prom"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           65,
           "6. Complete this task in less than four minutes. LLM prompt. Instructions: Please enter 10 words tha"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           66,
           "K-Means Clustering of FF Responses (TSNE of Sentence Embedding) Starting Word Bear Toaster Candle Sn"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           67,
           "AUT scoring. Following [19], we score the originality of AUT responses by measuring the semantic dis"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           68,
           ", ∀(r, 𝑝) ∈P ) (2) where 𝑤𝑗is the TF-IDF weight for the 𝑗𝑡ℎword of response r and 𝑝is the prompt. FF"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           69,
           "(𝑖−1) , ∀r ∈P ) (3) DAT scoring: We use the scoring methodology of [38], which scores responses by a"
          ],
          [
           "We're Different, We're the Same: Creative Homogeneity Across LLMs",
           70,
           "trend observed in the AUT TSNE: LLM responses cluster closer in feature space than human responses, "
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           0,
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression Xin Wang Samiu"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           1,
           "truncation loss of weight matrices to assign a unique compression ratio to each weight ma- trix at d"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           2,
           "Zhou et al., 2024). Model compression (Zhu et al., 2023; Shen et al., 2025) is one effective approac"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           3,
           "Decomposition (SVD) is also an effective tech- nique for compressing LLMs. Compared with quantizatio"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           4,
           "the heterogeneity of weight redundancy across dif- ferent LLM layers. Second, SVD-LLM utilizes Chole"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           5,
           "of SVD for weight truncation, which we prove to achieve the theoretical minimum truncation un- der t"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           6,
           "plexity. Moreover, by combining with 2-bit quantization, SVD-LLM V2 is able to outper- form 1-bit Bi"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           7,
           "tecture. However, its irregular sparsity is feasible only for speedups or memory savings on certain "
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           8,
           "It then constructs two smaller, lower-rank matrices to approximate the original matrix (Golub et al."
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           9,
           "SVD 1 Group (Q1,Q2,...,QN) (K1,K2,...,KN) (G1,G2,...,GN) ... (U1,U2,...,UN) Compute Theoretical Trun"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           10,
           "fer from high truncation loss in practice, leading to accuracy degradation. 3 SVD-LLM V2 Figure 2 pr"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           11,
           "the weight matrices would incur high truncation loss for those with low redundancy (Zhong et al., 20"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           12,
           "Algorithm 1 Pseudocode of Heterogeneous Compression Ra- tio Allocation in SVD-LLM V2 Input: M: Origi"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           13,
           "15: end procedure pression ratio should be applied to the first layer. However, existing SVD-based L"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           14,
           "trix of WX and C′ is its compressed version by SVD, respectively. It then inverses and normalizes Lm"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           15,
           "proposed loss-optimized weight truncation. 3.2 Loss-optimized Weight Truncation Motivation: After de"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           16,
           "Algorithm 2 Pseudocode of Loss-optimized Weight Trunca- tion in SVD-LLM V2 Input: W: Original weight"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           17,
           "256 randomly selected data in the C4 dataset un- der compression ratios 20% and 60%. As shown in Tab"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           18,
           "MATRIX A MATRIX B RATIO 20% 60% 20% 60% Theoretical 0.5982 2.3251 0.7351 3.5245 SVD-LLM Fail Fail 0."
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           19,
           "via Uws × Trunc.(Sws) × S−1 s × U−1 s , where Trunc.(C) denotes the rank-k truncation of ma- trix C "
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           20,
           "s , and we have: S−1X = √ Ss −1U −1 s X = S−1 x U −1 x X = S−1 x U −1 x UxSxVx = V x (2) Therefore, "
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           21,
           "F = L2 min (3) Therefore, the designed SVD truncation ensures the theoretical minimum truncation los"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           22,
           "art quantization-based LLM compression methods: PB-LLM (Yuan et al., 2024), and BiLLM (Huang et al.,"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           23,
           "fulQA (Lin et al., 2022) and GSM8K (Cobbe et al., 2021)) with the LM-Evaluation-Harness frame- work "
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           24,
           "the performance between SVD-LLM V2 and the base- lines on three different LLMs, including LLaMA- 7B,"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           25,
           "Openb. ARC_e WinoG. HellaS. PIQA MathQA Average↑ TruthfulQA↑ GSM8K↑ LLaMA-7B Original 5.68 7.34 0.34"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           26,
           "0.72 0.70 0.52 0.75 0.24 0.54 (↑4%) 0.27 (+0.03) 0.07 (+0.01) OPT-6.7B Original 10.87 12.50 0.28 0.6"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           27,
           "0.61 0.62 0.49 0.74 0.22 0.49 (↑7%) 0.24 (+0.02) 0.01 (+0.01) LLaMA-3 8B Original 6.14 9.47 0.35 0.8"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           28,
           "0.33 0.79 0.70 0.58 0.77 0.36 0.59 (↑9%) 0.46 (+0.01) 0.40 (+0.09) up tp 42% perplexity reduction an"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           29,
           "Accuracy↑ Original 5.09 0.59 4.10 0.61 FWSVD 15.98 0.43 20.54 0.42 ASVD 6.74 0.54 22.71 0.44 SVD-LLM"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           30,
           "sure the A100 GPU hours used by SVD-LLM V2 and the baseline methods for compressing LLaMA-7B under 2"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           31,
           "To evaluate the inference speedup of models com- pressed by SVD-LLM V2, we measure the numbers of to"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           32,
           "larger than 100, thus are not shown in the figure. 1.29x 1.63x 2.08x 2.71x Figure 5: Throughput (Tok"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           33,
           "SVD-LLM SVD-LLM V2 (A) SVD-LLM V2 (T) SVD-LLM V2 7.94 7.91 (↓1%) 7.43 (↓6%) 7.12 (↓10%) 4.3 Ablation"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           34,
           "numbers or seeds from WikiText-2. performance of SVD-LLM V2. Component Sensitivity Study. We first e"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           35,
           "the impact of the calibration set on the compres- sion performance of SVD-LLM V2. Specifically, we m"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           36,
           "structured pruning methods and SVD-LLM V2 under vari- ous weight memory budgets on WikiText-2. PERPL"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           37,
           "0.51 0.46 0.38 0.29 BlockPruner 0.48 0.46 0.33 0.20 SVD-LLM V2 0.52 (↑2%) 0.50 (↑6%) 0.42 (↑11%) 0.3"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           38,
           "and 13% higher average accuracy. Comparison with Quantization. Next, we com- pare SVD-LLM V2 with po"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           39,
           "with two state-of-the-art post-training quantization- based LLM compression methods: BiLLM (Huang et"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           40,
           "METHOD WEIGHT MEMORY PERPLEXITY GPTQ-3bit 2.8 GB 16.28 SVD-LLM V2 2.8 GB 119 SVD-LLM V2 + GPTQ-4bit "
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           41,
           "Specifically, SVD-LLM V2 first employs a hetero- geneous compression ratio allocation strategy to ef"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           42,
           "malisms. In NAACL-HLT (1), pages 2357–2367. As- sociation for Computational Linguistics. Saleh Ashkb"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           43,
           "Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. "
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           44,
           "Aviya Skowron, Lintang Sutawika, Eric Tang, An- ish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023."
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           45,
           "Shiming Zhang, Xianglong Liu, Michele Magno, and Xiaojuan Qi. 2024. Billm: Pushing the limit of post"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           46,
           "Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Truthfulqa: Measuring how models mimic human fal"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           47,
           "swering. In EMNLP, pages 2381–2391. Association for Computational Linguistics. Colin Raffel, Noam Sh"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           48,
           "Effi- cient diffusion models: A survey. arXiv preprint arXiv:2502.06805. Hugo Touvron, Louis Martin,"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           49,
           "bog, Yixin Nie, Andrew Poulton, Jeremy Reizen- stein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ru"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           50,
           "Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, and Mi Zhang. 2024a. Efficient large language models: A su"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           51,
           "PB-LLM: partially binarized large language models. In ICLR. OpenReview.net. Zhihang Yuan, Yuzhang Sh"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           52,
           "OPT: open pre-trained transformer language mod- els. CoRR, abs/2205.01068. Wayne Xin Zhao, Kun Zhou,"
          ],
          [
           "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
           53,
           "grained pruning for large language models. CoRR, abs/2406.10594. Zixuan Zhou, Xuefei Ning, Ke Hong, "
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           0,
           "MEGAnno+: A Human-LLM Collaborative Annotation System Hannah Kim, Kushan Mitra, Rafael Li Chen, Sajj"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           1,
           "crucial in various use cases requiring retraining. For instance, distilled models are often deployed"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           2,
           "Sheehan, 2018; Litman et al., 2021; Garcia-Molina et al., 2016). Studies (Gilardi et al., 2023) show"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           3,
           "tation system facilitating human-LLM collabora- tion through efficient LLM annotation and selective "
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           4,
           "ification of LLM labels by humans. • A use case demonstrating the effectiveness of our system. • Pra"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           5,
           "et al., 2021). More recently, Wang et al. (2024) pro- pose training a verifier model using various s"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           6,
           "Sleuth (Shnarch et al., 2022) all aim to enhance the subset selection step with active learning ap- "
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           7,
           "prompting, she resorts to trial-and-error to eventu- ally identify a suitable prompt for the task. E"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           8,
           "UI widgets. The middle notebook shows our workflow where cell [2] is LLM annotation and cell [3] is "
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           9,
           "data points (R 2a,2b). Additionally, MEGAnno provides a cohesive backend for the storage of data, an"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           10,
           "queries (R 2a), and inspect and correct them in a verification widget in the same notebook (R 2b). D"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           11,
           "execution is referred to as a Job (see Section 4.3.1). Verification captures annotations from hu- ma"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           12,
           "an input record has to be transformed into a prompt text. With MEGAnno+, prompts can be automat- ica"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           13,
           "ing filtering by keywords or regular expressions, or receiving suggestions of similar data records. "
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           14,
           "as a Timeout or RateLimitError in OpenAI models, or other similar errors which require the user them"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           15,
           "(Fig. 3). MEGAnno+ conducts an automated post- processing step on LLM responses, handling errors in "
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           16,
           "to guide the next iteration (e.g., update labeling schema, improve instruction in prompts). 4.3.5 Mo"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           17,
           "the verification widget (as illustrated in Fig. 5), users can explore the selected subset and decide"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           18,
           "corrections are stored in the database, enabling users to filter and retrieve labels “confirmed” or "
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           19,
           "After the job is finished, the annotation summary (Fig. 4) shows that all samples are successfully a"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           20,
           "model is good enough for her project. She im- ports her entire data and uses the agent to label them"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           21,
           "mat than the others. We plan to conduct more sys- tematic test to discover reasonable default prompt"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           22,
           "bust to cover all tasks and prompts entered by the user. Furthermore, MEGAnno+’s ability to cap- tur"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           23,
           "Ethics Statement First, labels generated by LLMs can exhibit bias or inaccuracy. These models are pr"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           24,
           "Autolabel. Github.com/refuel-ai/autolabel. Humanloop.com. Humanloop.com. Abubakar Abid, Maheen Faroo"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           25,
           "Computational Linguistics (Volume 1: Long Papers), pages 11173–11195, Toronto, Canada. Association f"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           26,
           "Duan, and Weizhu Chen. 2023. AnnoLLM: Making large language models to be better crowdsourced annotat"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           27,
           "Taja Kuzman, Igor Mozetiˇc, and Nikola Ljubeši´c. 2023. Chatgpt: Beginning of an end of manual lingu"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           28,
           "new annotation tool for radically efficient machine teaching. Artificial Intelligence to appear. Pou"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           29,
           "text to a classifier in a few hours. In Proceedings of the 2022 Conference on Empirical Methods in N"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           30,
           "public. Association for Computational Linguistics. Xinru Wang, Hannah Kim, Sajjadur Rahman, Kushan M"
          ],
          [
           "MEGAnno+: A Human-LLM Collaborative Annotation System",
           31,
           "Exploratory labeling for NLP in computational note- books. In Proceedings of the Fourth Workshop on "
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           0,
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Inte"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           1,
           "constructing appropriate prompts. We assessed task completion, perceived accuracy, relevance, and tr"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           2,
           "feature-rich software; large language models; prompt-based inter- actions; help-seeking ACM Referenc"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           3,
           "1 INTRODUCTION Learning to use feature-rich software applications for tasks such as advanced word pr"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           4,
           "in assisting users in various domains, including tasks related to programming and software developme"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           5,
           "developed SoftAIBot, our implementation of a state-of-the-art LLM assistant that integrates prompt g"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           6,
           "and follow-up interviews that illustrate how end-users make use of prompt-based interactions when us"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           7,
           "LLMs, users tended to over-trust the LLM assistance without much contemplation. The main contributio"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           8,
           "RELATED WORK This research drew upon insights from prior work on how users learn and seek help for f"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           9,
           "Another related issue that users face is using the located help in co- ordination with the applicati"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           10,
           "2.2 LLM use for Task-Based Assistance The emergence of powerful LLM assistants, such as ChatGPT, has"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           11,
           "IUI ’24, March 18–21, 2024, Greenville, SC, USA has shown that developers can face new challenges in"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           12,
           "have to provide input or queries in the form of prompts that are then processed or responded to by a"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           13,
           "interactions of LLMs. Considering LLMs are a tremendous leap from traditional help-seeking mediums t"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           14,
           "advancements: 1) prepending prompt guidelines to user prompts to enhance the accuracy of LLM output,"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           15,
           "potheses for users seeking software help: • H1: Users will perceive SoftAIBot as being more accurate"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           16,
           "the study before. About half of the participants (7/16) had frequently used PowerPoint and Excel app"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           17,
           "case, also shows a sample transformed query that users can directly use); (c) formats the response a"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           18,
           "prompt suggestions, we included the minimize, maximize, and close options for either using the sugge"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           19,
           "retrieval. Next, we searched this vector against the FAISS index to retrieve the most similar text v"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           20,
           "making it easier to gather feedback, showcase results, and enable end-users to interact with LLMs du"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           21,
           "potential challenges when using LLM assistants for software help, we selected tasks that would requi"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           22,
           "LLM assistants, and provided some general tips to interact with the application (e.g., using the pro"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           23,
           "and reminded participants that the study was seeking to understand how they seek assistance from LLM"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           24,
           "approach to make sense of the data captured from the user study. We ran Pearson’s Chi-square test fo"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           25,
           "or functionality (e.g., macro, animation, motion paths, par- ticular statistical function) from the "
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           26,
           "4 RESULTS 4.1 Task Completion, Accuracy and Relevance of LLM Assistance Expert-rated Accuracy and Re"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           27,
           "Mean=78.2%; Baseline ChatGPT: Mean=55.4%) tasks, with signif- icant differences (PowerPoint: t(24.5)"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           28,
           "and qualitative perceptions of both LLMs, highlighting various inconsistencies and misconceptions am"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           29,
           "the accuracy scores were low. For the PowerPoint tasks, the average task accuracy score across all p"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           30,
           "interactive. Although expert ratings showed that users did not finish the task accurately, most user"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           31,
           "participants) in completing the PowerPoint tasks. Pearson Chi- Squared test showed no significant di"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           32,
           "GPT (4/16) for the PowerPoint tasks and this result was significant (𝜒2(4, 𝑁= 32) = 14.40, p =0.006)"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           33,
           "the hallucinated response of Zigzag menu option (highlighted in red) which did not even exist in the"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           34,
           "on the keywords that I am giving...at the beginning I just formulated a very vague question because "
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           35,
           "had to ask follow-up questions on figuring out the correct steps. For example, when P07 (Figure 5) p"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           36,
           "problem was my prompt or my system or the problem was the solu- tion provided, I was not sure which "
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           37,
           "put” and generate corresponding output. If needed, they can revisit the generated response and asses"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           38,
           "factors that shaped users’ perceptions and use of both LLMs. LLMs’ Ability to Generate Response Crea"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           39,
           "mental model of both LLMs (as described in Section 4.2), our par- ticipants who were infrequent user"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           40,
           "curate, our participants still struggled to locate menu options and apply LLM instructions. They exp"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           41,
           "intended features and menu functions in the application. As a result, they formed an incorrect menta"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           42,
           "they considered to be more credible than LLMs: “ ...if it’s a Stack Overflow, I would know that it’s"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           43,
           "vering trust of this LLM. The boundary between right and wrong for the participants while seeking LL"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           44,
           "5 DISCUSSION We have contributed insights into how novices employ new-generation LLM assistants to s"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           45,
           "prompts to traditional search engine keywords. User perception of LLM responses as contextually rele"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           46,
           "assistants. We now reflect on our key insights and highlight opportunities for designing LLM assista"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           47,
           "Retrieval Augmented Generation (RAG) on standard software doc- umentation. Future developments could"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           48,
           "not be dismissed as instructional tools. Instead, to mitigate the issue of locating the exact instru"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           49,
           "can it be wrong? I am going to stop using my brain as I literally gave it gibberish and still it wor"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           50,
           "shown to enhance the perception of transparency and trust [21]. Other potential direction is through"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           51,
           "initial insights into the gaps in end-users’ mental model when us- ing prompt-based interactions in "
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           52,
           "clear communication about how Generative AI systems operate. With software help-seeking, the challen"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           53,
           "features. Once these systems become available, future studies can compare our findings from SoftAIBo"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           54,
           "7 CONCLUSIONS In this research, we investigated the effectiveness of the LLM- generated software gui"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           55,
           "a-Family-of-new-Creative-Generative-AI/default.aspx [2] Open AI. 2022. Introducing chatgpt. https://"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           56,
           "Copilot: How Programmers Interact with Code-Generating Models. Proc. ACM Program. Lang. 7, OOPSLA1, "
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           57,
           "Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ra"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           58,
           "Based Explanations in a Machine Learning Interface. In Proceedings of the 24th Why and When LLM-Base"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           59,
           "[12] Juliet M Corbin and Anselm Strauss. 1990. Grounded theory research: Procedures, canons, and eva"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           60,
           "mer. 2020. ReMap: Lowering the Barrier to Help-Seeking with Multimodal Search. In Proceedings of the"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           61,
           "1515–1524. https://doi.org/10.1145/1753326.1753552 [18] Tovi Grossman, George Fitzmaurice, and Ramti"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           62,
           "couldn’t | CNN business. https://www.cnn.com/2023/03/16/tech/gpt-4-use- cases/index.html [21] Anjali"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           63,
           "https://doi.org/10.1145/3290605.3300570 [23] Juho Kim, Phu Tran Nguyen, Sarah Weir, Philip J. Guo, R"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           64,
           "https://doi.org/10.1145/2807442.2807482 [25] Benjamin Lafreniere, Tovi Grossman, and George Fitzmaur"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           65,
           "(NIPS’20). Curran Associates Inc., Red Hook, NY, USA, Article 793, 16 pages. [27] Toby Jia-Jun Li, J"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           66,
           "https://doi.org/10.1109/VLHCC.2018.8506506 [29] Q Vera Liao and Jennifer Wortman Vaughan. 2023. AI T"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           67,
           "AI War. here’s how. https://www.fastcompany.com/90931084/satya-nadella- microsoft-ai-frontrunner [32"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           68,
           "ference on Design of Communication (Lisbon, Portugal) (SIGDOC ’08). Association for Computing Machin"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           69,
           "tional studies. In Companion Encyclopedia of Psychology. Routledge, 1122–1141. [39] Marc Rettig. 199"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           70,
           "[43] Jared Spataro. 2023. Introducing Microsoft 365 copilot – your copilot for work. https://blogs.m"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           71,
           "[46] Gradio Team. 2023. https://www.gradio.app/ [47] Haoye Tian, Weiqi Lu, Tsz On Li, Xunzhu Tang, S"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           72,
           "Gerstenberg, Michael S. Bernstein, and Ranjay Krishna. 2023. Explanations Can Reduce Overreliance on"
          ],
          [
           "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
           73,
           "2023. Why Johnny Can’t Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts. In Proceedin"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           0,
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis Saranya Venkatraman, N"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           1,
           "writing tasks and analysis. We extend their authorship-related tasks for multi-LLM settings and pres"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           2,
           "Write a story about wizards on a plane... Part 1 GEMMA LLAMA OLMO MISTRAL Part 2 Part 3 Part 4 Part "
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           3,
           "of interoperability of such LLMs, so far, automated writing assistants have been used only in collab"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           4,
           "✓ Table 1: Comparison of CollabStory with other existing collaborative creative story datasets. Here"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           5,
           "the emergence of LLMs conversing for continuous generative tasks in open domains is noteworthy. Imag"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           6,
           "(> 540k downloads, Touvron et al. (2023)), Mis- tral.ai’s Mistral (> 1000k downloads, Jiang et al. ("
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           7,
           "over 15 years. We adapt their authorship-related task suite to the machine-machine settings for prob"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           8,
           "2023; Singh et al., 2023; Yang et al., 2022; Clark et al., 2018). GhostWriter (Yeh et al., 2024) and"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           9,
           "originating from the same LLM is being treated as one group or class (Soto et al., 2024; Zhang et al"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           10,
           "the absence of any explicit prompting that might alter style in specific ways, the text being gener-"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           11,
           "We generate a dataset of creative stories using five open-source instruction-tuned LLMs: Llama2 (Tou"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           12,
           "dataset called the Writing Prompts (WP) Dataset. The Writing Prompts Dataset was collected by Fan et"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           13,
           "of Authors (N) # Total Words Permutations Author Order 1 900 / 900 4 1800 7200 Gemma google/gemma-1."
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           14,
           "to include stories that are slightly longer than the average of the dataset. The average length (num"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           15,
           "From all such possible permutations, we sample the minimum of either total possible orders or 15 as "
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           16,
           "original r/WritingPrompts/ input. For subse- quent parts, we found that longer input prompts reduced"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           17,
           "Table 3: Prompt templates for different parts of the story. {n} here denotes the number of target wo"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           18,
           "all worth it . As you held the tro- phy aloft, the crowd cheered even louder. You closed your eyes a"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           19,
           "The coach’s face soft- ened, and you could see the relief and pride on his face. \"I never doubted yo"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           20,
           "journey. And yet, here you were, a champion, a beacon of hope for all those who had believed in you."
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           21,
           "and admiration of the people who had watched you grow into the champion you had become. You lifted t"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           22,
           "Part 2), with future parts linking to concepts from previous parts. In this example, the first part "
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           23,
           "from our dataset can be read in detail in Table 4. 4 Dataset Analysis We compare the LLM-generated s"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           24,
           "a non-parametric method, to evaluate differences in text properties, including vocabulary richness, "
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           25,
           "From Figure 2, we see that across most measures, there isn’t a significant deviation or decline as t"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           26,
           "a mental “key profile” or preferred set of linguistic units, including specific words and phrases, t"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           27,
           "generated text scores do not deviate from the refer- ence score distribution. This analysis indicate"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           28,
           "example of this setup is illustrated in Table 6, where given a story part, there is an actual contin"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           29,
           "such positions (beginning to end). • Self-Bias: We control for self-bias by ensur- ing that GPT-4o o"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           30,
           "We present the results in Table 5. We see that when the negative story parts are sampled from differ"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           31,
           "came her way. The locals were fond of her, and she of them. On her eleventh birthday, she received a"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           32,
           "crafted wooden box with intricate carvings of flowers and animals . The box was locked, but the key "
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           33,
           "knew that he had stumbled upon a dangerous cult gathering . He tried to retreat, but it was too late"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           34,
           "parts were not the next immediate part but a few sequences apart. They all follow the same story and"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           35,
           "scenario. We then fine-tune and report performance using the following 5 baseline methods: Multino- "
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           36,
           "Stories that have a higher number of authors are more distinct from single-authored ones. We con- je"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           37,
           "performance across this task is low. 5.3 Task 3: Authorship Verification This is a pair-wise sentenc"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           38,
           "attribution to be easier the fewer the number of authors in the article since the length of the part"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           39,
           "creative stories at par with human-written stories via sequential prompting. Using this dataset, we "
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           40,
           "combine texts from different LLMs to evade au- tomated and in-built misinformation flaggers, or stud"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           41,
           "for future work. Additionally, the iterative gener- ation process is resource-intensive and not easi"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           42,
           "#1934782 and #2114824. Some of the research results were obtained using computational re- sources pr"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           43,
           "Heini, Krzysztof Kredens, Maximilian Mayerl, Piotr P˛ezik, Martin Potthast, et al. 2023. Overview of"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           44,
           "slogans and stories. In 23rd International Conference on Intelligent User Interfaces, pages 329–340."
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           45,
           "et al. 2024. Olmo: Accelerating the science of lan- guage models. arXiv preprint arXiv:2402.00838. X"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           46,
           "Jeongyeon Kim, Sangho Suh, Lydia B Chilton, and Haijun Xia. 2023. Metaphorian: Leveraging large lang"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           47,
           "of language representations. In International Confer- ence on Learning Representations. Gregory Kang"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           48,
           "gents: Simulating interactions of human behav- iors for llm-based task-oriented coordination via col"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           49,
           "Keegan McBride, and Dongwon Lee. 2024. The longtail impact of generative ai on disinformation: Harmo"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           50,
           "Agentcoord: Visually exploring coordination strat- egy for llm-based multi-agent collaboration. arXi"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           51,
           "Andrews. 2024. Few-shot detection of machine- generated text using style representations. In The Twe"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           52,
           "Thai Le, and Dongwon Lee. 2024. A ship of theseus: Curious cases of paraphrasing in llm-generated te"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           53,
           "through personalization and agency. arXiv preprint arXiv:2402.08855. Ann Yuan, Andy Coenen, Emily Re"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           54,
           "pages 409–436. Yang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Xuelong Li, and Zhen Wang. 2024"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           55,
           "vious part” in Table 3. Other than this, our prompt for the three types of story sections only diffe"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           56,
           "parts often began with a short title for the section it was to generate surrounded by “###” for exam"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           57,
           "is closely linked to real-world implications, as fol- lows: Task 1: Predict multi-author or not In t"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           58,
           "strengths in writing tasks is vital. It ensures effec- tive collaboration without unnecessary comple"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           59,
           "an article Keeping track of LLM-LLM agent interactions in growing open-source market Author Verifica"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           60,
           "Mistral 177.25 ± 12.61 182.24 ± 12.88 178.09 ± 22.71 178.82 ± 19.47 178.15 ± 22.69 Olmo 168.01 ± 8.6"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           61,
           "0.57 ± 0.06 0.58 ± 0.06 0.58 ± 0.06 0.57 ± 0.07 Orca 0.62 ± 0.05 0.62 ± 0.04 0.61 ± 0.04 0.61 ± 0.05"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           62,
           "80.86 ± 9.65 79.95 ± 9.97 79.51 ± 9.80 Coherence Gemma 0.49 ± 0.07 0.47 ± 0.08 0.47 ± 0.08 0.47 ± 0."
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           63,
           "Table 8: Descriptive Statistics or Features for stories generated by different authors for different"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           64,
           "0.71 0.64 0.99 0.76 SVM - 0.61 0.68 0.58 0.97 0.71 BERT - 0.70 0.71 0.64 0.99 0.76 ALBERT - 0.78 0.7"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           65,
           "0.56 0.94 0.68 N=3 Method Orca Olmo Lama Mistral Gemma AVG MNB - 0.60 0.67 0.63 0.94 0.71 SVM - 0.57"
          ],
          [
           "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
           66,
           "- 0.66 0.73 0.70 0.93 0.75 RoBERTa - 0.66 0.68 0.64 0.93 0.73 N=5 Method Orca Olmo Llama Mistral Gem"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           0,
           "1 On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models Sarah R Gao"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           1,
           "rapidly generates a variety of visualizations, namely dendrograms, graphs, word clouds, and scatter "
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           2,
           "similarity [5]. We also construct a graph of LLMs and detect communities using the Louvain method. A"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           3,
           "instantaneous. In some instances, we failed to retrieve a number of likes or downloads for a model. "
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           4,
           "down the model names into n-grams ranging from 2 to 8 characters. Hierarchical Clustering: Hierarchi"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           5,
           "undirected graph, and each model name was added as a node using the 'add_node' function. The model n"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           6,
           "a model and was color-coded based on the community it belonged to. Edges between the nodes indicated"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           7,
           "evidence of parameter size. We were able to infer model parameters for 4,560 models (28.8%). We expe"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           8,
           "visualize relationships and families. From the dendrogram, families of LLMs like Wizard, Pythia, Cau"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           9,
           "We present a publicly available web application (https://constellation.sites.stanford.edu/) to dynam"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           10,
           "high-level view of prominent model families, while the graph-based visualization depicts the relatio"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           11,
           "10 such as Constellation will be instrumental in assisting the researcher and developer communities "
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           12,
           "Wei, D., Jiang, Q., Wei, Y., & Wang, S. (2012). A novel hierarchical clustering algorithm for gene s"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           13,
           "11. numpy/numpy. (2021, October 9). GitHub. https://github.com/numpy/numpy 12. Scikit-Learn. (2019)."
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           14,
           "using NetworkX. Conference.scipy.org. https://conference.scipy.org/proceedings/SciPy2008/paper_2 18."
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           15,
           "8k 167 sft 167 large 166 dialogpt 160 test 157 2 156 all 155 medium 154 lora 153 ds 146 merged 145 s"
          ],
          [
           "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
           16,
           "bloomz 66 v0 66 bert 65 guanaco 64 ft 64 imdb 63 gptj 62 160m 61 gptneo 61 redpajama 58 neox 57 14 g"
          ]
         ],
         "hovertemplate": "x=%{x}<br>y=%{y}<br>document=%{customdata[0]}<br>chunk=%{customdata[1]}<br>preview=%{customdata[2]}<br>color_id=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": {
           "bdata": "AgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsLCwsODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4ODg4MDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBBEREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREREPDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDw8PDxAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwcHBwEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkJCQkKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKChMTExMTExMTExMTExMTExMTExMTExMTExMTExMTExMTExMTExMTExMTExMTExMTExMTExMTExMTExMTExMTExMTExMTExMTAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0GBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBgYGBhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhIICAgICAgICAgICAgICAgICA==",
           "dtype": "i1"
          },
          "coloraxis": "coloraxis",
          "opacity": 0.6,
          "size": 6,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "showlegend": false,
         "type": "scattergl",
         "x": {
          "bdata": "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",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "color_id"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "legend": {
         "tracegroupgap": 0
        },
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Interactive PCA Projection of Chunks"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "y"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"color_id\",\n",
    "    hover_data=[\"document\", \"chunk\", \"preview\"],\n",
    "    title=\"Interactive PCA Projection of Chunks\"\n",
    ")\n",
    "fig.update_traces(marker=dict(size=6, opacity=0.6))\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-playground-ABXqpzBr-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
