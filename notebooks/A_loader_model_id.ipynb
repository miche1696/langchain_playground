{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "757f0f5a",
   "metadata": {},
   "source": [
    "# Processing: Load, Chunk, and Prepare for Vector DB\n",
    "\n",
    "This notebook demonstrates a step-by-step approach to loading, enriching,\n",
    "chunking, and storing documents into a vector database.\n",
    "It compares different outputs at each stage interactively with helpful visualizations.\n",
    "\n",
    "Create from scratch a chain that: \n",
    "- takes an input document. (In this case many Arxiv pdf documents.)\n",
    "- Chunks the document, keeping the metadata.\n",
    "- Embed the document chunks.\n",
    "- Saves the embeddings into a vector DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f71e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c404115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Supports all arguments of `ArxivAPIWrapper`\n",
    "loader = ArxivLoader(\n",
    "    query=\"LLM\",\n",
    "    load_max_docs=50,\n",
    "    top_k_results=20\n",
    "    # doc_content_chars_max=1000,\n",
    "    # load_all_available_meta=False,\n",
    "    # ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0df738bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cca765",
   "metadata": {},
   "source": [
    "# Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "860cd865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1 : Selecting the modelID from HF and the tokenizer, choosing a model\n",
    "model_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer,\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=30,\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    #separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1f7b754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': None,\n",
       " 'metadata': {'Published': '2024-12-23',\n",
       "  'Title': 'Trustworthy and Efficient LLMs Meet Databases',\n",
       "  'Authors': 'Kyoungmin Kim, Anastasia Ailamaki',\n",
       "  'Summary': 'In the rapidly evolving AI era with large language models (LLMs) at the core,\\nmaking LLMs more trustworthy and efficient, especially in output generation\\n(inference), has gained significant attention. This is to reduce plausible but\\nfaulty LLM outputs (a.k.a hallucinations) and meet the highly increased\\ninference demands. This tutorial explores such efforts and makes them\\ntransparent to the database community. Understanding these efforts is essential\\nin harnessing LLMs in database tasks and adapting database techniques to LLMs.\\nFurthermore, we delve into the synergy between LLMs and databases, highlighting\\nnew opportunities and challenges in their intersection. This tutorial aims to\\nshare with database researchers and practitioners essential concepts and\\nstrategies around LLMs, reduce the unfamiliarity of LLMs, and inspire joining\\nin the intersection between LLMs and databases.'},\n",
       " 'page_content': 'Trustworthy and Efficient LLMs Meet Databases\\nKyoungmin Kim\\nkyoung-min.kim@epfl.ch\\nEPFL\\nSwitzerland\\nAnastasia Ailamaki\\nanastasia.ailamaki@epfl.ch\\nEPFL\\nSwitzerland\\nAbstract\\nIn the rapidly evolving AI era with large language models (LLMs) at\\nthe core, making LLMs more trustworthy and efficient, especially in\\noutput generation (inference), has gained significant attention. This\\nis to reduce plausible but faulty LLM outputs (a.k.a hallucinations)\\nand meet the highly increased inference demands. This tutorial\\nexplores such efforts and makes them transparent to the database\\ncommunity. Understanding these efforts is essential in harness-\\ning LLMs in database tasks and adapting database techniques to\\nLLMs. Furthermore, we delve into the synergy between LLMs and\\ndatabases, highlighting new opportunities and challenges in their\\nintersection. This tutorial aims to share with database researchers\\nand practitioners essential concepts and strategies around LLMs,\\nreduce the unfamiliarity of LLMs, and inspire joining in the inter-\\nsection between LLMs and databases.\\n1\\nIntroduction\\nLarge language models (LLMs) have recently transformed various\\nfields with their ability to understand and generate human-like text.\\nIn the database domain, researchers are leveraging LLMs to tackle\\ncomplex data management tasks [55, 194]. LLMs can function not\\nonly as assistants for database administrators (DBAs) [271, 340] but\\nalso as internal components of database systems, optimizing query\\nplans [8, 168] and translating natural languages to SQLs [224].\\nBeyond these applications, key concepts and advancements from\\nthe LLM community remain underexplored by database researchers.\\nThis tutorial aims to bridge that gap by focusing on enhancing the\\ntrustworthiness and efficiency of LLMs. Improving trustworthiness\\ninvolves reducing hallucinations [124] to ensure LLMs generate\\naccurate, factual responses, thereby increasing their reliability in\\ndatabase tasks requiring precise answers and reasoning. Enhancing\\nefficiency focuses on decreasing inference latency and boosting\\nthroughput.\\nInference efficiency is particularly important because, while\\ntraining LLMs demands substantial resources and expertise, infer-\\nence occurs daily across numerous users, leading to significant oper-\\national costs. For instance, OpenAI handles millions of requests, in-\\ncurring substantial monthly expenses to run ChatGPT [73, 215]. In-\\ntegrating LLMs with external data sources, such as vector databases\\nand document retrieval systems in retrieval-augmented generation\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\\nConference’17, July 2017, Washington, DC, USA\\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM\\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\\n(RAG) [154], increases the number and complexity of LLM calls, es-\\npecially with longer inputs. Recent trends in chain-of-thought and\\nmulti-path reasoning, exemplified by models like OpenAI’s o1 [205],\\nfurther amplify inference demands, as generating final answers may\\nrequire multiple LLM calls to enhance trustworthiness.\\nFrom a systems perspective, improving LLM inference efficiency\\nparallels database management system (DBMS) development, pre-\\nsenting opportunities for database researchers to contribute to\\ncreating more efficient LLMs, promoting economic and environ-\\nmental sustainability by reducing the CO2 footprint associated with\\nextensive GPU usage.\\nAfter introducing the essential ideas in making LLMs more trust-\\nworthy and efficient, we will explain the intersection of LLMs and\\ndatabases with new challenges and opportunities.\\n1.1\\nTarget Audience and Prerequisites\\nOur tutorial is designed for conference attendees, focusing on three\\nkey areas to maximize engagement:\\nTrustworthy LLMs (Section 2.1): Aimed at individuals seeking to\\neffectively utilize large language models (LLMs) in database tasks\\nwith minimal errors. Prerequisites include experience with LLMs\\nlike ChatGPT and the distinction between training and inference\\nin machine learning. No in-depth knowledge of LLM internals is\\nrequired.\\nEfficient LLMs (Section 2.2): Targeted at those interested in en-\\nhancing LLM inference efficiency or contributing to the develop-\\nment of fast LLM inference systems by applying database tech-\\nniques. Prerequisites include basic database knowledge and an\\nunderstanding of GPUs. Familiarity with Transformer architecture,\\nattention mechanisms, and key-value (KV) caching is advantageous.\\nLLMs Meet Databases (Section 2.3): Intended for participants ex-\\nploring new research opportunities at the intersection of databases\\nand LLMs. A background in databases, including OLAP, relational\\nalgebra, cost-based query optimization, and approximate/adaptive\\nquery processing, will be helpful.\\nOur goal is to bridge the gap between essential LLM knowledge\\nand the database community, enabling researchers already utiliz-\\ning LLMs to uncover and develop unexplored ideas. Rather than\\nmerely listing state-of-the-art papers, we employ consistent visu-\\nals and focus on core concepts and insights, facilitating a deeper\\nunderstanding and navigation of the evolving LLM landscape.\\n1.2\\nTutorial Length\\nThe intended length of this tutorial is 1.5 hour, with 40, 30, and 20\\nminutes each for Sections 2.1, 2.2, and 2.3, respectively.\\n2\\nTutorial Outline\\nThe tutorial is structured into three main sections, addressing crit-\\nical aspects of LLMs and their interplay with database systems.\\narXiv:2412.18022v1  [cs.DB]  23 Dec 2024\\nImproving Bare LLMs (§2.1.2)\\nFine-tuning, LoRA, RLHF\\nBackground (§2.1.1)\\nAutoregressive training, in-context learning\\nHallucination, lost-in-the-middle problem\\nScaling laws\\nMaking LLMs Interact with the World (§2.1.3)\\nKnowledge/memory/tool retrieval\\nMaking LLMs Self-drive (§2.1.4)\\nSelf-reflection, adaptivity\\nChain-of-thought, multi-hop, \\nmulti-path reasoning\\nAgentic LLMs, network of LLMs\\nSemantic variable, compound AI\\nBackground (§2.2.1)\\nAttention operation, key-value (KV) caching\\nPrefill/decode/preempt/refill stages\\nPipeline/model parallelism\\nLLMs Behave as DBMSs (§2.2.2)\\nContinuous batching, KV cache paging\\nPrefill-decode disaggregation\\nKV cache/attention offloading\\nNano-batching\\nOperation (§2.2.3)\\nFlash/sparse/flex attention \\nData (§2.2.4)\\nKV compression, model quantization, SSM\\nHardware (§2.2.5)\\nRoofline model\\nWorkload (§2.2.6)\\nScheduling, prefix KV sharing, speculation\\nDBAs and DBMS Internal (§2.3.1)\\nDatabase tuning, query optimization, text2sql\\nAdaptive Cost-based Scheduling (§2.3.2)\\nCost model for LLMs, CSP\\nMixed Relational-LLM Workload (§2.3.3)\\nSemantic operators, benchmark\\nMulti-objective Query Optimization (§2.3.4)\\nAccuracy-efficiency trade-off\\nLLM-database System (§2.3.5)\\nIntegration with OLAP databases\\nConvergence and Future (§2.3.6)\\nDisaggregation, adaptive query processing\\nTrustworthy LLMs (§2.1)\\nEfficient LLMs (§2.2)\\nLLMs Meet Databases (§2.3)\\nFigure 1: Tutorial outline (each subsection with keywords).\\nFigure 1 visualizes the outline with keywords for each subsection.\\nSection 2.1 focuses on improving the trustworthiness of LLMs, ex-\\nploring challenges such as hallucination and context limitations\\nwhile presenting state-of-the-art solutions to improve the accu-\\nracy and reliability of generated outputs. Section 2.2 emphasizes\\nefficiency, covering optimization strategies for inference, data man-\\nagement, and hardware utilization. Finally, Section 2.3 highlights\\nthe convergence of LLMs and databases, exploring opportunities for\\nintegration, new workloads, and emerging system designs. Since\\nthe field is changing fast, we will regularly reflect new information\\nuntil the tutorial date.\\n2.1\\nTrustworthy LLMs\\nThe first part of the tutorial explains the efforts to reduce halluci-\\nnations and make LLMs more trustworthy, using an analogy that\\nLLMs resemble humans. We explain background (Section 2.1.1),\\nhow LLMs can solely improve (Section 2.1.2), how LLMs can im-\\nprove by interacting with the external world (Section 2.1.3), and\\nhow LLMs can automatically make such decisions and interact with\\nother LLMs (Section 2.1.4).\\n2.1.1\\nBackground. Large Language Models (LLMs) function as\\ntext-in, text-out systems, generating texts based on their training.\\nTraining an LLM is akin to nurturing a child: by exposing it to\\nextensive text data, the model acquires world knowledge and rea-\\nsoning abilities. This process involves predicting the most probable\\nnext token in a sequence, a type of self-supervised learning. For a\\nsequence of tokens, the model learns to predict the latter tokens\\nbased on the preceding ones, enabling it to generate coherent text\\ncontinuations.\\nFine-tuning refines this process for specific tasks or domains,\\nsimilar to how individuals specialize in particular professions. In\\ncontrast, in-context learning provides additional information or\\nexamples within the input without altering the model’s parameters,\\nakin to consulting external references during an open-book exam.\\nMany prompting techniques [19, 34, 112, 240, 247, 258, 275, 319]\\nincluding chain-of-thought prompting [144, 289] and its variants\\n[18, 312] may leverage in-context learning to enhance performance.\\nDuring inference, LLMs generate texts autoregressively, produc-\\ning one token at a time. This process may involve deterministic\\nmethods like greedy or beam search, or probabilistic approaches\\nsuch as nucleus sampling [80, 114], which helps avoid selecting\\nlow-probability tokens.\\nHowever, LLMs experience hallucinations [13, 124, 270, 305],\\ngenerating plausible-sounding but incorrect or fabricated infor-\\nmation. This is an unavoidable aspect of LLMs [13, 305] which\\narises from limitations in capturing real-world knowledge, inherent\\napproximations in training and inference, input noise, etc. Even\\nslight input perturbations can significantly influence hallucinations\\n[65, 101], and the detection of hallucinations has been a major\\nproblem [47, 54, 77, 195, 204, 233, 264].\\nAdditionally, the lost-in-the-middle problem [117, 181] indicates\\nthat LLMs may struggle to utilize information located in the middle\\nof long contexts, often performing better when relevant informa-\\ntion is at the beginning or end of the input, exhibiting a U-shaped\\nperformance curve. This phenomenon has been attributed to inher-\\nent attention biases within LLMs, where tokens at the start and end\\nof the input receive higher attention, regardless of their relevance\\n[117]. This tendency can lead to increased hallucinations as context\\nlengthens [230].\\nScaling laws [113, 130, 221] explain that error rates decrease as\\nmodel size and training data increase, with optimal scaling requiring\\nproportional growth in both [113]. However, this may not hold for\\nsmaller models [221]. Laws can also relate to temporal loss in the\\ntraining curve [297], downstream tasks [121], model quantization\\n[306], transfer learning [14, 111], number of generated samples [27],\\nand inference time [205] with the advance of using long, complex\\nreasoning paths. Due to automatic prompting techniques [43, 136,\\n255, 302, 330] and that larger models tend to be less sensitive to\\nprompt variations [75], we focus less on prompting techniques.\\nTarget. The audience will distinguish pre-training, fine-tuning, and\\nin-context learning phases of LLMs, and understand the inherent\\nchallenges in making LLMs trustworthy.\\n2.1.2\\nImproving Bare LLMs. We briefly explain the approaches to\\nimprove the LLM itself to make it more trustworthy. Since LLM\\nis a specific class of machine learning (ML) models, general ML\\napproaches to enhance accuracy may work for LLMs. However, as\\nsuch approaches have been extensively studied from the classic ML\\nera, we target more LLM-specific approaches.\\nAs it is infeasible to increase the model size indefinitely, and the\\nmodels typically follow the Transformer architecture [274], efforts\\nhave been put to increase or augment training data (where LLMs\\nthemselves can be used to generate data) [36, 64, 82, 88, 142, 157,\\n163, 235, 241, 279, 341], improve data quality (again, LLMs can be\\nused to clean data) [23, 66, 72, 103, 143, 328], make inferences more\\nrobust [91, 276], and apply better training and fine-tuning methods.\\nSpecifically, fine-tuning covers a broad spectrum of work for\\nexample, parameter-efficient fine-tuning (PEFT) [116, 119, 152, 162,\\n222], instruction tuning [51, 198, 199, 243, 285, 288], reinforce-\\nment learning from human feedback (RLHF) [30, 52, 57, 81, 102,\\n134, 167, 208, 250, 262], and direct preference optimization (DPO)\\n[83, 138, 234, 338]. RLHF leverages human feedback to train a\\nreward model in reinforcement learning (RL), guiding the LLM\\nthrough RL to produce desired outputs. DPO simplifies the align-\\nment process by directly optimizing the policy model without a\\nseparate reward model. While these approaches rely on RL that\\ncontinuously interacts with human or the world external to LLMs,\\nsuch interactions are often limited to training and do not occur in\\ninferences, which we explain in the following sections.\\nOther than the training methods, a new model architecture of\\ndifferential Transformer [315] reduces the distractions of the models\\nto focus on unnecessary information in the long context, which\\nworks similarly to robust decoding strategies [91, 276].\\nTarget. The audience will learn about tuning LLMs to make them\\nmore trustworthy and aligned with user intentions.\\n2.1.3\\nMaking LLMs Interact with the World: Adding Eyes and Hands.\\nLLMs alone can encounter knowledge, memory, and capability limi-\\ntations [326]. Their knowledge is confined to the static information\\nencoded during training, leading to potential inaccuracies over\\ntime. Memory constraints arise from limited context windows, hin-\\ndering the handling of extended conversations. Additionally, their\\ntext-based nature restricts interactions with the physical world. To\\naddress these challenges, LLMs can retrieve knowledge, memory,\\nand tools.\\nThis section focuses on what and how to retrieve. When to re-\\ntrieve is the key to autonomy and will be detailed in the next section.\\nKnowledge retrieval is represented by well-known retrieval-\\naugmented generation (RAG) [76, 90, 126, 154, 166]. Based on the\\ndata type, it can fetch knowledge from knowledge graphs [38, 109,\\n169, 211, 225, 245, 266, 301, 309], tables [9, 22, 40, 45, 96, 98, 115, 128,\\n150, 158, 190, 265, 320], images [41, 42, 314], not just documents.\\nThe data may be chunked/vectorized, stored in vector databases,\\nthen similar chunks are searched online. While vector similarities\\nare typically used, more advanced similarity scores are possible,\\ne.g., using dual or cross encoders [203, 239].\\nMemory retrieval attempts to overcome the limited context\\nsize of LLMs by storing previously seen tokens as key-value pairs\\n[25, 183, 193, 281, 294] and fetching relevant pairs in upcoming\\nrequests, managing memory stores in hierarchical or partitioned\\nway [145, 287] or even as a database [118]. Fetching information\\nfrom long input can also be done without maintaining a separate\\nmemory store, but by sparsifying the model layers [15, 186]. One\\ncan relate low-rank adapters and mixture-of-experts [29, 71, 78,\\n110, 119, 155, 228, 292] with memory retrieval since lightweight\\nmodel parameters are fine-tuned per specific task and domain, and\\ndynamically fetched at online inferences.\\nTool retrieval searches for the APIs to interact with external\\nenvironments [188, 197, 218, 229, 232, 246, 286, 300]. One can con-\\nnect LLMs with databases to call SQLs that can help answering\\nuser questions [22]. Constrained decoding [20, 69, 92, 93] allows\\noutput to follow specific structure which can increase correctness\\nand efficiency.\\nThe challenges in retrieval include the followings. 1) Heterogene-\\nity: LLMs are text-based, but knowledge can be of any type. Even for\\ntext retrieval, heterogeneous lengths and intents between queries\\nand documents can lead to suboptimal retrieval accuracy [74, 89],\\nand the vector-similarity search may be too simple to retrieve neces-\\nsary information [87, 210]. 2) Scalability: Not only that LLMs have\\nlimited context or data they can utilize per inference, but main-\\ntaining a large set of retrieval entities and retrieving a subset may\\nincur overheads [269, 303]. While approximation can mitigate the\\nsearch overhead and make the search negligible to LLM inference\\ncosts, it is limited to vector-similarity search, and generalization\\nto more complex searches [131, 135, 137, 239] remains challenging.\\n3) Sparsity: This is also relevant to data sparsity and noise [56],\\nwhere relevant data is sparse compared to large information pools.\\n4) Reliability: Retrieved knowledge may be imperfect [277].\\nTarget. The audience will understand how LLMs can interact with\\nthe world and exploit external knowledge to overcome the limits\\nin using LLMs alone.\\n2.1.4\\nMaking LLMs Self-drive: Adding Brain. Now we have more\\npowerful models and interactions with the world. The last part is\\nhow we can make LLMs smart enough to maximize these capacities,\\nadding autonomy. Self-consistency and major voting enables a sim-\\nple yet effective solution for increasing consistency [283], however,\\nit fails to generate accurate and diverse answers [33, 44, 46] and\\nis yet passive. More active approaches include self-reflection and\\nadaptive retrieval [11, 32, 123, 125, 159, 185, 331], which adaptively\\nretrieves information multiple times based on the generated output,\\nmodel confidence, query complexity, or fine-tuned policies. This is\\nparticularly helpful for chain-of-thought/multi-hop reasoning and\\nquestion answering [175, 278, 282, 284, 329].\\nThe next step is to use multiple reasoning paths instead of a single\\npath. This multi-path reasoning has been an effective approach\\nfor driving LLMs [224, 226, 324, 332]. While the exact mechanism\\nremains closed, OpenAI’s o1 model is assumed to plan subtasks,\\nconduct these, and revise the results to decide whether to extend\\nthe current plan or generate different plans, forming a tree-like\\nreasoning structure. They suggest a new scaling law that LLM\\naccuracy increases with inference time, not only with training time\\nand data [205].\\nAgentic LLM indicates that LLMs can act as agents, selecting\\nactions based on observations [49, 179, 256, 313]. Multiple agents\\nexploit collaborative reasoning, parallel processing, diversity, and\\nspecialization akin to humans [35, 104, 172, 212, 214, 227, 299].\\nSemantic variables [173] regard LLM input and output tokens as\\ndependent variables to explicitly model control flows.\\nA broader view includes compound AI [323] where AI and non-\\nAI components interact with each other, including retrievals, control\\nflows, agentic LLMs, and more. An interesting example is automated\\nresearch process [187, 259].\\nTarget. The audience will learn about approaches to make LLMs\\nself-driving and build systems around LLMs for complex tasks.\\n2.2\\nEfficient LLMs\\nThe second part of the tutorial demystifies the internals of LLM\\ninference process and explains the efforts to make it more efficient,\\nusing an analogy that LLMs behave as DBMSs. We explain back-\\nground (Section 2.2.1) and how LLM inference systems resemble\\nDBMSs in improving their efficiency (Section 2.2.2). We then ex-\\nplain further work for each dimension of operation (Section 2.2.3),\\ndata (Section 2.2.4), hardware (Section 2.2.5), and workload (Section\\n2.2.6).\\n2.2.1\\nBackground. The dominant Transformer architecture em-\\nploys an attention mechanism [274] that calculates similarity scores\\nbetween a token and its preceding tokens, effectively capturing\\ninter-token relationships and managing extended contexts. This\\nprocess has quadratic complexity, but key-value (KV) caching [58]\\noptimizes it by storing and reusing these computations, reducing\\nthe complexity to linear during inference. Non-attention operations\\nmostly consist of matrix multiplications and activations.\\nInference in Large Language Models (LLMs) involves two pri-\\nmary phases: prefill and decode. During the prefill, the model pro-\\ncesses input tokens to generate the initial output token. The at-\\ntention operates with quadratic complexity due to the absence of\\nprecomputed KVs, making it compute-intensive. During the decode,\\nthe model generates subsequent tokens sequentially, each time us-\\ning the last generated token as input. Here, the attention leverages\\nKV caching, resulting in linear complexity relative to the number\\nof processed tokens and reading their KVs, which makes this phase\\nmore memory-intensive.\\nIn case of multiple requests, they face a race condition as in\\nmulti-tenant systems. If the GPU memory is insufficient to keep\\nall requests’ KVs, some running requests are preempted (evicted),\\nreleasing their KVs from the memory, and restarted (refilled) later\\n[146]. Due to the low PCIe bandwidth, the released KVs are often\\nrecomputed when restarted, instead of offloading to other storage\\ndevices and loading back. Multiple requests in either prefill or\\ndecode steps can be batched to amortize the cost of loading model\\nweights from GPU memory.\\nNote that the model weights also occupy the GPU memory. When\\nthe model size exceeds a single GPU capacity, techniques like model\\nand pipeline parallelism [105, 120, 257] distribute model weights\\nacross multiple GPUs. This partitioning introduces data transfer\\noverhead between GPUs.\\nTarget. The audience will understand the KV caching and different\\nphases of LLM inference requests, and how they compete for the\\nsame GPU resource.\\n2.2.2\\nLLM Inference Systems: LLMs Behave as DBMSs. LLM infer-\\nence systems (e.g., vLLM [146]) behave similarly to (in-memory)\\nDBMS. KVs and model weights correspond to the data, which are\\nmaintained in GPU memory. Operators include matrix multipli-\\ncations, activations, attentions, and data transfers. Compared to\\nOLAP in databases, the operations are simpler yet much more time-\\nsensitive, where the requests should be served in real-time.\\nSignificant efforts have been made to increase the efficiency of\\nLLM inference [342], largely based on operating and database sys-\\ntems. Orca [318] forms a new batch of requests after each iteration\\n(of prefills and decodes) whenever resources are available. Thus, a\\nnew request does not have to wait for all current running requests\\nto finish, just like the pipelining in OS. vLLM [146] adopts paging\\nand virtual memory to manage KVs, reducing memory fragmenta-\\ntion and enlarging the batch size. Since prefills are typically more\\ncostly than decodes, making stalls for decodes when batched to-\\ngether, Sarathi [4, 5] chunks prefills to reduce pipeline bubbles.\\nSome other work [217, 263, 339] rather disaggregates the prefills\\nand decodes into different GPUs, so the workload for each GPU\\nis homogeneous. vTensor [298] decouples the KV cache manage-\\nment and attention computation of vLLM for better flexibility and\\nperformance. NanoFlow [343] splits each batch into nano-batches\\nfor finer-grained pipelining, increasing the overlap of computation,\\nmemory operation, and data transfer between GPUs. It also hides\\nCPU scheduling latency by asynchronous scheduling. InfiniGen\\n[149] offloads the KVs to CPU memory to extend the KV cache and\\nreloads the KVs from CPU layer-wise, but fetches a subset of KVs\\nfor efficiency, similarly to sparse attentions (Section 2.2.3). InstIn-\\nfer [213] offloads KVs and attention computations to flash drives,\\njust like the storage-disaggregation and computation pushdown in\\ndatabases [310]. NEO [127] selectively offloads attention computa-\\ntions and KVs from GPU to CPU, in order to maximize both GPU\\nand CPU utilization.\\nTarget. The audience will understand why LLMs behave similarly\\nto DBMSs and how database techniques can improve LLM infer-\\nence efficiency. In subsequent sections, the audience will learn\\nabout efforts and challenges in further improving efficiency in four\\ndimensions: operation data, hardware, and workload.\\n2.2.3\\nOperation: Attention. While matrix multiplications take the\\nmajor portion in LLM latency in general [5], attentions can domi-\\nnate the runtime for large inputs due to their quadratic complexity.\\nFlashAttention [59, 60, 249] has become a de facto standard as an\\nefficient attention implementation, utilizing recent GPU technolo-\\ngies to boost the inference speed. The ideas include kernel operator\\nfusion and GPU cache-aware KV transfer. As in approximate query\\nprocessing (AQP) in databases, sparse attentions [15, 178, 186] do\\nnot compute the full attention scores for all preceding tokens but\\na subset as an approximation. Some attentions rather optimize\\nfor long contexts [2, 62]. FlexAttention [107] offers flexible and\\nperformant implementation of such attentions.\\n2.2.4\\nData: KV and Model Weights. Reading KVs from GPU mem-\\nory in decode-attentions is similar to sequential table scan. As KVs\\nare maintained per each attention layer, reading KVs for a layer\\ncan overlap with other layers’ operators [149, 263]. While offload-\\ning KVs and attention computation have been popular recently\\n[127, 149, 213], we need to be careful as it is challenging to predict\\nthe output lengths of LLM requests and thus their utilization pat-\\nterns, and a KV for a single token may consume a few MBs. KVs\\nof long documents can be precomputed, compressed, and fetched\\nfor later retrievals [184]. To reduce memory latency, one can opt\\nfor KV sharing across different attention heads [7, 26, 50], KV com-\\npression [63, 129, 176, 184, 236], model quantization [94, 97, 106,\\n147, 164, 200, 251, 295, 306], or different model architectures than\\nTransformer, such as State Space Models (SSMs) [61, 100] that do\\nnot rely on attentions, thereby not generating KVs. While hybrid\\narchitectures [10, 67, 99, 108, 171, 223, 237] can balance between\\nthe efficiency of SSMs and memorization capacity of Transformers,\\nSSMs remain niche in the market [17]. A recent work even shows\\nthat tokenizers can be removed from the models [209].\\n2.2.5\\nHardware: Theory and Practice. We briefly explain 1) the\\nroofline model [322] and 2) some efforts to overcome the hard-\\nware limits [161, 238, 253, 261, 317, 333] or leverage advanced hard-\\nware for LLM inference [170, 304]. The roofline model is based\\non the computation speed (e.g., GPU FLOPS) and memory band-\\nwidth, which acts as a theoretical hardware bound and determines\\nwhether an operator is compute-intensive or memory-intensive\\nacross different inputs.\\n2.2.6\\nWorkload: Scheduling, Prefix Sharing, Speculation. To handle\\nmultiple LLM requests, LLM inference systems implement request\\nscheduler to send LLM requests to appropriate machines or GPUs\\nto maximize throughput or minimize latency. Assuming indepen-\\ndent requests, early schedulers either prioritize prefills [146] or\\ndecodes [4], which tend to optimize latency or throughput, respec-\\ntively. More complex schedulers consider fairness [252, 291] while\\ncompromising performance, or predict the output lengths of re-\\nquests (not known in advance) and schedule shorter requests first\\n[85, 231, 337].\\nIf different LLM requests can share a prefix in their inputs, the\\nKVs of the prefix can be stored just once and reused for multiple\\nrequests [334, 335]. This forms a trie structure with shared prefixes.\\nHowever, a single-token difference in inputs may invalidate the\\nsharing of KVs of all subsequent tokens. To increase the sharing\\nopportunity, [311] uses the KVs of multiple token sequences to\\napproximate the KVs of the concatenated sequence. The mechanism\\nis similar to the speculation in OLAP [260] and healing protocol in\\ntransactions [293] in databases.\\nThis speculation and healing patterns also appear in speculative\\ndecoding [153] and model cascades [39, 177, 316, 325, 336], acceler-\\nating the generation of tokens by leveraging smaller, faster models\\nthen validate the tokens using larger models, since the validation\\ncosts less than the generation.\\n2.3\\nLLMs Meet Databases\\nThe last part of the tutorial discusses the intersection between\\nLLMs and databases, opportunities and challenges in how we can\\nexploit LLMs for databases, how the development of databases can\\nhelp LLMs, and how we can exploit new types of workloads and\\nintegrations of LLMs and databases. We explain from more well-\\nknown to more untapped, deeper integrations in Sections 2.3.1-2.3.5\\nand provide more proactive visions in Section 2.3.6.\\n2.3.1\\nLLMs for DBs: DBAs and DBMS Internal. We briefly explain\\nhow LLMs are utilized for well-known tasks of DBAs and DBMS\\ninternals such as database tuning [271, 340], text2sql [151, 224] and\\nquery optimization [8, 168]. As we mentioned in Section 1, we will\\nnot cover every detail, as many of these efforts are covered in a\\nprevious tutorial [194] and its additional list of papers [55].\\n2.3.2\\nDBs for LLMs: Adaptive Cost-based Scheduling. Unlike the\\nsophisticated query optimizers in DBMSs, LLMs lack cost models\\nand cost-based scheduling of LLM requests. [3] measures the batch\\ntimes across various inputs (number of tokens to process and KV\\nsize to read). [322] computes batch times based on the roofline\\nmodels. These can be used to model batch times and formulate the\\nproblem of finding optimal schedules as a constrained satisfaction\\nproblem (CSP) [139]. While schedulers try to avoid preemptions to\\nmaximize performance, [139] shows that harnessing preemptions\\ncan rather reduce overall latency compared to zero-preemptions.\\nAs the exact hardware utilization of each request is not known in\\nadvance, the scheduling should be adaptive based on the observa-\\ntions, and it has not been explored much to schedule dependent\\nrequests connected via semantic variables or shared prefixes [139].\\n2.3.3\\nDBs with LLMs: Mixed Relational-LLM Workload. Not only\\nsolving existing tasks with LLMs, LLMs offer new functionalities\\nwhen integrated into DBMSs. Semantic operators [216] extend rela-\\ntional operators to batch-process the tabular data with LLMs (e.g.,\\nfilters and joins using LLMs), which can be regarded as an AQP.\\nWorkloads with LLMs provide a justification to use LLMs inside\\nDBMSs (heavy LLM calls in plan optimization can be negligible com-\\npared to query execution with LLMs). However, different pipelines\\n(with semantic operators) lead to different accuracy and efficiency,\\nthus defining the equivalence between two pipelines is non-trivial.\\nFurthermore, more complex pipelines or LLM calls do not always\\nguarantee higher accuracy [37, 74], and searching similar entities\\nwith LLMs can be replaced with efficient vector-similarity searches\\n[177, 242] as a type of model cascade.\\n2.3.4\\nDBs with LLMs: Multi-objective Query Optimization and Bench-\\nmarks. The challenge is therefore how we can automatically find\\ngood pipelines for mixed relational-LLM workloads under the multi-\\nobjective of accuracy and efficiency [22, 273, 321] as in compound\\nAI systems [95, 244, 267]. This calls for development of accurate\\ncost models and accuracy-prediction models for LLMs and mixed\\nrelational-LLM workloads, in order to enable the holistic optimiza-\\ntion of query plans consisting of both relational and non-relational\\noperators. The cost model itself can be learned via LLMs (or any ML\\nmodels), possibly using RLHF or feedback from query execution\\nwithout human intervention, where such an automatic training\\ndata generation is one of the advantages of solving database tasks\\ncompared to conventional ML tasks (e.g., natural language process-\\ning with human-labeled translation data) [140]. Another model for\\npredicting the output accuracy or detecting hallucination may be\\nchosen from the scaling laws (using the general fact that larger\\nmodels are more trustworthy) or separately trained.\\nTo balance efficiency and accuracy, during the physical query\\noptimization we should select proper models (ones used for ex-\\necution) to avoid calling heavy LLMs unnecessarily. Depending\\non the complexity of the task, simple ML models with a small set\\nof supervised data [123], or larger deep generative models such\\nas in tabular foundation models tailored to domain-specific data\\n[141, 160, 308], or LLMs with world knowledge and reasoning capac-\\nity [296] can fit the task with different accuracy-efficiency trade-offs.\\nSmall language models (SLMs) [1, 16, 67, 189, 196, 202, 226] are also\\na good choice. Automatically finding the best prompt configuration\\n[136, 280] tailored to the mixed workloads and more (e.g., previ-\\nously mentioned fine-tuning or multi-hop/multi-path reasoning\\nwith adaptivity during inference) might be desired.\\nFurthermore, unlike the TPC benchmarks for databases, another\\nproblem is that there is no comprehensive benchmark for relational-\\nLLM workloads yet. [22, 182] provide exploratory benchmarks\\nwithout focusing on semantic joins.\\n2.3.5\\nDBs with LLMs: Integrated System. Other than the cost mod-\\nels, we also need DBMSs with native LLM support to increase\\nthe optimization opportunities, alike systems for relational-vector\\nworkloads [242, 327]. Current prototypes for relational-LLM work-\\nloads [177, 182, 216] separate table processing (e.g., pandas [192])\\nand LLM inference engine (e.g., vLLM [146]).\\nTo maximize efficiency and scalability, we should focus on hard-\\nware utilization, data movement [122], caching hot data, locating\\ncomputations close to data (e.g., computation pushdown in storage-\\naggregation setting) [84, 191], asynchronous API calls [95], balanc-\\ning loads, and multi-tenancy just like in DBMSs [248, 310]. One\\nalso has to decide whether to maintain a separate vector database\\nfor faster online vector retrievals, or use just-in-time vectorization\\nfor reducing storage overhead. This also applies to precomputing\\nKVs of data tokens [184] for faster LLM inferences or not, but with\\na higher caution as KVs are typically larger than vectors.\\n2.3.6\\nConvergence and Future. We envision LLMs and databases\\nto converge (e.g., neuro-symbolic systems [48, 79, 268, 321]), more\\nthan just applying the techniques from one domain to another. A\\nnew LLM inference system optimized for DBMSs might be devel-\\noped from an open-source cloud DBMS, utilizing recent implemen-\\ntations and optimizations for processing relational operators, such\\nas storage-disaggregation and computation pushdown for scalable\\ndata and model management [310], GPU-based OLAP processing\\n[86, 219, 220] for the full use of GPUs for both relational opera-\\ntors and LLMs, hybrid operators with heterogeneous data transfer\\npaths [53, 310], adaptive query execution [307] and more. A unified\\nquery optimizer and data model for both relational data, KVs, and\\nmodel weights, could offer opportunities for better data manage-\\nment and hardware utilization. Finally, if we look into the near\\nfuture, we could also harness the emerging CXL technology for\\nmemory disaggregation [6, 148, 180] to manage model weights\\nand KVs, and increased interest in pruning unnecessary data in\\nOLAP [21, 24, 174, 206] could lead to higher trustworthiness (due\\nto reduced noise) and efficiency (due to less data to process) in the\\nrelational-LLM workloads, with connections to online aggregation\\n[254] and incremental view maintenance [28].\\nTarget. The audience will understand the different depths of LLM-\\ndatabase integrations and be able to find interesting research topics\\nfrom each of the integration, which are closely related to the current\\nand near-future trends of databases.\\n3\\nRelated Tutorial\\nXupeng et al. [194] presented a tutorial at SIGMOD 2024 about\\nthe role of data management in the development (training, fine-\\ntuning) and deployment (inference) of LLMs. It focused on how the\\nknowledge is encoded into model parameters and extracted during\\nthe inference, and explained the concept of KV caching but not LLM\\ninference systems. Trummer [272] presented at VLDB 2023 about\\nTransformer architecture, pre-training/fine-tuning/prompting in\\nLLMs, and LLM applications in data management. As pointed out\\nby [194], most of other tutorials presented at SIGMOD and VLDB\\nabout AI and databases focused on traditional machine learning\\nand deep learning tasks not tailored to LLMs [31, 70, 156, 165, 201,\\n207, 290], or specific LLM-related applications such as tabular data\\nunderstanding [12] and queries with natural languages [132, 133].\\nDong et al. [68] presented at SIGKDD 2023 about the role of LLMs\\nin building intelligent AR/VR assistants.\\nIn this tutorial, we will focus on more recent, general approaches\\nto enhance the trustworthiness and efficiency of LLMs, which have\\nnot been addressed in previous tutorials. For trustworthiness, we\\nwill start with enhancing LLMs alone, LLMs with tools, and agentic\\nLLMs and collaboration. For efficiency, we will explain how LLM\\ninference systems resemble DBMSs. Then we will discuss how we\\ncan integrate LLMs and databases in depth. We expect that these\\nare what researchers, who aim to use LLMs in their applications or\\noptimize LLMs using database techniques, need to know about. In-\\nstead of a common analogy that LLMs are knowledge bases as they\\ngenerate plausible facts, we will use analogies that LLMs behave as\\nDBMSs and improve as how humans solve challenging problems.\\nReferences\\n[1] Marah I Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed\\nAwadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harki-\\nrat S. Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Mar-\\ntin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul\\nChopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan\\nIter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng\\nHao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauff-\\nmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko,\\nJames R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi\\nLin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick,\\nBarun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin,\\nMarko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi,\\nAmin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi\\nSharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang,\\nPhilipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang,\\nZiyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang,\\nLi Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. 2024.\\nPhi-3 Technical Report: A Highly Capable Language Model Locally on Your\\nPhone. CoRR abs/2404.14219 (2024). https://doi.org/10.48550/ARXIV.2404.14219\\narXiv:2404.14219\\n[2] Amey Agrawal, Junda Chen, Íñigo Goiri, Ramachandran Ramjee, Chaojie\\nZhang, Alexey Tumanov, and Esha Choukse. 2024. Mnemosyne: Paralleliza-\\ntion Strategies for Efficiently Serving Multi-Million Context Length LLM\\nInference Requests Without Approximations.\\nCoRR abs/2409.17264 (2024).\\nhttps://doi.org/10.48550/ARXIV.2409.17264 arXiv:2409.17264\\n[3] Amey Agrawal, Nitin Kedia, Jayashree Mohan, Ashish Panwar, Nipun\\nKwatra, Bhargav S. Gulavani, Ramachandran Ramjee, and Alexey Tu-\\nmanov. 2024.\\nVIDUR: A Large-Scale Simulation Framework for LLM\\nInference. In Proceedings of the Seventh Annual Conference on Machine\\nLearning and Systems, MLSys 2024, Santa Clara, CA, USA, May 13-16,\\n2024, Phillip B. Gibbons, Gennady Pekhimenko, and Christopher De Sa\\n(Eds.). mlsys.org. https://proceedings.mlsys.org/paper_files/paper/2024/hash/\\nb74a8de47d2b3c928360e0a011f48351-Abstract-Conference.html\\n[4] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwa-\\ntra, Bhargav S. Gulavani, Alexey Tumanov, and Ramachandran Ramjee. 2024.\\nTaming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve.\\nIn 18th USENIX Symposium on Operating Systems Design and Implementation,\\nOSDI 2024, Santa Clara, CA, USA, July 10-12, 2024, Ada Gavrilovska and Dou-\\nglas B. Terry (Eds.). USENIX Association, 117–134. https://www.usenix.org/\\nconference/osdi24/presentation/agrawal\\n[5] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S.\\nGulavani, and Ramachandran Ramjee. 2023. SARATHI: Efficient LLM Inference\\nby Piggybacking Decodes with Chunked Prefills. CoRR abs/2308.16369 (2023).\\nhttps://doi.org/10.48550/ARXIV.2308.16369 arXiv:2308.16369\\n[6] Minseon Ahn, Thomas Willhalm, Norman May, Donghun Lee, Suprasad Mutalik\\nDesai, Daniel Booss, Jungmin Kim, Navneet Singh, Daniel Ritter, and Oliver\\nRebholz. 2024. An Examination of CXL Memory Use Cases for In-Memory\\nDatabase Management Systems using SAP HANA. Proc. VLDB Endow. 17, 12\\n(2024), 3827–3840. https://www.vldb.org/pvldb/vol17/p3827-ahn.pdf\\n[7] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico\\nLebrón, and Sumit Sanghai. 2023. GQA: Training Generalized Multi-Query\\nTransformer Models from Multi-Head Checkpoints. In Proceedings of the 2023\\nConference on Empirical Methods in Natural Language Processing, EMNLP 2023,\\nSingapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali\\n(Eds.). Association for Computational Linguistics, 4895–4901. https://doi.org/\\n10.18653/V1/2023.EMNLP-MAIN.298\\n[8] Peter Akioyamen, Zixuan Yi, and Ryan Marcus. 2024. The Unreasonable Ef-\\nfectiveness of LLMs for Query Optimization. arXiv preprint arXiv:2411.02862\\n(2024).\\n[9] Uday Allu, Biddwan Ahmed, and Vishesh Tripathi. 2024. Beyond Extraction:\\nContextualising Tabular Data for Efficient Summarisation by Language Mod-\\nels. CoRR abs/2401.02333 (2024). https://doi.org/10.48550/ARXIV.2401.02333\\narXiv:2401.02333\\n[10] Quentin Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge. 2024.\\nBlackMamba: Mixture of Experts for State-Space Models. CoRR abs/2402.01771\\n(2024). https://doi.org/10.48550/ARXIV.2402.01771 arXiv:2402.01771\\n[11] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi.\\n2024. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-\\nReflection. In The Twelfth International Conference on Learning Representations,\\nICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.\\nnet/forum?id=hSyW5go0v8\\n[12] Gilbert Badaro and Paolo Papotti. 2022. Transformers for Tabular Data Repre-\\nsentation: A Tutorial on Models and Applications. Proc. VLDB Endow. 15, 12\\n(2022), 3746–3749. https://doi.org/10.14778/3554821.3554890\\n[13] Sourav Banerjee, Ayushi Agarwal, and Saloni Singla. 2024. LLMs Will Always\\nHallucinate, and We Need to Live With This. CoRR abs/2409.05746 (2024).\\nhttps://doi.org/10.48550/ARXIV.2409.05746 arXiv:2409.05746\\n[14] Matthew Barnett. 2024.\\nAn Empirical Study of Scaling Laws for Trans-\\nfer. CoRR abs/2408.16947 (2024). https://doi.org/10.48550/ARXIV.2408.16947\\narXiv:2408.16947\\n[15] Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The\\nLong-Document Transformer. CoRR abs/2004.05150 (2020). arXiv:2004.05150\\nhttps://arxiv.org/abs/2004.05150\\n[16] Loubna Ben Allal, Anton Lozhkov, and Elie Bakouch. 2024. SmolLM - blazingly\\nfast and remarkably powerful. https://huggingface.co/blog/smollm. Accessed:\\nDecember 15, 2024.\\n[17] Nathan Benaich and Ian Hogarth. 2024. State of AI Report 2024. https://www.\\nstateof.ai.\\n[18] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski,\\nLukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Pi-\\notr Nyczyk, and Torsten Hoefler. 2024. Graph of Thoughts: Solving Elaborate\\nProblems with Large Language Models. In Thirty-Eighth AAAI Conference on\\nArtificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applica-\\ntions of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational\\nAdvances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver,\\nCanada, Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan (Eds.).\\nAAAI Press, 17682–17690. https://doi.org/10.1609/AAAI.V38I16.29720\\n[19] Maciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger, Nils Blach,\\nPiotr Nyczyk, Marcin Copik, Grzegorz Kwasniewski, Jürgen Müller, Lukas Gian-\\ninazzi, Ales Kubicek, Hubert Niewiadomski, Onur Mutlu, and Torsten Hoefler.\\n2024. Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of\\nThoughts. CoRR abs/2401.14295 (2024). https://doi.org/10.48550/ARXIV.2401.\\n14295 arXiv:2401.14295\\n[20] Luca Beurer-Kellner, Marc Fischer, and Martin T. Vechev. 2024. Guiding LLMs\\nThe Right Way: Fast, Non-Invasive Constrained Generation. In Forty-first In-\\nternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July\\n21-27, 2024. OpenReview.net. https://openreview.net/forum?id=pXaEYzrFae\\n[21] Altan Birler, Alfons Kemper, and Thomas Neumann. 2024. Robust Join Process-\\ning with Diamond Hardened Joins. Proc. VLDB Endow. 17, 11 (2024), 3215–3228.\\nhttps://www.vldb.org/pvldb/vol17/p3215-birler.pdf\\n[22] Asim Biswal, Liana Patel, Siddarth Jha, Amog Kamsetty, Shu Liu, Joseph E.\\nGonzalez, Carlos Guestrin, and Matei Zaharia. 2024. Text2SQL is Not Enough:\\nUnifying AI and Databases with TAG. CoRR abs/2408.14717 (2024).\\nhttps:\\n//doi.org/10.48550/ARXIV.2408.14717 arXiv:2408.14717\\n[23] Quinten Bolding, Baohao Liao, Brandon James Denis, Jun Luo, and Christof\\nMonz. 2023. Ask Language Model to Clean Your Noisy Translation Data. In\\nFindings of the Association for Computational Linguistics: EMNLP 2023, Singapore,\\nDecember 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Associ-\\nation for Computational Linguistics, 3215–3236. https://doi.org/10.18653/V1/\\n2023.FINDINGS-EMNLP.212\\n[24] Angela Bonifati, Stefania Dumbrava, George Fletcher, Jan Hidders, Matthias\\nHofer, Wim Martens, Filip Murlak, Joshua Shinavier, Slawek Staworko, and\\nDominik Tomaszuk. 2023. Threshold Queries. SIGMOD Rec. 52, 1 (2023), 64–73.\\nhttps://doi.org/10.1145/3604437.3604452\\n[25] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Ruther-\\nford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan\\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman\\nRing, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cas-\\nsirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osin-\\ndero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improv-\\ning Language Models by Retrieving from Trillions of Tokens. In International\\nConference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland,\\nUSA (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaudhuri,\\nStefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (Eds.).\\nPMLR, 2206–2240. https://proceedings.mlr.press/v162/borgeaud22a.html\\n[26] William Brandon, Mayank Mishra, Aniruddha Nrusimha, Rameswar Panda, and\\nJonathan Ragan-Kelley. 2024. Reducing Transformer Key-Value Cache Size with\\nCross-Layer Attention. CoRR abs/2405.12981 (2024). https://doi.org/10.48550/\\nARXIV.2405.12981 arXiv:2405.12981\\n[27] Bradley C. A. Brown, Jordan Juravsky, Ryan Saul Ehrlich, Ronald Clark, Quoc V.\\nLe, Christopher Ré, and Azalia Mirhoseini. 2024. Large Language Monkeys:\\nScaling Inference Compute with Repeated Sampling. CoRR abs/2407.21787\\n(2024). https://doi.org/10.48550/ARXIV.2407.21787 arXiv:2407.21787\\n[28] Mihai Budiu, Tej Chajed, Frank McSherry, Leonid Ryzhyk, and Val Tannen. 2023.\\nDBSP: Automatic Incremental View Maintenance for Rich Query Languages.\\nProc. VLDB Endow. 16, 7 (2023), 1601–1614. https://doi.org/10.14778/3587136.\\n3587137\\n[29] Shiyi Cao, Shu Liu, Tyler Griggs, Peter Schafhalter, Xiaoxuan Liu, Ying Sheng,\\nJoseph E Gonzalez, Matei Zaharia, and Ion Stoica. 2024. MoE-Lightning: High-\\nThroughput MoE Inference on Memory-constrained GPUs. arXiv preprint\\narXiv:2411.11217 (2024).\\n[30] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy\\nScheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro\\nFreire, Tony Tong Wang, Samuel Marks, Charbel-Raphaël Ségerie, Micah Carroll,\\nAndi Peng, Phillip J. K. Christoffersen, Mehul Damani, Stewart Slocum, Usman\\nAnwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii\\nKrasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Biyik, Anca D.\\nDragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. 2023. Open\\nProblems and Fundamental Limitations of Reinforcement Learning from Human\\nFeedback. Trans. Mach. Learn. Res. 2023 (2023). https://openreview.net/forum?\\nid=bx24KpJ4Eb\\n[31] Chengliang Chai, Nan Tang, Ju Fan, and Yuyu Luo. 2023. Demystifying Artificial\\nIntelligence for Data Preparation. In Companion of the 2023 International Confer-\\nence on Management of Data, SIGMOD/PODS 2023, Seattle, WA, USA, June 18-23,\\n2023, Sudipto Das, Ippokratis Pandis, K. Selçuk Candan, and Sihem Amer-Yahia\\n(Eds.). ACM, 13–20. https://doi.org/10.1145/3555041.3589406\\n[32] Andong Chen, Lianzhang Lou, Kehai Chen, Xuefeng Bai, Yang Xiang, Muyun\\nYang, Tiejun Zhao, and Min Zhang. 2024. DUAL-REFLECT: Enhancing Large\\nLanguage Models for Reflective Translation through Dual Learning Feedback\\nMechanisms. In Proceedings of the 62nd Annual Meeting of the Association for\\nComputational Linguistics, ACL 2024 - Short Papers, Bangkok, Thailand, August 11-\\n16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for\\nComputational Linguistics, 693–704. https://aclanthology.org/2024.acl-short.64\\n[33] Angelica Chen, Jason Phang, Alicia Parrish, Vishakh Padmakumar, Chen Zhao,\\nSamuel R. Bowman, and Kyunghyun Cho. 2024. Two Failures of Self-Consistency\\nin the Multi-Step Reasoning of LLMs. Trans. Mach. Learn. Res. 2024 (2024).\\nhttps://openreview.net/forum?id=5nBqY1y96B\\n[34] Banghao Chen, Zhaofeng Zhang, Nicolas Langrené, and Shengxin Zhu. 2023.\\nUnleashing the potential of prompt engineering in Large Language Models: a\\ncomprehensive review. CoRR abs/2310.14735 (2023). https://doi.org/10.48550/\\nARXIV.2310.14735 arXiv:2310.14735\\n[35] Huaben Chen, Wenkang Ji, Lufeng Xu, and Shiyu Zhao. 2023. Multi-Agent\\nConsensus Seeking via Large Language Models. CoRR abs/2310.20151 (2023).\\nhttps://doi.org/10.48550/ARXIV.2310.20151 arXiv:2310.20151\\n[36] Jiaao Chen, Derek Tam, Colin Raffel, Mohit Bansal, and Diyi Yang. 2023. An\\nEmpirical Survey of Data Augmentation for Limited Data Learning in NLP.\\nTrans. Assoc. Comput. Linguistics 11 (2023), 191–211. https://doi.org/10.1162/\\nTACL_A_00542\\n[37] Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei\\nZaharia, and James Zou. 2024. Are more llm calls all you need? towards scaling\\nlaws of compound inference systems. arXiv preprint arXiv:2403.02419 (2024).\\n[38] Liyi Chen, Panrong Tong, Zhongming Jin, Ying Sun, Jieping Ye, and Hui Xiong.\\n2024. Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language\\nModel on Knowledge Graphs. CoRR abs/2410.23875 (2024). https://doi.org/10.\\n48550/ARXIV.2410.23875 arXiv:2410.23875\\n[39] Lingjiao Chen, Matei Zaharia, and James Zou. 2023.\\nFrugalGPT: How to\\nUse Large Language Models While Reducing Cost and Improving Perfor-\\nmance. CoRR abs/2305.05176 (2023). https://doi.org/10.48550/ARXIV.2305.05176\\narXiv:2305.05176\\n[40] Wenhu Chen. 2023. Large Language Models are few(1)-shot Table Reasoners. In\\nFindings of the Association for Computational Linguistics: EACL 2023, Dubrovnik,\\nCroatia, May 2-6, 2023, Andreas Vlachos and Isabelle Augenstein (Eds.). Associ-\\nation for Computational Linguistics, 1090–1100. https://doi.org/10.18653/V1/\\n2023.FINDINGS-EACL.83\\n[41] Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William W. Cohen. 2022.\\nMuRAG: Multimodal Retrieval-Augmented Generator for Open Question An-\\nswering over Images and Text. In Proceedings of the 2022 Conference on Em-\\npirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi,\\nUnited Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zornitsa Kozareva,\\nand Yue Zhang (Eds.). Association for Computational Linguistics, 5558–5570.\\nhttps://doi.org/10.18653/V1/2022.EMNLP-MAIN.375\\n[42] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W. Cohen. 2023.\\nRe-Imagen: Retrieval-Augmented Text-to-Image Generator. In The Eleventh\\nInternational Conference on Learning Representations, ICLR 2023, Kigali, Rwanda,\\nMay 1-5, 2023. OpenReview.net. https://openreview.net/forum?id=XSEBx0iSjFQ\\n[43] Weizhe Chen, Sven Koenig, and Bistra Dilkina. 2024.\\nRePrompt: Plan-\\nning by Automatic Prompt Engineering for Large Language Models Agents.\\nCoRR abs/2406.11132 (2024).\\nhttps://doi.org/10.48550/ARXIV.2406.11132\\narXiv:2406.11132\\n[44] Wenqing Chen, Weicheng Wang, Zhixuan Chu, Kui Ren, Zibin Zheng, and\\nZhichao Lu. 2024. Self-Para-Consistency: Improving Reasoning Tasks at Low\\nCost for Large Language Models. In Findings of the Association for Computational\\nLinguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16,\\n2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for\\nComputational Linguistics, 14162–14167.\\nhttps://doi.org/10.18653/V1/2024.\\nFINDINGS-ACL.842\\n[45] Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and\\nWilliam Yang Wang. 2020. HybridQA: A Dataset of Multi-Hop Question An-\\nswering over Tabular and Textual Data. In Findings of the Association for Compu-\\ntational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020 (Findings of\\nACL, Vol. EMNLP 2020), Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association\\nfor Computational Linguistics, 1026–1036. https://doi.org/10.18653/V1/2020.\\nFINDINGS-EMNLP.91\\n[46] Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin,\\nSushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. 2023. Univer-\\nsal Self-Consistency for Large Language Model Generation. CoRR abs/2311.17311\\n(2023). https://doi.org/10.48550/ARXIV.2311.17311 arXiv:2311.17311\\n[47] Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang, Qiang Li,\\nYue Shen, Lei Liang, Jinjie Gu, and Huajun Chen. 2024. Unified Hallucination\\nDetection for Multimodal Large Language Models. In Proceedings of the 62nd\\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long\\nPapers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, Lun-Wei Ku, Andre\\nMartins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,\\n3235–3252. https://doi.org/10.18653/V1/2024.ACL-LONG.178\\n[48] Zui Chen, Zihui Gu, Lei Cao, Ju Fan, Samuel Madden, and Nan Tang. 2023.\\nSymphony: Towards Natural Language Query Answering over Multi-modal\\nData Lakes. In 13th Conference on Innovative Data Systems Research, CIDR 2023,\\nAmsterdam, The Netherlands, January 8-11, 2023. www.cidrdb.org. https://www.\\ncidrdb.org/cidr2023/papers/p51-chen.pdf\\n[49] Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong,\\nWenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, and Xiuqiang He.\\n2024. Exploring Large Language Model based Intelligent Agents: Definitions,\\nMethods, and Prospects. CoRR abs/2401.03428 (2024). https://doi.org/10.48550/\\nARXIV.2401.03428 arXiv:2401.03428\\n[50] Sai Sena Chinnakonduru and Astarag Mohapatra. 2024. Weighted Grouped\\nQuery Attention in Transformers. CoRR abs/2407.10855 (2024). https://doi.org/\\n10.48550/ARXIV.2407.10855 arXiv:2407.10855\\n[51] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021. Unifying Vision-and-\\nLanguage Tasks via Text Generation. In Proceedings of the 38th International\\nConference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event\\n(Proceedings of Machine Learning Research, Vol. 139), Marina Meila and Tong\\nZhang (Eds.). PMLR, 1931–1942. http://proceedings.mlr.press/v139/cho21a.html\\n[52] Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg,\\nand Dario Amodei. 2017. Deep Reinforcement Learning from Human Pref-\\nerences. In Advances in Neural Information Processing Systems 30: Annual\\nConference on Neural Information Processing Systems 2017, December 4-9,\\n2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Ben-\\ngio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman\\nGarnett (Eds.). 4299–4307.\\nhttps://proceedings.neurips.cc/paper/2017/hash/\\nd5e2c0adad503c91f91df240d0cd4e49-Abstract.html\\n[53] Periklis Chrysogelos, Manos Karpathiotakis, Raja Appuswamy, and Anastasia\\nAilamaki. 2019. HetExchange: Encapsulating heterogeneous CPU-GPU par-\\nallelism in JIT compiled engines. Proc. VLDB Endow. 12, 5 (2019), 544–556.\\nhttps://doi.org/10.14778/3303753.3303760\\n[54] Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, and\\nJames R. Glass. 2024. Lookback Lens: Detecting and Mitigating Contextual Hallu-\\ncinations in Large Language Models Using Only Attention Maps. In Proceedings\\nof the 2024 Conference on Empirical Methods in Natural Language Processing,\\nEMNLP 2024, Miami, FL, USA, November 12-16, 2024, Yaser Al-Onaizan, Mohit\\nBansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics,\\n1419–1436. https://aclanthology.org/2024.emnlp-main.84\\n[55] code4DB. 2024. LLM4DB: A Curated List of Resources on Large Language\\nModels for Databases. https://github.com/code4DB/LLM4DB. Accessed: 2024-\\n11-30.\\n[56] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare\\nCampagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024. The\\nPower of Noise: Redefining Retrieval for RAG Systems. In Proceedings of the 47th\\nInternational ACM SIGIR Conference on Research and Development in Information\\nRetrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024, Grace Hui Yang,\\nHongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, and Yi Zhang (Eds.).\\nACM, 719–729. https://doi.org/10.1145/3626772.3657834\\n[57] Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou\\nWang, and Yaodong Yang. 2024. Safe RLHF: Safe Reinforcement Learning\\nfrom Human Feedback. In The Twelfth International Conference on Learning\\nRepresentations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net.\\nhttps://openreview.net/forum?id=TyFrPOKYXw\\n[58] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and\\nRuslan Salakhutdinov. 2019. Transformer-XL: Attentive Language Models\\nbeyond a Fixed-Length Context. In Proceedings of the 57th Conference of the\\nAssociation for Computational Linguistics, ACL 2019, Florence, Italy, July 28-\\nAugust 2, 2019, Volume 1: Long Papers, Anna Korhonen, David R. Traum, and\\nLluís Màrquez (Eds.). Association for Computational Linguistics, 2978–2988.\\nhttps://doi.org/10.18653/V1/P19-1285\\n[59] Tri Dao. 2024.\\nFlashAttention-2: Faster Attention with Better Parallelism\\nand Work Partitioning. In The Twelfth International Conference on Learning\\nRepresentations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net.\\nhttps://openreview.net/forum?id=mZn2Xyh9Ec\\n[60] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher\\nRé. 2022.\\nFlashAttention: Fast and Memory-Efficient Exact Attention\\nwith IO-Awareness. In Advances in Neural Information Processing Sys-\\ntems 35: Annual Conference on Neural Information Processing Systems\\n2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9,\\n2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho,\\nand A. Oh (Eds.).\\nhttp://papers.nips.cc/paper_files/paper/2022/hash/\\n67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html\\n[61] Tri Dao and Albert Gu. 2024. Transformers are SSMs: Generalized Models\\nand Efficient Algorithms Through Structured State Space Duality. In Forty-first\\nInternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July\\n21-27, 2024. OpenReview.net. https://openreview.net/forum?id=ztn8FCR1td\\n[62] Tri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. 2023.\\nFlash-\\nDecoding for long-context inference. https://pytorch.org/blog/flash-decoding/.\\nAccessed: December 15, 2024.\\n[63] Alessio Devoto, Yu Zhao, Simone Scardapane, and Pasquale Minervini. 2024. A\\nSimple and Effective L_2 Norm-Based Strategy for KV Cache Compression. In\\nProceedings of the 2024 Conference on Empirical Methods in Natural Language Pro-\\ncessing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, Yaser Al-Onaizan,\\nMohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Lin-\\nguistics, 18476–18499. https://aclanthology.org/2024.emnlp-main.1027\\n[64] Bosheng Ding, Chengwei Qin, Ruochen Zhao, Tianze Luo, Xinze Li, Guizhen\\nChen, Wenhan Xia, Junjie Hu, Anh Tuan Luu, and Shafiq Joty. 2024. Data\\nAugmentation using LLMs: Data Perspectives, Learning Paradigms and Chal-\\nlenges. In Findings of the Association for Computational Linguistics, ACL 2024,\\nBangkok, Thailand and virtual meeting, August 11-16, 2024, Lun-Wei Ku, Andre\\nMartins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,\\n1679–1705. https://doi.org/10.18653/V1/2024.FINDINGS-ACL.97\\n[65] Peng Ding, Jingyu Wu, Jun Kuang, Dan Ma, Xuezhi Cao, Xunliang Cai, Shi Chen,\\nJiajun Chen, and Shujian Huang. 2024. Hallu-PI: Evaluating Hallucination in\\nMulti-modal Large Language Models within Perturbed Inputs. In Proceedings of\\nthe 32nd ACM International Conference on Multimedia, MM 2024, Melbourne, VIC,\\nAustralia, 28 October 2024 - 1 November 2024, Jianfei Cai, Mohan S. Kankanhalli,\\nBalakrishnan Prabhakaran, Susanne Boll, Ramanathan Subramanian, Liang\\nZheng, Vivek K. Singh, Pablo César, Lexing Xie, and Dong Xu (Eds.). ACM,\\n10707–10715. https://doi.org/10.1145/3664647.3681251\\n[66] Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco,\\nDirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Documenting\\nLarge Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus.\\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language\\nProcessing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11\\nNovember, 2021, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and\\nScott Wen-tau Yih (Eds.). Association for Computational Linguistics, 1286–1305.\\nhttps://doi.org/10.18653/V1/2021.EMNLP-MAIN.98\\n[67] Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil\\nMahabaleshwarkar, Shih-Yang Liu, Matthijs Van Keirsbilck, Min-Hung Chen,\\nYoshi Suhara, et al. 2024. Hymba: A Hybrid-head Architecture for Small Lan-\\nguage Models. arXiv preprint arXiv:2411.13676 (2024).\\n[68] Xin Luna Dong, Seungwhan Moon, Yifan Ethan Xu, Kshitiz Malik, and Zhou\\nYu. 2023. Towards Next-Generation Intelligent Assistants Leveraging LLM\\nTechniques. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge\\nDiscovery and Data Mining, KDD 2023, Long Beach, CA, USA, August 6-10, 2023,\\nAmbuj K. Singh, Yizhou Sun, Leman Akoglu, Dimitrios Gunopulos, Xifeng\\nYan, Ravi Kumar, Fatma Ozcan, and Jieping Ye (Eds.). ACM, 5792–5793. https:\\n//doi.org/10.1145/3580305.3599572\\n[69] Yixin Dong, Charlie F Ruan, Yaxing Cai, Ruihang Lai, Ziyi Xu, Yilong Zhao, and\\nTianqi Chen. 2024. XGrammar: Flexible and Efficient Structured Generation\\nEngine for Large Language Models. arXiv preprint arXiv:2411.15100 (2024).\\n[70] Alexey Drutsa, Valentina Fedorova, Dmitry Ustalov, Olga Megorskaya,\\nEvfrosiniya Zerminova, and Daria Baidakova. 2020. Crowdsourcing Practice\\nfor Efficient Data Labeling: Aggregation, Incremental Relabeling, and Pric-\\ning. In Proceedings of the 2020 International Conference on Management of\\nData, SIGMOD Conference 2020, online conference [Portland, OR, USA], June\\n14-19, 2020, David Maier, Rachel Pottinger, AnHai Doan, Wang-Chiew Tan,\\nAbdussalam Alawini, and Hung Q. Ngo (Eds.). ACM, 2623–2627.\\nhttps:\\n//doi.org/10.1145/3318464.3383127\\n[71] Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin,\\nYuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat,\\nBarret Zoph, Liam Fedus, Maarten P. Bosma, Zongwei Zhou, Tao Wang,\\nYu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen S.\\nMeier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V. Le, Yonghui\\nWu, Zhifeng Chen, and Claire Cui. 2022.\\nGLaM: Efficient Scaling of Lan-\\nguage Models with Mixture-of-Experts. In International Conference on Machine\\nLearning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA (Proceedings\\nof Machine Learning Research, Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka,\\nLe Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (Eds.). PMLR, 5547–5569.\\nhttps://proceedings.mlr.press/v162/du22c.html\\n[72] Yaxin Du, Rui Ye, Yuchi Fengting, Wanru Zhao, Jingjing Qu, Yanfeng Wang, and\\nSiheng Chen. 2024. Data Quality Control in Federated Instruction-tuning of\\nLarge Language Models. CoRR abs/2410.11540 (2024). https://doi.org/10.48550/\\nARXIV.2410.11540 arXiv:2410.11540\\n[73] Fabio Duarte. 2024.\\nNumber of ChatGPT Users (Dec 2024).\\nhttps://\\nexplodingtopics.com/blog/chatgpt-users. Accessed: 2024-12-15.\\n[74] Matous Eibich, Shivay Nagpal, and Alexander Fred-Ojala. 2024. ARAGOG:\\nAdvanced RAG Output Grading. CoRR abs/2404.01037 (2024). https://doi.org/\\n10.48550/ARXIV.2404.01037 arXiv:2404.01037\\n[75] Oluwole Fagbohun, Rachel M. Harrison, and Anton Dereventsov. 2024. An\\nEmpirical Categorization of Prompting Techniques for Large Language Models:\\nA Practitioner’s Guide. CoRR abs/2402.14837 (2024). https://doi.org/10.48550/\\nARXIV.2402.14837 arXiv:2402.14837\\n[76] Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin,\\nTat-Seng Chua, and Qing Li. 2024.\\nA Survey on RAG Meeting LLMs: To-\\nwards Retrieval-Augmented Large Language Models. In Proceedings of the 30th\\nACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2024,\\nBarcelona, Spain, August 25-29, 2024, Ricardo Baeza-Yates and Francesco Bonchi\\n(Eds.). ACM, 6491–6501. https://doi.org/10.1145/3637528.3671470\\n[77] Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. 2024. Detecting\\nhallucinations in large language models using semantic entropy. Nat. 630, 8017\\n(2024), 625–630. https://doi.org/10.1038/S41586-024-07421-0\\n[78] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch Transformers:\\nScaling to Trillion Parameter Models with Simple and Efficient Sparsity. J. Mach.\\nLearn. Res. 23 (2022), 120:1–120:39. https://jmlr.org/papers/v23/21-0998.html\\n[79] Jonathan Feldstein, Paulius Dilkas, Vaishak Belle, and Efthymia Tsamoura. 2024.\\nMapping the Neuro-Symbolic AI Landscape by Architectures: A Handbook on\\nAugmenting Deep Learning Through Symbolic Reasoning. CoRR abs/2410.22077\\n(2024). https://doi.org/10.48550/ARXIV.2410.22077 arXiv:2410.22077\\n[80] Matthew Finlayson, John Hewitt, Alexander Koller, Swabha Swayamdipta, and\\nAshish Sabharwal. 2024. Closing the Curious Case of Neural Text Degeneration.\\nIn The Twelfth International Conference on Learning Representations, ICLR 2024,\\nVienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?\\nid=dONpC9GL1o\\n[81] Evan Frick, Tianle Li, Connor Chen, Wei-Lin Chiang, Anastasios N. Angelopou-\\nlos, Jiantao Jiao, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. 2024.\\nHow to Evaluate Reward Models for RLHF.\\nCoRR abs/2410.14872 (2024).\\nhttps://doi.org/10.48550/ARXIV.2410.14872 arXiv:2410.14872\\n[82] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi,\\nRuiqi Zhong, Scott Yih, Luke Zettlemoyer, and Mike Lewis. 2023. InCoder: A\\nGenerative Model for Code Infilling and Synthesis. In The Eleventh International\\nConference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5,\\n2023. OpenReview.net. https://openreview.net/forum?id=hQwb-lbM6EL\\n[83] Yuhan Fu, Ruobing Xie, Xingwu Sun, Zhanhui Kang, and Xirong Li. 2024. Miti-\\ngating Hallucination in Multimodal Large Language Model via Hallucination-\\ntargeted Direct Preference Optimization. arXiv preprint arXiv:2411.10436 (2024).\\n[84] Yao Fu, Leyang Xue, Yeqi Huang, Andrei-Octavian Brabete, Dmitrii Ustiugov,\\nYuvraj Patel, and Luo Mai. 2024. ServerlessLLM: Locality-Enhanced Serverless\\nInference for Large Language Models. CoRR abs/2401.14351 (2024).\\nhttps:\\n//doi.org/10.48550/ARXIV.2401.14351 arXiv:2401.14351\\n[85] Yichao Fu, Siqi Zhu, Runlong Su, Aurick Qiao, Ion Stoica, and Hao Zhang. 2024.\\nEfficient LLM Scheduling by Learning to Rank. CoRR abs/2408.15792 (2024).\\nhttps://doi.org/10.48550/ARXIV.2408.15792 arXiv:2408.15792\\n[86] Henning Funke and Jens Teubner. 2020. Data-parallel query processing on\\nnon-uniform data. Proceedings of the VLDB Endowment 13, 6 (2020), 884–897.\\n[87] Hang Gao and Yongfeng Zhang. 2024. VRSD: Rethinking Similarity and Diversity\\nfor Retrieval in Large Language Models. CoRR abs/2407.04573 (2024). https:\\n//doi.org/10.48550/ARXIV.2407.04573 arXiv:2407.04573\\n[88] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles\\nFoster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser,\\nand Connor Leahy. 2021. The Pile: An 800GB Dataset of Diverse Text for\\nLanguage Modeling. CoRR abs/2101.00027 (2021). arXiv:2101.00027 https:\\n//arxiv.org/abs/2101.00027\\n[89] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2023. Precise Zero-Shot\\nDense Retrieval without Relevance Labels. In Proceedings of the 61st Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\nACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber,\\nand Naoaki Okazaki (Eds.). Association for Computational Linguistics, 1762–\\n1777. https://doi.org/10.18653/V1/2023.ACL-LONG.99\\n[90] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi,\\nYi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023.\\nRetrieval-Augmented Generation for Large Language Models: A Survey.\\nCoRR abs/2312.10997 (2023).\\nhttps://doi.org/10.48550/ARXIV.2312.10997\\narXiv:2312.10997\\n[91] Aryo Pradipta Gema, Chen Jin, Ahmed Abdulaal, Tom Diethe, Philip Teare,\\nBeatrice Alex, Pasquale Minervini, and Amrutha Saseendran. 2024. DeCoRe:\\nDecoding by Contrasting Retrieval Heads to Mitigate Hallucinations. arXiv\\npreprint arXiv:2410.18860 (2024).\\n[92] Saibo Geng, Berkay Döner, Chris Wendler, Martin Josifoski, and Robert West.\\n2024.\\nSketch-Guided Constrained Decoding for Boosting Blackbox Large\\nLanguage Models without Logit Access. In Proceedings of the 62nd Annual\\nMeeting of the Association for Computational Linguistics, ACL 2024 - Short Pa-\\npers, Bangkok, Thailand, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and\\nVivek Srikumar (Eds.). Association for Computational Linguistics, 234–245.\\nhttps://aclanthology.org/2024.acl-short.23\\n[93] Saibo Geng, Martin Josifoski, Maxime Peyrard, and Robert West. 2023. Grammar-\\nConstrained Decoding for Structured NLP Tasks without Finetuning. In Proceed-\\nings of the 2023 Conference on Empirical Methods in Natural Language Processing,\\nEMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and\\nKalika Bali (Eds.). Association for Computational Linguistics, 10932–10952.\\nhttps://doi.org/10.18653/V1/2023.EMNLP-MAIN.674\\n[94] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney,\\nand Kurt Keutzer. 2021. A Survey of Quantization Methods for Efficient Neural\\nNetwork Inference. CoRR abs/2103.13630 (2021). arXiv:2103.13630 https://arxiv.\\norg/abs/2103.13630\\n[95] In Gim, Seung-seob Lee, and Lin Zhong. 2024. Asynchronous LLM Function\\nCalling. arXiv preprint arXiv:2412.07017 (2024).\\n[96] Michael R. Glass, Xueqing Wu, Ankita Rajaram Naik, Gaetano Rossiello,\\nand Alfio Gliozzo. 2023. Retrieval-Based Transformer for Table Augmenta-\\ntion. In Findings of the Association for Computational Linguistics: ACL 2023,\\nToronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and\\nNaoaki Okazaki (Eds.). Association for Computational Linguistics, 5635–5648.\\nhttps://doi.org/10.18653/V1/2023.FINDINGS-ACL.348\\n[97] Ruihao Gong, Yang Yong, Shiqiao Gu, Yushi Huang, Chengtao Lv, Yunchen\\nZhang, Dacheng Tao, and Xianglong Liu. 2024. LLMC: Benchmarking Large\\nLanguage Model Quantization with a Versatile Compression Toolkit. In Proceed-\\nings of the 2024 Conference on Empirical Methods in Natural Language Processing:\\nEMNLP 2024 - Industry Track, Miami, Florida, USA, November 12-16, 2024, Franck\\nDernoncourt, Daniel Preotiuc-Pietro, and Anastasia Shimorina (Eds.). Associ-\\nation for Computational Linguistics, 132–152. https://aclanthology.org/2024.\\nemnlp-industry.12\\n[98] Yury Gorishniy, Ivan Rubachev, Nikolay Kartashev, Daniil Shlenskii, Akim\\nKotelnikov, and Artem Babenko. 2023. TabR: Unlocking the Power of Retrieval-\\nAugmented Tabular Deep Learning. CoRR abs/2307.14338 (2023). https://doi.\\norg/10.48550/ARXIV.2307.14338 arXiv:2307.14338\\n[99] Albert Gu and Tri Dao. 2023. Mamba: Linear-Time Sequence Modeling with\\nSelective State Spaces. CoRR abs/2312.00752 (2023). https://doi.org/10.48550/\\nARXIV.2312.00752 arXiv:2312.00752\\n[100] Albert Gu, Karan Goel, and Christopher Ré. 2022. Efficiently Modeling Long\\nSequences with Structured State Spaces. In The Tenth International Conference\\non Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenRe-\\nview.net. https://openreview.net/forum?id=uYLFoz1vlAC\\n[101] Nuno Miguel Guerreiro, Duarte M. Alves, Jonas Waldendorf, Barry Haddow,\\nAlexandra Birch, Pierre Colombo, and André F. T. Martins. 2023. Hallucinations\\nin Large Multilingual Translation Models. Trans. Assoc. Comput. Linguistics 11\\n(2023), 1500–1517. https://doi.org/10.1162/TACL_A_00615\\n[102] Çaglar Gülçehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova,\\nLotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen\\nWang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, and\\nNando de Freitas. 2023. Reinforced Self-Training (ReST) for Language Model-\\ning. CoRR abs/2308.08998 (2023). https://doi.org/10.48550/ARXIV.2308.08998\\narXiv:2308.08998\\n[103] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del\\nGiorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa,\\nOlli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien\\nBubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023.\\nTextbooks Are All You Need. CoRR abs/2306.11644 (2023). https://doi.org/10.\\n48550/ARXIV.2306.11644 arXiv:2306.11644\\n[104] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V.\\nChawla, Olaf Wiest, and Xiangliang Zhang. 2024. Large Language Model Based\\nMulti-agents: A Survey of Progress and Challenges. In Proceedings of the Thirty-\\nThird International Joint Conference on Artificial Intelligence, IJCAI 2024, Jeju,\\nSouth Korea, August 3-9, 2024. ijcai.org, 8048–8057.\\nhttps://www.ijcai.org/\\nproceedings/2024/890\\n[105] Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil R.\\nDevanur, Gregory R. Ganger, and Phillip B. Gibbons. 2018. PipeDream: Fast\\nand Efficient Pipeline Parallel DNN Training. CoRR abs/1806.03377 (2018).\\narXiv:1806.03377 http://arxiv.org/abs/1806.03377\\n[106] Jahid Hasan. 2024. Optimizing Large Language Models through Quantiza-\\ntion: A Comparative Analysis of PTQ and QAT Techniques. arXiv preprint\\narXiv:2411.06084 (2024).\\n[107] Horace He, Driss Guessous, Yanbo Liang, and Joy Dong. 2024. FlexAttention:\\nThe Flexibility of PyTorch with the Performance of FlashAttention. https:\\n//pytorch.org/blog/flexattention/.\\n[108] Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo,\\nand Yunhe Wang. 2024. DenseMamba: State Space Models with Dense Hidden\\nConnection for Efficient Large Language Models. CoRR abs/2403.00818 (2024).\\nhttps://doi.org/10.48550/ARXIV.2403.00818 arXiv:2403.00818\\n[109] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann\\nLeCun, Xavier Bresson, and Bryan Hooi. 2024.\\nG-Retriever: Retrieval-\\nAugmented Generation for Textual Graph Understanding and Question Answer-\\ning. CoRR abs/2402.07630 (2024). https://doi.org/10.48550/ARXIV.2402.07630\\narXiv:2402.07630\\n[110] Xu Owen He. 2024. Mixture of A Million Experts. CoRR abs/2407.04153 (2024).\\nhttps://doi.org/10.48550/ARXIV.2407.04153 arXiv:2407.04153\\n[111] Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 2021.\\nScaling Laws for Transfer.\\nCoRR abs/2102.01293 (2021).\\narXiv:2102.01293\\nhttps://arxiv.org/abs/2102.01293\\n[112] Michael Hewing and Vincent Leinhos. 2024. The Prompt Canvas: A Literature-\\nBased Practitioner Guide for Creating Effective Prompts in Large Language\\nModels. arXiv preprint arXiv:2412.05127 (2024).\\n[113] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,\\nTrevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes\\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den\\nDriessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan,\\nErich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training\\nCompute-Optimal Large Language Models. CoRR abs/2203.15556 (2022). https:\\n//doi.org/10.48550/ARXIV.2203.15556 arXiv:2203.15556\\n[114] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The\\nCurious Case of Neural Text Degeneration. In 8th International Conference on\\nLearning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\\nOpenReview.net. https://openreview.net/forum?id=rygGQyrFvH\\n[115] Zijin Hong, Zheng Yuan, Hao Chen, Qinggang Zhang, Feiran Huang, and Xiao\\nHuang. 2024. Knowledge-to-SQL: Enhancing SQL Generation with Data Expert\\nLLM. In Findings of the Association for Computational Linguistics, ACL 2024,\\nBangkok, Thailand and virtual meeting, August 11-16, 2024, Lun-Wei Ku, Andre\\nMartins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,\\n10997–11008. https://doi.org/10.18653/V1/2024.FINDINGS-ACL.653\\n[116] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin\\nde Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\\nParameter-Efficient Transfer Learning for NLP. In Proceedings of the 36th In-\\nternational Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long\\nBeach, California, USA (Proceedings of Machine Learning Research, Vol. 97), Ka-\\nmalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, 2790–2799. http:\\n//proceedings.mlr.press/v97/houlsby19a.html\\n[117] Cheng-Yu Hsieh, Yung-Sung Chuang, Chun-Liang Li, Zifeng Wang, Long T.\\nLe, Abhishek Kumar, James R. Glass, Alexander Ratner, Chen-Yu Lee, Ranjay\\nKrishna, and Tomas Pfister. 2024. Found in the middle: Calibrating Positional\\nAttention Bias Improves Long Context Utilization. In Findings of the Association\\nfor Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting,\\nAugust 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.).\\nAssociation for Computational Linguistics, 14982–14995. https://doi.org/10.\\n18653/V1/2024.FINDINGS-ACL.890\\n[118] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao.\\n2023. ChatDB: Augmenting LLMs with Databases as Their Symbolic Mem-\\nory. CoRR abs/2306.03901 (2023). https://doi.org/10.48550/ARXIV.2306.03901\\narXiv:2306.03901\\n[119] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\\nWang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of\\nLarge Language Models. In The Tenth International Conference on Learning\\nRepresentations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\\nhttps://openreview.net/forum?id=nZeVKeeFYf9\\n[120] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen,\\nMia Xu Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and\\nZhifeng Chen. 2019. GPipe: Efficient Training of Giant Neural Networks us-\\ning Pipeline Parallelism. In Advances in Neural Information Processing Systems\\n32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS\\n2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo\\nLarochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Ro-\\nman Garnett (Eds.). 103–112. https://proceedings.neurips.cc/paper/2019/hash/\\n093f65e080a295f8076b1c5722a46aa2-Abstract.html\\n[121] Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei\\nVassilvitskii, and Sanmi Koyejo. 2024. Scaling Laws for Downstream Task\\nPerformance of Large Language Models. CoRR abs/2402.04177 (2024). https:\\n//doi.org/10.48550/ARXIV.2402.04177 arXiv:2402.04177\\n[122] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler.\\n2021. Data Movement Is All You Need: A Case Study on Optimizing Transform-\\ners. In Proceedings of the Fourth Conference on Machine Learning and Systems,\\nMLSys 2021, virtual, April 5-9, 2021, Alex Smola, Alex Dimakis, and Ion Stoica\\n(Eds.). mlsys.org. https://proceedings.mlsys.org/paper_files/paper/2021/hash/\\nbc86e95606a6392f51f95a8de106728d-Abstract.html\\n[123] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong Park.\\n2024. Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language\\nModels through Question Complexity. In Proceedings of the 2024 Conference of\\nthe North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico\\nCity, Mexico, June 16-21, 2024, Kevin Duh, Helena Gómez-Adorno, and Steven\\nBethard (Eds.). Association for Computational Linguistics, 7036–7050. https:\\n//doi.org/10.18653/V1/2024.NAACL-LONG.389\\n[124] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,\\nYejin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of Hallucination in\\nNatural Language Generation. ACM Comput. Surv. 55, 12 (2023), 248:1–248:38.\\nhttps://doi.org/10.1145/3571730\\n[125] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung.\\n2023. Towards Mitigating Hallucination in Large Language Models via Self-\\nReflection. CoRR abs/2310.06271 (2023). https://doi.org/10.48550/ARXIV.2310.\\n06271 arXiv:2310.06271\\n[126] Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, and Ji-Rong Wen.\\n2023. StructGPT: A General Framework for Large Language Model to Rea-\\nson over Structured Data. In Proceedings of the 2023 Conference on Empirical\\nMethods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10,\\n2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Com-\\nputational Linguistics, 9237–9251. https://doi.org/10.18653/V1/2023.EMNLP-\\nMAIN.574\\n[127] Xuanlin Jiang, Yang Zhou, Shiyi Cao, Ion Stoica, and Minlan Yu. 2024. NEO:\\nSaving GPU Memory Crisis with CPU Offloading for Online LLM Inference.\\narXiv preprint arXiv:2411.01142 (2024).\\n[128] Deokhyung Kang, Baikjin Jung, Yunsu Kim, and Gary Geunbae Lee. 2024. De-\\nnoising Table-Text Retrieval for Open-Domain Question Answering. In Pro-\\nceedings of the 2024 Joint International Conference on Computational Linguis-\\ntics, Language Resources and Evaluation, LREC/COLING 2024, 20-25 May, 2024,\\nTorino, Italy, Nicoletta Calzolari, Min-Yen Kan, Véronique Hoste, Alessandro\\nLenci, Sakriani Sakti, and Nianwen Xue (Eds.). ELRA and ICCL, 4634–4640.\\nhttps://aclanthology.org/2024.lrec-main.414\\n[129] Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar\\nKrishna, and Tuo Zhao. 2024. Gear: An efficient kv cache compression recipefor\\nnear-lossless generative inference of llm. arXiv preprint arXiv:2403.05527 (2024).\\n[130] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin\\nChess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.\\n2020. Scaling Laws for Neural Language Models. CoRR abs/2001.08361 (2020).\\narXiv:2001.08361 https://arxiv.org/abs/2001.08361\\n[131] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu,\\nSergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval\\nfor Open-Domain Question Answering. In Proceedings of the 2020 Conference\\non Empirical Methods in Natural Language Processing, EMNLP 2020, Online,\\nNovember 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu\\n(Eds.). Association for Computational Linguistics, 6769–6781. https://doi.org/\\n10.18653/V1/2020.EMNLP-MAIN.550\\n[132] George Katsogiannis-Meimarakis and Georgia Koutrika. 2021. A Deep Dive into\\nDeep Learning Approaches for Text-to-SQL Systems. In SIGMOD ’21: Interna-\\ntional Conference on Management of Data, Virtual Event, China, June 20-25, 2021,\\nGuoliang Li, Zhanhuai Li, Stratos Idreos, and Divesh Srivastava (Eds.). ACM,\\n2846–2851. https://doi.org/10.1145/3448016.3457543\\n[133] George Katsogiannis-Meimarakis, Mike Xydas, and Georgia Koutrika. 2023.\\nNatural Language Interfaces for Databases with Deep Learning. Proc. VLDB\\nEndow. 16, 12 (2023), 3878–3881. https://doi.org/10.14778/3611540.3611575\\n[134] Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke Hüllermeier. 2023. A\\nSurvey of Reinforcement Learning from Human Feedback. CoRR abs/2312.14925\\n(2023). https://doi.org/10.48550/ARXIV.2312.14925 arXiv:2312.14925\\n[135] Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael\\nBendersky. 2024. Bridging the Preference Gap between Retrievers and LLMs.\\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16,\\n2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for\\nComputational Linguistics, 10438–10451.\\nhttps://doi.org/10.18653/V1/2024.\\nACL-LONG.562\\n[136] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav\\nSanthanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi,\\nHanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. 2024.\\nDSPy: Compiling Declarative Language Model Calls into State-of-the-Art\\nPipelines. In The Twelfth International Conference on Learning Representa-\\ntions, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net.\\nhttps:\\n//openreview.net/forum?id=sY5N0zY5Od\\n[137] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage\\nSearch via Contextualized Late Interaction over BERT. In Proceedings of the 43rd\\nInternational ACM SIGIR conference on research and development in Information\\nRetrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020, Jimmy X. Huang,\\nYi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun\\nLiu (Eds.). ACM, 39–48. https://doi.org/10.1145/3397271.3401075\\n[138] Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon\\nKim, and Chanjun Park. 2024.\\nsDPO: Don’t Use Your Data All at Once.\\nCoRR abs/2403.19270 (2024).\\nhttps://doi.org/10.48550/ARXIV.2403.19270\\narXiv:2403.19270\\n[139] Kyoungmin Kim, Kijae Hong, Caglar Gulcehre, and Anastasia Ailamaki. 2024.\\nThe Effect of Scheduling and Preemption on the Efficiency of LLM Inference\\nServing. arXiv preprint arXiv:2411.07447 (2024).\\n[140] Kyoungmin Kim, Jisung Jung, In Seo, Wook-Shin Han, Kangwoo Choi, and\\nJaehyok Chong. 2022. Learned Cardinality Estimation: An In-depth Study. In\\nSIGMOD ’22: International Conference on Management of Data, Philadelphia, PA,\\nUSA, June 12 - 17, 2022, Zachary G. Ives, Angela Bonifati, and Amr El Abbadi\\n(Eds.). ACM, 1214–1227. https://doi.org/10.1145/3514221.3526154\\n[141] Kyoungmin Kim, Sangoh Lee, Injung Kim, and Wook-Shin Han. 2024. ASM:\\nHarmonizing Autoregressive Model, Sampling, and Multi-dimensional Statistics\\nMerging for Cardinality Estimation. Proc. ACM Manag. Data 2, 1 (2024), 45:1–\\n45:27. https://doi.org/10.1145/3639300\\n[142] Seungone Kim, Juyoung Suk, Xiang Yue, Vijay Viswanathan, Seongyun Lee,\\nYizhong Wang, Kiril Gashteovski, Carolin Lawrence, Sean Welleck, and Graham\\nNeubig. 2024. Evaluating Language Models as Synthetic Data Generators. arXiv\\npreprint arXiv:2412.03679 (2024).\\n[143] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Yacine\\nJernite, Margaret Mitchell, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf,\\nDzmitry Bahdanau, Leandro von Werra, and Harm de Vries. 2023. The Stack: 3\\nTB of permissively licensed source code. Trans. Mach. Learn. Res. 2023 (2023).\\nhttps://openreview.net/forum?id=pxpbTdUEpD\\n[144] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke\\nIwasawa. 2022. Large Language Models are Zero-Shot Reasoners. In Advances in\\nNeural Information Processing Systems 35: Annual Conference on Neural Informa-\\ntion Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 -\\nDecember 9, 2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave,\\nK. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper_files/paper/2022/hash/\\n8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html\\n[145] Juri Kong, Hong Liang, Yuan Zhang, Hongxiang Li, Pengcheng Shen, and Fang\\nLu. 2024. Dynamic semantic memory retention in large language models: An\\nexploration of spontaneous retrieval mechanisms. (2024).\\n[146] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,\\nCody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient\\nMemory Management for Large Language Model Serving with PagedAttention.\\nIn Proceedings of the 29th Symposium on Operating Systems Principles, SOSP\\n2023, Koblenz, Germany, October 23-26, 2023, Jason Flinn, Margo I. Seltzer, Pe-\\nter Druschel, Antoine Kaufmann, and Jonathan Mace (Eds.). ACM, 611–626.\\nhttps://doi.org/10.1145/3600006.3613165\\n[147] Jiedong Lang, Zhehao Guo, and Shuyu Huang. 2024.\\nA Comprehensive\\nStudy on Quantization Techniques for Large Language Models. arXiv preprint\\narXiv:2411.02530 (2024).\\n[148] Sangjin Lee, Alberto Lerner, Philippe Bonnet, and Philippe Cudré-Mauroux.\\n2024. Database Kernels: Seamless Integration of Database Systems and Fast\\nStorage via CXL. In 14th Conference on Innovative Data Systems Research, CIDR\\n2024, Chaminade, HI, USA, January 14-17, 2024. www.cidrdb.org. https://www.\\ncidrdb.org/cidr2024/papers/p43-lee.pdf\\n[149] Wonbeom Lee, Jungi Lee, Junghwan Seo, and Jaewoong Sim. 2024. InfiniGen:\\nEfficient Generative Inference of Large Language Models with Dynamic KV\\nCache Management. In 18th USENIX Symposium on Operating Systems Design\\nand Implementation, OSDI 2024, Santa Clara, CA, USA, July 10-12, 2024, Ada\\nGavrilovska and Douglas B. Terry (Eds.). USENIX Association, 155–172. https:\\n//www.usenix.org/conference/osdi24/presentation/lee\\n[150] Younghun Lee, Sungchul Kim, Tong Yu, Ryan A. Rossi, and Xiang Chen. 2024.\\nLearning to Reduce: Optimal Representations of Structured Data in Prompting\\nLarge Language Models. CoRR abs/2402.14195 (2024). https://doi.org/10.48550/\\nARXIV.2402.14195 arXiv:2402.14195\\n[151] Fangyu Lei, Jixuan Chen, Yuxiao Ye, Ruisheng Cao, Dongchan Shin, Hongjin Su,\\nZhaoqing Suo, Hongcheng Gao, Wenjing Hu, Pengcheng Yin, et al. 2024. Spider\\n2.0: Evaluating language models on real-world enterprise text-to-sql workflows.\\narXiv preprint arXiv:2411.07763 (2024).\\n[152] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale\\nfor Parameter-Efficient Prompt Tuning. In Proceedings of the 2021 Conference\\non Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual\\nEvent / Punta Cana, Dominican Republic, 7-11 November, 2021, Marie-Francine\\nMoens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association\\nfor Computational Linguistics, 3045–3059. https://doi.org/10.18653/V1/2021.\\nEMNLP-MAIN.243\\n[153] Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast Inference from\\nTransformers via Speculative Decoding. In International Conference on Machine\\nLearning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA (Proceedings of Ma-\\nchine Learning Research, Vol. 202), Andreas Krause, Emma Brunskill, Kyunghyun\\nCho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR,\\n19274–19286. https://proceedings.mlr.press/v202/leviathan23a.html\\n[154] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\\nKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim\\nRocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented\\nGeneration for Knowledge-Intensive NLP Tasks. In Advances in Neural In-\\nformation Processing Systems 33: Annual Conference on Neural Information\\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo\\nLarochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and\\nHsuan-Tien Lin (Eds.).\\nhttps://proceedings.neurips.cc/paper/2020/hash/\\n6b493230205f780e1bc26945df7481e5-Abstract.html\\n[155] Dengchun Li, Yingzi Ma, Naizheng Wang, Zhiyuan Cheng, Lei Duan, Jie Zuo,\\nCal Yang, and Mingjie Tang. 2024. MixLoRA: Enhancing Large Language Models\\nFine-Tuning with LoRA based Mixture of Experts. CoRR abs/2404.15159 (2024).\\nhttps://doi.org/10.48550/ARXIV.2404.15159 arXiv:2404.15159\\n[156] Guoliang Li, Xuanhe Zhou, and Lei Cao. 2021. AI Meets Database: AI4DB and\\nDB4AI. In SIGMOD ’21: International Conference on Management of Data, Virtual\\nEvent, China, June 20-25, 2021, Guoliang Li, Zhanhuai Li, Stratos Idreos, and\\nDivesh Srivastava (Eds.). ACM, 2859–2866.\\nhttps://doi.org/10.1145/3448016.\\n3457542\\n[157] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak\\nGadre, Hritik Bansal, Etash Kumar Guha, Sedrick Keh, Kushal Arora, Saurabh\\nGarg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee\\nChen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bit-\\nton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh\\nGardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah M. Pratt, Sunny\\nSanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan,\\nJieyu Zhang, Khyathi Raghavi Chandu, Thao Nguyen, Igor Vasiljevic, Sham M.\\nKakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke\\nZettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander To-\\nshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia\\nJitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Lud-\\nwig Schmidt, and Vaishaal Shankar. 2024. DataComp-LM: In search of the next\\ngeneration of training sets for language models. CoRR abs/2406.11794 (2024).\\nhttps://doi.org/10.48550/ARXIV.2406.11794 arXiv:2406.11794\\n[158] Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin\\nWang, Bowen Qin, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma,\\nGuoliang Li, Kevin Chen-Chuan Chang, Fei Huang, Reynold Cheng, and\\nYongbin Li. 2023.\\nCan LLM Already Serve as A Database Interface? A\\nBIg Bench for Large-Scale Database Grounded Text-to-SQLs. In Advances\\nin Neural Information Processing Systems 36: Annual Conference on Neural\\nInformation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA,\\nDecember 10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate\\nSaenko, Moritz Hardt, and Sergey Levine (Eds.).\\nhttp://papers.nips.cc/\\npaper_files/paper/2023/hash/83fc8fab1710363050bbd1d4b8cc0021-Abstract-\\nDatasets_and_Benchmarks.html\\n[159] Lingyu Li, Yixu Wang, Haiquan Zhao, Shuqi Kong, Yan Teng, Chunbo Li, and\\nYingchun Wang. 2024. Reflection-Bench: probing AI intelligence with reflection.\\narXiv preprint arXiv:2410.16270 (2024).\\n[160] Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\\nDanielle Rifinski Fainman, Dongmei Zhang, and Surajit Chaudhuri. 2024. Table-\\nGPT: Table Fine-tuned GPT for Diverse Table Tasks. Proc. ACM Manag. Data 2,\\n3 (2024), 176. https://doi.org/10.1145/3654979\\n[161] Xiangyu Li, Yuanchun Li, Yuanzhe Li, Ting Cao, and Yunxin Liu. 2024. FlexNN:\\nEfficient and Adaptive DNN Inference on Memory-Constrained Edge Devices.\\nIn Proceedings of the 30th Annual International Conference on Mobile Computing\\nand Networking, ACM MobiCom 2024, Washington D.C., DC, USA, November\\n18-22, 2024, Weisong Shi, Deepak Ganesan, and Nicholas D. Lane (Eds.). ACM,\\n709–723. https://doi.org/10.1145/3636534.3649391\\n[162] Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous\\nPrompts for Generation. In Proceedings of the 59th Annual Meeting of the As-\\nsociation for Computational Linguistics and the 11th International Joint Con-\\nference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long\\nPapers), Virtual Event, August 1-6, 2021, Chengqing Zong, Fei Xia, Wenjie Li, and\\nRoberto Navigli (Eds.). Association for Computational Linguistics, 4582–4597.\\nhttps://doi.org/10.18653/V1/2021.ACL-LONG.353\\n[163] Yichuan Li, Kaize Ding, Jianling Wang, and Kyumin Lee. 2024. Empowering\\nLarge Language Models for Textual Data Augmentation. In Findings of the\\nAssociation for Computational Linguistics, ACL 2024, Bangkok, Thailand and\\nvirtual meeting, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek\\nSrikumar (Eds.). Association for Computational Linguistics, 12734–12751. https:\\n//doi.org/10.18653/V1/2024.FINDINGS-ACL.756\\n[164] Yuhang Li, Mingzhu Shen, Jian Ma, Yan Ren, Mingxin Zhao, Qi Zhang, Ruihao\\nGong, Fengwei Yu, and Junjie Yan. 2021. MQBench: Towards Reproducible\\nand Deployable Model Quantization Benchmark. In Proceedings of the Neural\\nInformation Processing Systems Track on Datasets and Benchmarks 1, NeurIPS\\nDatasets and Benchmarks 2021, December 2021, virtual, Joaquin Vanschoren\\nand Sai-Kit Yeung (Eds.). https://datasets-benchmarks-proceedings.neurips.cc/\\npaper/2021/hash/c20ad4d76fe97759aa27a0c99bff6710-Abstract-round1.html\\n[165] Yuliang Li, Xiaolan Wang, Zhengjie Miao, and Wang-Chiew Tan. 2021. Data\\nAugmentation for ML-driven Data Preparation and Integration. Proc. VLDB\\nEndow. 14, 12 (2021), 3182–3185. https://doi.org/10.14778/3476311.3476403\\n[166] Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky.\\n2024. Retrieval Augmented Generation or Long-Context LLMs? A Compre-\\nhensive Study and Hybrid Approach. In Proceedings of the 2024 Conference\\non Empirical Methods in Natural Language Processing: EMNLP 2024 - Industry\\nTrack, Miami, Florida, USA, November 12-16, 2024, Franck Dernoncourt, Daniel\\nPreotiuc-Pietro, and Anastasia Shimorina (Eds.). Association for Computational\\nLinguistics, 881–893. https://aclanthology.org/2024.emnlp-industry.66\\n[167] Zihao Li, Zhuoran Yang, and Mengdi Wang. 2023.\\nReinforcement Learn-\\ning with Human Feedback: Learning Dynamic Choices via Pessimism.\\nCoRR abs/2305.18438 (2023).\\nhttps://doi.org/10.48550/ARXIV.2305.18438\\narXiv:2305.18438\\n[168] Zhaodonghui Li, Haitao Yuan, Huiming Wang, Gao Cong, and Lidong Bing.\\n2024. LLM-R2: A Large Language Model Enhanced Rule-based Rewrite System\\nfor Boosting Query Efficiency. CoRR abs/2404.12872 (2024). https://doi.org/10.\\n48550/ARXIV.2404.12872 arXiv:2404.12872\\n[169] Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Sihang\\nZhou, Xinwang Liu, Fuchun Sun, and Kunlun He. 2024. A Survey of Knowledge\\nGraph Reasoning on Graph Types: Static, Dynamic, and Multi-Modal. IEEE\\nTrans. Pattern Anal. Mach. Intell. 46, 12 (2024), 9456–9478. https://doi.org/10.\\n1109/TPAMI.2024.3417451\\n[170] Sean Lie. 2023. Cerebras Architecture Deep Dive: First Look Inside the Hard-\\nware/Software Co-Design for Deep Learning. IEEE Micro 43, 3 (2023), 18–30.\\nhttps://doi.org/10.1109/MM.2023.3256384\\n[171] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedi-\\ngos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri\\nAbend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael\\nGokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor\\nZusman, and Yoav Shoham. 2024. Jamba: A Hybrid Transformer-Mamba Lan-\\nguage Model. CoRR abs/2403.19887 (2024). https://doi.org/10.48550/ARXIV.\\n2403.19887 arXiv:2403.19887\\n[172] Jonathan Light, Min Cai, Weiqin Chen, Guanzhi Wang, Xiusi Chen, Wei Cheng,\\nYisong Yue, and Ziniu Hu. 2024. Strategist: Learning Strategic Skills by LLMs\\nvia Bi-Level Tree Search. CoRR abs/2408.10635 (2024). https://doi.org/10.48550/\\nARXIV.2408.10635 arXiv:2408.10635\\n[173] Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen\\nChen, and Lili Qiu. 2024. Parrot: Efficient Serving of LLM-based Applications\\nwith Semantic Variable. In 18th USENIX Symposium on Operating Systems Design\\nand Implementation, OSDI 2024, Santa Clara, CA, USA, July 10-12, 2024, Ada\\nGavrilovska and Douglas B. Terry (Eds.). USENIX Association, 929–945. https:\\n//www.usenix.org/conference/osdi24/presentation/lin-chaofan\\n[174] Yiming Lin and Sharad Mehrotra. 2024. PLAQUE: Automated Predicate Learning\\nat Query Time. Proc. ACM Manag. Data 2, 1 (2024), 46:1–46:25. https://doi.org/\\n10.1145/3639301\\n[175] Zicheng Lin, Tian Liang, Jiahao Xu, Xing Wang, Ruilin Luo, Chufan Shi, Siheng\\nLi, Yujiu Yang, and Zhaopeng Tu. 2024. Critical Tokens Matter: Token-Level\\nContrastive Estimation Enhence LLM’s Reasoning Capability. arXiv preprint\\narXiv:2411.19943 (2024).\\n[176] Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza Haffari, and Bohan\\nZhuang. 2024. MiniCache: KV Cache Compression in Depth Dimension for\\nLarge Language Models. CoRR abs/2405.14366 (2024). https://doi.org/10.48550/\\nARXIV.2405.14366 arXiv:2405.14366\\n[177] Chunwei Liu, Matthew Russo, Michael J. Cafarella, Lei Cao, Peter Baile Chen, Zui\\nChen, Michael J. Franklin, Tim Kraska, Samuel Madden, and Gerardo Vitagliano.\\n2024. A Declarative System for Optimizing AI Workloads. CoRR abs/2405.14696\\n(2024). https://doi.org/10.48550/ARXIV.2405.14696 arXiv:2405.14696\\n[178] Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang,\\nQi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang,\\nYuqing Yang, and Lili Qiu. 2024. RetrievalAttention: Accelerating Long-Context\\nLLM Inference via Vector Retrieval. CoRR abs/2409.10516 (2024). https://doi.\\norg/10.48550/ARXIV.2409.10516 arXiv:2409.10516\\n[179] Hao Liu and Pieter Abbeel. 2023. Emergent Agentic Transformer from Chain of\\nHindsight Experience. In International Conference on Machine Learning, ICML\\n2023, 23-29 July 2023, Honolulu, Hawaii, USA (Proceedings of Machine Learning\\nResearch, Vol. 202), Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara\\nEngelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 21362–21374.\\nhttps://proceedings.mlr.press/v202/liu23a.html\\n[180] Jinshu Liu, Hamid Hadian, Hanchen Xu, Daniel S. Berger, and Huaicheng Li.\\n2024. Dissecting CXL Memory Performance at Scale: Analysis, Modeling, and\\nOptimization. CoRR abs/2409.14317 (2024). https://doi.org/10.48550/ARXIV.\\n2409.14317 arXiv:2409.14317\\n[181] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,\\nFabio Petroni, and Percy Liang. 2024.\\nLost in the Middle: How Language\\nModels Use Long Contexts. Trans. Assoc. Comput. Linguistics 12 (2024), 157–173.\\nhttps://doi.org/10.1162/TACL_A_00638\\n[182] Shu Liu, Asim Biswal, Audrey Cheng, Xiangxi Mo, Shiyi Cao, Joseph E. Gonzalez,\\nIon Stoica, and Matei Zaharia. 2024. Optimizing LLM Queries in Relational\\nWorkloads. CoRR abs/2403.05821 (2024). https://doi.org/10.48550/ARXIV.2403.\\n05821 arXiv:2403.05821\\n[183] Weijie Liu, Zecheng Tang, Juntao Li, Kehai Chen, and Min Zhang.\\n2024.\\nMemLong: Memory-Augmented Retrieval for Long Text Modeling.\\nCoRR abs/2408.16967 (2024).\\nhttps://doi.org/10.48550/ARXIV.2408.16967\\narXiv:2408.16967\\n[184] Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng\\nZhang, Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, Michael\\nMaire, Henry Hoffmann, Ari Holtzman, and Junchen Jiang. 2024. CacheGen:\\nKV Cache Compression and Streaming for Fast Large Language Model Serving.\\nIn Proceedings of the ACM SIGCOMM 2024 Conference, ACM SIGCOMM 2024,\\nSydney, NSW, Australia, August 4-8, 2024. ACM, 38–56. https://doi.org/10.1145/\\n3651890.3672274\\n[185] Yanming Liu, Xinyue Peng, Xuhong Zhang, Weihao Liu, Jianwei Yin, Jiannan\\nCao, and Tianyu Du. 2024. RA-ISF: Learning to Answer and Understand from\\nRetrieval Augmentation via Iterative Self-Feedback. In Findings of the Association\\nfor Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting,\\nAugust 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.).\\nAssociation for Computational Linguistics, 4730–4749. https://doi.org/10.18653/\\nV1/2024.FINDINGS-ACL.281\\n[186] Chao Lou, Zixia Jia, Zilong Zheng, and Kewei Tu. 2024. Sparser is Faster\\nand Less is More: Efficient Sparse Attention for Long-Range Transform-\\ners. CoRR abs/2406.16747 (2024). https://doi.org/10.48550/ARXIV.2406.16747\\narXiv:2406.16747\\n[187] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David\\nHa. 2024. The AI Scientist: Towards Fully Automated Open-Ended Scientific\\nDiscovery. CoRR abs/2408.06292 (2024). https://doi.org/10.48550/ARXIV.2408.\\n06292 arXiv:2408.06292\\n[188] Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Felix\\nBai, Shuang Ma, Shen Ma, Mengyu Li, Guoli Yin, Zirui Wang, and Ruoming\\nPang. 2024. ToolSandbox: A Stateful, Conversational, Interactive Evaluation\\nBenchmark for LLM Tool Use Capabilities. CoRR abs/2408.04682 (2024). https:\\n//doi.org/10.48550/ARXIV.2408.04682 arXiv:2408.04682\\n[189] Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang,\\nNicholas D. Lane, and Mengwei Xu. 2024. Small Language Models: Survey,\\nMeasurements, and Insights. CoRR abs/2409.15790 (2024). https://doi.org/10.\\n48550/ARXIV.2409.15790 arXiv:2409.15790\\n[190] Kaixin Ma, Hao Cheng, Xiaodong Liu, Eric Nyberg, and Jianfeng Gao. 2022.\\nOpen-domain Question Answering via Chain of Reasoning over Heterogeneous\\nKnowledge. In Findings of the Association for Computational Linguistics: EMNLP\\n2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zor-\\nnitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics,\\n5360–5374. https://doi.org/10.18653/V1/2022.FINDINGS-EMNLP.392\\n[191] Ziming Mao, Tian Xia, Zhanghao Wu, Wei-Lin Chiang, Tyler Griggs, Romil\\nBhardwaj, Zongheng Yang, Scott Shenker, and Ion Stoica. 2024.\\nSky-\\nServe: Serving AI Models across Regions and Clouds with Spot Instances.\\nCoRR abs/2411.01438 (2024).\\nhttps://doi.org/10.48550/ARXIV.2411.01438\\narXiv:2411.01438\\n[192] Wes McKinney. 2010. Data Structures for Statistical Computing in Python. In\\nProceedings of the 9th Python in Science Conference, Stéfan van der Walt and\\nJarrod Millman (Eds.). 56–61. https://doi.org/10.25080/Majora-92bf1922-00a\\n[193] Kevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, and David\\nBau. 2023. Mass-Editing Memory in a Transformer. In The Eleventh International\\nConference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5,\\n2023. OpenReview.net. https://openreview.net/forum?id=MkbcAHIYgyS\\n[194] Xupeng Miao, Zhihao Jia, and Bin Cui. 2024. Demystifying Data Management\\nfor Large Language Models. In Companion of the 2024 International Conference\\non Management of Data, SIGMOD/PODS 2024, Santiago AA, Chile, June 9-15, 2024,\\nPablo Barceló, Nayat Sánchez-Pi, Alexandra Meliou, and S. Sudarshan (Eds.).\\nACM, 547–555. https://doi.org/10.1145/3626246.3654683\\n[195] Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham\\nNeubig, Yulia Tsvetkov, and Hannaneh Hajishirzi. 2024. Fine-grained Hallucina-\\ntion Detection and Editing for Language Models. CoRR abs/2401.06855 (2024).\\nhttps://doi.org/10.48550/ARXIV.2401.06855 arXiv:2401.06855\\n[196] M. Mehdi Mojarradi, Lingyi Yang, Robert McCraith, and Adam Mahdi.\\n2024. Improving In-Context Learning with Small Language Model Ensem-\\nbles. CoRR abs/2410.21868 (2024). https://doi.org/10.48550/ARXIV.2410.21868\\narXiv:2410.21868\\n[197] Laurent Mombaerts, Terry Ding, Adi Banerjee, Florian Felice, Jonathan Taws,\\nand Tarik Borogovac. 2024. Meta Knowledge for Retrieval Augmented Large\\nLanguage Models. CoRR abs/2408.09017 (2024). https://doi.org/10.48550/ARXIV.\\n2408.09017 arXiv:2408.09017\\n[198] Niklas Muennighoff, Qian Liu, Armel Randy Zebaze, Qinkai Zheng, Binyuan\\nHui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and\\nShayne Longpre. 2024. OctoPack: Instruction Tuning Code Large Language\\nModels. In The Twelfth International Conference on Learning Representations,\\nICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.\\nnet/forum?id=mw1PWNSWZP\\n[199] Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. 2024. Generative Representational Instruc-\\ntion Tuning. CoRR abs/2402.09906 (2024). https://doi.org/10.48550/ARXIV.2402.\\n09906 arXiv:2402.09906\\n[200] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart\\nvan Baalen, and Tijmen Blankevoort. 2021. A White Paper on Neural Network\\nQuantization. CoRR abs/2106.08295 (2021). arXiv:2106.08295 https://arxiv.org/\\nabs/2106.08295\\n[201] Fatemeh Nargesian, Abolfazl Asudeh, and H. V. Jagadish. 2022. Responsible\\nData Integration: Next-generation Challenges. In SIGMOD ’22: International\\nConference on Management of Data, Philadelphia, PA, USA, June 12 - 17, 2022,\\nZachary G. Ives, Angela Bonifati, and Amr El Abbadi (Eds.). ACM, 2458–2464.\\nhttps://doi.org/10.1145/3514221.3522567\\n[202] Chien Van Nguyen, Xuan Shen, Ryan Aponte, Yu Xia, Samyadeep Basu, Zheng-\\nmian Hu, Jian Chen, Mihir Parmar, Sasidhar Kunapuli, Joe Barrow, Junda Wu,\\nAshish Singh, Yu Wang, Jiuxiang Gu, Franck Dernoncourt, Nesreen K. Ahmed,\\nNedim Lipka, Ruiyi Zhang, Xiang Chen, Tong Yu, Sungchul Kim, Hanieh Deil-\\namsalehy, Namyong Park, Mike Rimer, Zhehao Zhang, Huanrui Yang, Ryan A.\\nRossi, and Thien Huu Nguyen. 2024.\\nA Survey of Small Language Mod-\\nels. CoRR abs/2410.20011 (2024). https://doi.org/10.48550/ARXIV.2410.20011\\narXiv:2410.20011\\n[203] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma,\\nVincent Y. Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. 2022.\\nLarge Dual Encoders Are Generalizable Retrievers. In Proceedings of the 2022\\nConference on Empirical Methods in Natural Language Processing, EMNLP 2022,\\nAbu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zornitsa\\nKozareva, and Yue Zhang (Eds.). Association for Computational Linguistics,\\n9844–9855. https://doi.org/10.18653/V1/2022.EMNLP-MAIN.669\\n[204] Noa Nonkes, Sergei Agaronian, Evangelos Kanoulas, and Roxana Petcu. 2024.\\nLeveraging Graph Structures to Detect Hallucinations in Large Language Mod-\\nels. CoRR abs/2407.04485 (2024). https://doi.org/10.48550/ARXIV.2407.04485\\narXiv:2407.04485\\n[205] OpenAI. 2024. Introducing OpenAI o1. https://openai.com/o1.\\nAccessed:\\n2024-12-15.\\n[206] Laurel J. Orr, Srikanth Kandula, and Surajit Chaudhuri. 2019. Pushing Data-\\nInduced Predicates Through Joins in Big-Data Clusters. Proc. VLDB Endow. 13,\\n3 (2019), 252–265. https://doi.org/10.14778/3368289.3368292\\n[207] Laurel J. Orr, Atindriyo Sanyal, Xiao Ling, Karan Goel, and Megan Leszczyn-\\nski. 2021.\\nManaging ML Pipelines: Feature Stores and the Coming Wave\\nof Embedding Ecosystems.\\nProc. VLDB Endow. 14, 12 (2021), 3178–3181.\\nhttps://doi.org/10.14778/3476311.3476402\\n[208] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,\\nPamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,\\nAmanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\\n2022. Training language models to follow instructions with human feedback. In\\nAdvances in Neural Information Processing Systems 35: Annual Conference on Neu-\\nral Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, No-\\nvember 28 - December 9, 2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle\\nBelgrave, K. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper_files/paper/\\n2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html\\n[209] Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin\\nMuller, Margaret Li, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer,\\nGargi Ghosh, Mike Lewis, Ari Holtzman, and Srini Iyer. 2024. Byte Latent Trans-\\nformer: Patches Scale Better Than Tokens. (2024). https://ai.meta.com/research/\\npublications/byte-latent-transformer-patches-scale-better-than-tokens/\\n[210] James Jie Pan, Jianguo Wang, and Guoliang Li. 2024. Survey of vector database\\nmanagement systems. VLDB J. 33, 5 (2024), 1591–1615. https://doi.org/10.1007/\\nS00778-024-00864-X\\n[211] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu.\\n2024. Unifying large language models and knowledge graphs: A roadmap. IEEE\\nTransactions on Knowledge and Data Engineering (2024).\\n[212] Xuchen Pan, Dawei Gao, Yuexiang Xie, Zhewei Wei, Yaliang Li, Bolin Ding,\\nJi-Rong Wen, and Jingren Zhou. 2024. Very Large-Scale Multi-Agent Simulation\\nin AgentScope. CoRR abs/2407.17789 (2024). https://doi.org/10.48550/ARXIV.\\n2407.17789 arXiv:2407.17789\\n[213] Xiurui Pan, Endian Li, Qiao Li, Shengwen Liang, Yizhou Shan, Ke Zhou, Yingwei\\nLuo, Xiaolin Wang, and Jie Zhang. 2024. InstInfer: In-Storage Attention Of-\\nfloading for Cost-Effective Long-Context LLM Inference. CoRR abs/2409.04992\\n(2024). https://doi.org/10.48550/ARXIV.2409.04992 arXiv:2409.04992\\n[214] Joon Sung Park, Joseph C. O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy\\nLiang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra\\nof Human Behavior. In Proceedings of the 36th Annual ACM Symposium on\\nUser Interface Software and Technology, UIST 2023, San Francisco, CA, USA, 29\\nOctober 2023- 1 November 2023, Sean Follmer, Jeff Han, Jürgen Steimle, and\\nNathalie Henry Riche (Eds.). ACM, 2:1–2:22. https://doi.org/10.1145/3586183.\\n3606763\\n[215] Dylan Patel and Afzal Ahmad. 2023. The Inference Cost of Search Disruption –\\nLarge Language Model Cost Analysis. https://www.semianalysis.com/p/the-\\ninference-cost-of-search-disruption. Accessed: 2024-12-15.\\n[216] Liana Patel, Siddharth Jha, Carlos Guestrin, and Matei Zaharia. 2024. LOTUS: En-\\nabling Semantic Queries with LLMs Over Tables of Unstructured and Structured\\nData. CoRR abs/2407.11418 (2024). https://doi.org/10.48550/ARXIV.2407.11418\\narXiv:2407.11418\\n[217] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed\\nMaleki, and Ricardo Bianchini. 2024. Splitwise: Efficient Generative LLM Infer-\\nence Using Phase Splitting. In 51st ACM/IEEE Annual International Symposium\\non Computer Architecture, ISCA 2024, Buenos Aires, Argentina, June 29 - July 3,\\n2024. IEEE, 118–132. https://doi.org/10.1109/ISCA59077.2024.00019\\n[218] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023. Gorilla:\\nLarge Language Model Connected with Massive APIs. CoRR abs/2305.15334\\n(2023). https://doi.org/10.48550/ARXIV.2305.15334 arXiv:2305.15334\\n[219] Johns Paul, Bingsheng He, Shengliang Lu, and Chiew Tong Lau. 2020. Improving\\nexecution efficiency of just-in-time compilation based query processing on GPUs.\\nProceedings of the VLDB Endowment 14, 2 (2020), 202–214.\\n[220] Johns Paul, Shengliang Lu, and Bingsheng He. 2021. Database Systems on GPUs.\\nFound. Trends Databases 11, 1 (2021), 1–108. https://doi.org/10.1561/1900000076\\n[221] Tim Pearce and Jinyeop Song. 2024. Reconciling Kaplan and Chinchilla Scaling\\nLaws. CoRR abs/2406.12907 (2024). https://doi.org/10.48550/ARXIV.2406.12907\\narXiv:2406.12907\\n[222] Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna\\nGurevych. 2021. AdapterFusion: Non-Destructive Task Composition for Transfer\\nLearning. In Proceedings of the 16th Conference of the European Chapter of the\\nAssociation for Computational Linguistics: Main Volume, EACL 2021, Online,\\nApril 19 - 23, 2021, Paola Merlo, Jörg Tiedemann, and Reut Tsarfaty (Eds.).\\nAssociation for Computational Linguistics, 487–503. https://doi.org/10.18653/\\nV1/2021.EACL-MAIN.39\\n[223] Jonathan Pilault, Mahan Fathi, Orhan Firat, Chris Pal, Pierre-Luc Bacon, and\\nRoss Goroshin. 2023. Block-State Transformers. In Advances in Neural Infor-\\nmation Processing Systems 36: Annual Conference on Neural Information Pro-\\ncessing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,\\n2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt,\\nand Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/\\n16ccd203e9e3696a7ab0dcf568316379-Abstract-Conference.html\\n[224] Mohammadreza Pourreza, Hailong Li, Ruoxi Sun, Yeounoh Chung, Shayan\\nTalaei, Gaurav Tarlok Kakkar, Yu Gan, Amin Saberi, Fatma Ozcan, and Sercan Ö.\\nArik. 2024. CHASE-SQL: Multi-Path Reasoning and Preference Optimized\\nCandidate Selection in Text-to-SQL. CoRR abs/2410.01943 (2024). https://doi.\\norg/10.48550/ARXIV.2410.01943 arXiv:2410.01943\\n[225] Tavva Prudhvith, Chakrabarty Swattik, and Selvakumar Prakash. 2024. En-\\nhancing Retrieval Augmented Generation Systems with Knowledge Graphs. In\\n2024 International Conference on Electrical, Computer and Energy Technologies\\n(ICECET. IEEE, 1–8.\\n[226] Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and\\nMao Yang. 2024. Mutual Reasoning Makes Smaller LLMs Stronger Problem-\\nSolvers. CoRR abs/2408.06195 (2024).\\nhttps://doi.org/10.48550/ARXIV.2408.\\n06195 arXiv:2408.06195\\n[227] Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize\\nChen, Cheng Yang, Zhiyuan Liu, and Maosong Sun. 2024.\\nScaling Large-\\nLanguage-Model-based Multi-Agent Collaboration. CoRR abs/2406.07155 (2024).\\nhttps://doi.org/10.48550/ARXIV.2406.07155 arXiv:2406.07155\\n[228] Yulei Qian, Fengcun Li, Xiangyang Ji, Xiaoyu Zhao, Jianchao Tan, Kefeng Zhang,\\nand Xunliang Cai. 2024. EPS-MoE: Expert Pipeline Scheduler for Cost-Efficient\\nMoE Inference. CoRR abs/2410.12247 (2024). https://doi.org/10.48550/ARXIV.\\n2410.12247 arXiv:2410.12247\\n[229] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin,\\nXin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian,\\nRuobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong\\nSun. 2024. ToolLLM: Facilitating Large Language Models to Master 16000+ Real-\\nworld APIs. In The Twelfth International Conference on Learning Representations,\\nICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.\\nnet/forum?id=dHng2O0Jjr\\n[230] Han Qiu, Jiaxing Huang, Peng Gao, Qin Qi, Xiaoqin Zhang, Ling Shao, and\\nShijian Lu. 2024.\\nLongHalQA: Long-Context Hallucination Evaluation for\\nMultiModal Large Language Models. CoRR abs/2410.09962 (2024).\\nhttps:\\n//doi.org/10.48550/ARXIV.2410.09962 arXiv:2410.09962\\n[231] Haoran Qiu, Weichao Mao, Archit Patke, Shengkun Cui, Saurabh Jha, Chen\\nWang, Hubertus Franke, Zbigniew T. Kalbarczyk, Tamer Basar, and Ravis-\\nhankar K. Iyer. 2024. Efficient Interactive LLM Serving with Proxy Model-\\nbased Sequence Length Prediction.\\nCoRR abs/2404.08509 (2024).\\nhttps:\\n//doi.org/10.48550/ARXIV.2404.08509 arXiv:2404.08509\\n[232] Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei\\nYin, Jun Xu, and Ji-Rong Wen. 2024. Towards Completeness-Oriented Tool Re-\\ntrieval for Large Language Models. In Proceedings of the 33rd ACM International\\nConference on Information and Knowledge Management, CIKM 2024, Boise, ID,\\nUSA, October 21-25, 2024, Edoardo Serra and Francesca Spezzano (Eds.). ACM,\\n1930–1940. https://doi.org/10.1145/3627673.3679847\\n[233] Ernesto Quevedo, Jorge Yero, Rachel Koerner, Pablo Rivas, and Tomás Cerný.\\n2024. Detecting Hallucinations in Large Language Model Generation: A Token\\nProbability Approach. CoRR abs/2405.19648 (2024). https://doi.org/10.48550/\\nARXIV.2405.19648 arXiv:2405.19648\\n[234] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Ste-\\nfano Ermon, and Chelsea Finn. 2023. Direct Preference Optimization: Your\\nLanguage Model is Secretly a Reward Model. In Advances in Neural Infor-\\nmation Processing Systems 36: Annual Conference on Neural Information Pro-\\ncessing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,\\n2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt,\\nand Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/\\na85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html\\n[235] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits\\nof Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn.\\nRes. 21 (2020), 140:1–140:67. https://jmlr.org/papers/v21/20-074.html\\n[236] Isaac Rehg. 2024. KV-Compress: Paged KV-Cache Compression with Variable\\nCompression Rates per Attention Head. CoRR abs/2410.00161 (2024). https:\\n//doi.org/10.48550/ARXIV.2410.00161 arXiv:2410.00161\\n[237] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen.\\n2024. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context\\nLanguage Modeling. arXiv preprint arXiv:2406.07522 (2024).\\n[238] Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar, and Stephen W.\\nKeckler. 2016. vDNN: Virtualized deep neural networks for scalable, memory-\\nefficient neural network design. In 49th Annual IEEE/ACM International Sympo-\\nsium on Microarchitecture, MICRO 2016, Taipei, Taiwan, October 15-19, 2016. IEEE\\nComputer Society, 18:1–18:13. https://doi.org/10.1109/MICRO.2016.7783721\\n[239] Guilherme Rosa, Luiz Bonifacio, Vitor Jeronymo, Hugo Abonizio, Marzieh\\nFadaee, Roberto Lotufo, and Rodrigo Nogueira. 2022.\\nIn defense of cross-\\nencoders for zero-shot retrieval. arXiv preprint arXiv:2212.06121 (2022).\\n[240] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal,\\nand Aman Chadha. 2024. A Systematic Survey of Prompt Engineering in Large\\nLanguage Models: Techniques and Applications. CoRR abs/2402.07927 (2024).\\nhttps://doi.org/10.48550/ARXIV.2402.07927 arXiv:2402.07927\\n[241] Gaurav Sahu, Pau Rodríguez, Issam H. Laradji, Parmida Atighehchian, David\\nVázquez, and Dzmitry Bahdanau. 2022. Data Augmentation for Intent Classi-\\nfication with Off-the-shelf Large Language Models. In Proceedings of the 4th\\nWorkshop on NLP for Conversational AI, ConvAI@ACL 2022, Dublin, Ireland,\\nMay 27, 2022, Bing Liu, Alexandros Papangelis, Stefan Ultes, Abhinav Rastogi,\\nYun-Nung Chen, Georgios Spithourakis, Elnaz Nouri, and Weiyan Shi (Eds.).\\nAssociation for Computational Linguistics, 47–57. https://doi.org/10.18653/V1/\\n2022.NLP4CONVAI-1.5\\n[242] Viktor Sanca and Anastasia Ailamaki. 2024. Efficient Data Access Paths for\\nMixed Vector-Relational Search. In Proceedings of the 20th International Workshop\\non Data Management on New Hardware, DaMoN 2024, Santiago, Chile, 10 June\\n2024, Carsten Binnig and Nesime Tatbul (Eds.). ACM, 6:1–6:9. https://doi.org/\\n10.1145/3662010.3663448\\n[243] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika,\\nZaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful\\nBari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla,\\nTaewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan\\nChang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin\\nYong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos\\nRozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason Alan Fries,\\nRyan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and\\nAlexander M. Rush. 2022. Multitask Prompted Training Enables Zero-Shot Task\\nGeneralization. In The Tenth International Conference on Learning Representations,\\nICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. https://openreview.\\nnet/forum?id=9Vrb9D0WI4\\n[244] Keshav Santhanam, Deepti Raghavan, Muhammad Shahir Rahman, Thejas\\nVenkatesh, Neha Kunjal, Pratiksha Thaker, Philip Alexander Levis, and Matei\\nZaharia. 2024. ALTO: An Efficient Network Orchestrator for Compound AI\\nSystems. In Proceedings of the 4th Workshop on Machine Learning and Systems,\\nEuroMLSys 2024, Athens, Greece, 22 April 2024. ACM, 117–125. https://doi.org/\\n10.1145/3642970.3655844\\n[245] Bhaskarjit Sarmah, Dhagash Mehta, Benika Hall, Rohan Rao, Sunil Patel, and\\nStefano Pasquali. 2024. HybridRAG: Integrating Knowledge Graphs and Vec-\\ntor Retrieval Augmented Generation for Efficient Information Extraction. In\\nProceedings of the 5th ACM International Conference on AI in Finance. 608–616.\\n[246] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli,\\nEric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.\\nToolformer: Language Models Can Teach Themselves to Use Tools. In Advances\\nin Neural Information Processing Systems 36: Annual Conference on Neural Infor-\\nmation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December\\n10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz\\nHardt, and Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/\\nhash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html\\n[247] Sander Schulhoff, Michael Ilie, Nishant Balepur, Konstantine Kahadze, Amanda\\nLiu, Chenglei Si, Yinheng Li, Aayush Gupta, HyoJung Han, Sevien Schulhoff,\\nPranav Sandeep Dulepet, Saurav Vidyadhara, Dayeon Ki, Sweta Agrawal, Chau\\nPham, Gerson C. Kroiz, Feileen Li, Hudson Tao, Ashay Srivastava, Hevan-\\nder Da Costa, Saloni Gupta, Megan L. Rogers, Inna Goncearenco, Giuseppe\\nSarli, Igor Galynker, Denis Peskoff, Marine Carpuat, Jules White, Shyamal Anad-\\nkat, Alexander Miserlis Hoyle, and Philip Resnik. 2024. The Prompt Report:\\nA Systematic Survey of Prompting Techniques. CoRR abs/2406.06608 (2024).\\nhttps://doi.org/10.48550/ARXIV.2406.06608 arXiv:2406.06608\\n[248] Robert Schulze, Tom Schreiber, Ilya Yatsishin, Ryadh Dahimene, and Alexey\\nMilovidov. 2024. ClickHouse-Lightning Fast Analytics for Everyone. Proceedings\\nof the VLDB Endowment 17, 12 (2024), 3731–3744.\\n[249] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and\\nTri Dao. 2024. FlashAttention-3: Fast and Accurate Attention with Asynchrony\\nand Low-precision. CoRR abs/2407.08608 (2024).\\nhttps://doi.org/10.48550/\\nARXIV.2407.08608 arXiv:2407.08608\\n[250] Lior Shani, Aviv Rosenberg, Asaf B. Cassel, Oran Lang, Daniele Calandriello,\\nAvital Zipori, Hila Noga, Orgad Keller, Bilal Piot, Idan Szpektor, Avinatan\\nHassidim, Yossi Matias, and Rémi Munos. 2024. Multi-turn Reinforcement\\nLearning from Preference Human Feedback.\\nCoRR abs/2405.14655 (2024).\\nhttps://doi.org/10.48550/ARXIV.2405.14655 arXiv:2405.14655\\n[251] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li,\\nKaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. 2024. OmniQuant: Omnidirec-\\ntionally Calibrated Quantization for Large Language Models. In The Twelfth Inter-\\nnational Conference on Learning Representations, ICLR 2024, Vienna, Austria, May\\n7-11, 2024. OpenReview.net. https://openreview.net/forum?id=8Wuvhh0LYW\\n[252] Ying Sheng, Shiyi Cao, Dacheng Li, Banghua Zhu, Zhuohan Li, Danyang\\nZhuo, Joseph E. Gonzalez, and Ion Stoica. 2024. Fairness in Serving Large\\nLanguage Models. In 18th USENIX Symposium on Operating Systems Design\\nand Implementation, OSDI 2024, Santa Clara, CA, USA, July 10-12, 2024, Ada\\nGavrilovska and Douglas B. Terry (Eds.). USENIX Association, 965–988. https:\\n//www.usenix.org/conference/osdi24/presentation/sheng\\n[253] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi\\nChen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. 2023. FlexGen:\\nHigh-Throughput Generative Inference of Large Language Models with a Single\\nGPU. In International Conference on Machine Learning, ICML 2023, 23-29 July\\n2023, Honolulu, Hawaii, USA (Proceedings of Machine Learning Research, Vol. 202),\\nAndreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan\\nSabato, and Jonathan Scarlett (Eds.). PMLR, 31094–31116. https://proceedings.\\nmlr.press/v202/sheng23a.html\\n[254] Nikhil Sheoran, Supawit Chockchowwat, Arav Chheda, Suwen Wang, Riya\\nVerma, and Yongjoo Park. 2023. A Step Toward Deep Online Aggregation. Proc.\\nACM Manag. Data 1, 2 (2023), 124:1–124:28. https://doi.org/10.1145/3589269\\n[255] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer\\nSingh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with\\nAutomatically Generated Prompts. In Proceedings of the 2020 Conference on\\nEmpirical Methods in Natural Language Processing, EMNLP 2020, Online, Novem-\\nber 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.).\\nAssociation for Computational Linguistics, 4222–4235. https://doi.org/10.18653/\\nV1/2020.EMNLP-MAIN.346\\n[256] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan,\\nand Shunyu Yao. 2023.\\nReflexion: language agents with verbal rein-\\nforcement learning. In Advances in Neural Information Processing Sys-\\ntems 36: Annual Conference on Neural Information Processing Systems 2023,\\nNeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, Alice\\nOh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and\\nSergey Levine (Eds.).\\nhttp://papers.nips.cc/paper_files/paper/2023/hash/\\n1b44b878bb782e6954cd888628510e90-Abstract-Conference.html\\n[257] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared\\nCasper, and Bryan Catanzaro. 2019.\\nMegatron-LM: Training Multi-Billion\\nParameter Language Models Using Model Parallelism. CoRR abs/1909.08053\\n(2019). arXiv:1909.08053 http://arxiv.org/abs/1909.08053\\n[258] Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jor-\\ndan L. Boyd-Graber, and Lijuan Wang. 2023. Prompting GPT-3 To Be Reliable.\\nIn The Eleventh International Conference on Learning Representations, ICLR 2023,\\nKigali, Rwanda, May 1-5, 2023. OpenReview.net. https://openreview.net/forum?\\nid=98p5x51L5af\\n[259] Tomer Simon. 2024. The scientist of the scientist. AI Soc. 39, 2 (2024), 803–804.\\nhttps://doi.org/10.1007/S00146-022-01544-6\\n[260] Panagiotis Sioulas, Viktor Sanca, Ioannis Mytilinis, and Anastasia Ailamaki.\\n2021. Accelerating Complex Analytics using Speculation. In 11th Conference\\non Innovative Data Systems Research, CIDR 2021, Virtual Event, January 11-15,\\n2021, Online Proceedings. www.cidrdb.org. http://cidrdb.org/cidr2021/papers/\\ncidr2021_paper03.pdf\\n[261] Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen. 2024. PowerInfer: Fast\\nLarge Language Model Serving with a Consumer-grade GPU. In Proceedings\\nof the ACM SIGOPS 30th Symposium on Operating Systems Principles, SOSP\\n2024, Austin, TX, USA, November 4-6, 2024, Emmett Witchel, Christopher J.\\nRossbach, Andrea C. Arpaci-Dusseau, and Kimberly Keeton (Eds.). ACM, 590–\\n606. https://doi.org/10.1145/3694715.3695964\\n[262] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe,\\nChelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. 2020.\\nLearning to summarize with human feedback. In Advances in Neural In-\\nformation Processing Systems 33: Annual Conference on Neural Information\\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo\\nLarochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and\\nHsuan-Tien Lin (Eds.).\\nhttps://proceedings.neurips.cc/paper/2020/hash/\\n1f89885d556929e98d3ef9b86448f951-Abstract.html\\n[263] Foteini Strati, Sara McAllister, Amar Phanishayee, Jakub Tarnawski, and Ana\\nKlimovic. 2024. DéjàVu: KV-cache Streaming for Fast, Fault-tolerant Generative\\nLLM Serving. In Forty-first International Conference on Machine Learning, ICML\\n2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. https://openreview.\\nnet/forum?id=AbGbGZFYOD\\n[264] Weihang Su, Changyue Wang, Qingyao Ai, Yiran Hu, Zhijing Wu, Yujia Zhou,\\nand Yiqun Liu. 2024. Unsupervised Real-Time Hallucination Detection based\\non the Internal States of Large Language Models. In Findings of the Association\\nfor Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting,\\nAugust 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.).\\nAssociation for Computational Linguistics, 14379–14391. https://doi.org/10.\\n18653/V1/2024.FINDINGS-ACL.854\\n[265] Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. 2024.\\nTable meets llm: Can large language models understand structured table data?\\na benchmark and empirical study. In Proceedings of the 17th ACM International\\nConference on Web Search and Data Mining. 645–654.\\n[266] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin,\\nYeyun Gong, Heung-Yeung Shum, and Jian Guo. 2023.\\nThink-on-Graph:\\nDeep and Responsible Reasoning of Large Language Model with Knowledge\\nGraph. CoRR abs/2307.07697 (2023). https://doi.org/10.48550/ARXIV.2307.07697\\narXiv:2307.07697\\n[267] Xin Tan, Yimin Jiang, Yitao Yang, and Hong Xu. 2024. Teola: Towards End-\\nto-End Optimization of LLM-based Applications. CoRR abs/2407.00326 (2024).\\nhttps://doi.org/10.48550/ARXIV.2407.00326 arXiv:2407.00326\\n[268] James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel,\\nand Alon Y. Levy. 2021. From Natural Language Processing to Neural Databases.\\nProc. VLDB Endow. 14, 6 (2021), 1033–1039. https://doi.org/10.14778/3447689.\\n3447706\\n[269] Bing Tian, Haikun Liu, Yuhang Tang, Shihai Xiao, Zhuohui Duan, Xiaofei Liao,\\nXuecang Zhang, Junhua Zhu, and Yu Zhang. 2024. FusionANNS: An Efficient\\nCPU/GPU Cooperative Processing Architecture for Billion-scale Approximate\\nNearest Neighbor Search. CoRR abs/2409.16576 (2024). https://doi.org/10.48550/\\nARXIV.2409.16576 arXiv:2409.16576\\n[270] S. M. Towhidul Islam Tonmoy, S. M. Mehedi Zaman, Vinija Jain, Anku\\nRani, Vipula Rawte, Aman Chadha, and Amitava Das. 2024. A Comprehen-\\nsive Survey of Hallucination Mitigation Techniques in Large Language Mod-\\nels. CoRR abs/2401.01313 (2024). https://doi.org/10.48550/ARXIV.2401.01313\\narXiv:2401.01313\\n[271] Immanuel Trummer. 2022. DB-BERT: A Database Tuning Tool that \"Reads\\nthe Manual\". In SIGMOD ’22: International Conference on Management of Data,\\nPhiladelphia, PA, USA, June 12 - 17, 2022, Zachary G. Ives, Angela Bonifati, and\\nAmr El Abbadi (Eds.). ACM, 190–203. https://doi.org/10.1145/3514221.3517843\\n[272] Immanuel Trummer. 2023. From BERT to GPT-3 Codex: Harnessing the Potential\\nof Very Large Language Models for Data Management. CoRR abs/2306.09339\\n(2023). https://doi.org/10.48550/ARXIV.2306.09339 arXiv:2306.09339\\n[273] Matthias Urban and Carsten Binnig. 2024. CAESURA: Language Models as\\nMulti-Modal Query Planners. In 14th Conference on Innovative Data Systems\\nResearch, CIDR 2024, Chaminade, HI, USA, January 14-17, 2024. www.cidrdb.org.\\nhttps://www.cidrdb.org/cidr2024/papers/p14-urban.pdf\\n[274] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.\\nAttention is\\nAll you Need. In Advances in Neural Information Processing Systems 30: An-\\nnual Conference on Neural Information Processing Systems 2017, December 4-\\n9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy\\nBengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman\\nGarnett (Eds.). 5998–6008.\\nhttps://proceedings.neurips.cc/paper/2017/hash/\\n3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\\n[275] Shubham Vatsal and Harsh Dubey. 2024. A Survey of Prompt Engineering Meth-\\nods in Large Language Models for Different NLP Tasks. CoRR abs/2407.12994\\n(2024). https://doi.org/10.48550/ARXIV.2407.12994 arXiv:2407.12994\\n[276] Jonas Waldendorf, Barry Haddow, and Alexandra Birch. 2024. Contrastive\\nDecoding Reduces Hallucinations in Large Multilingual Machine Translation\\nModels. In Proceedings of the 18th Conference of the European Chapter of the\\nAssociation for Computational Linguistics, EACL 2024 - Volume 1: Long Papers, St.\\nJulian’s, Malta, March 17-22, 2024, Yvette Graham and Matthew Purver (Eds.).\\nAssociation for Computational Linguistics, 2526–2539. https://aclanthology.\\norg/2024.eacl-long.155\\n[277] Fei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen, and Sercan Ö. Arik. 2024.\\nAstute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\\nConflicts for Large Language Models. CoRR abs/2410.07176 (2024).\\nhttps:\\n//doi.org/10.48550/ARXIV.2410.07176 arXiv:2410.07176\\n[278] Jinyuan Wang, Junlong Li, and Hai Zhao. 2023. Self-prompted chain-of-thought\\non large language models for open-domain multi-hop reasoning. arXiv preprint\\narXiv:2310.13552 (2023).\\n[279] Ke Wang, Jiahui Zhu, Minjie Ren, Zeming Liu, Shiwei Li, Zongye Zhang,\\nChenkai Zhang, Xiaoyu Wu, Qiqi Zhan, Qingjie Liu, and Yunhong Wang.\\n2024. A Survey on Data Synthesis and Augmentation for Large Language Mod-\\nels. CoRR abs/2410.12896 (2024). https://doi.org/10.48550/ARXIV.2410.12896\\narXiv:2410.12896\\n[280] Wenyi Wang, Hisham A Alyahya, Dylan R Ashley, Oleg Serikov, Dmitrii\\nKhizbullin, Francesco Faccio, and Jürgen Schmidhuber. 2024. How to Cor-\\nrectly do Semantic Backpropagation on Language-based Agentic Systems. arXiv\\npreprint arXiv:2412.03624 (2024).\\n[281] Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and\\nFuru Wei. 2023. Augmenting Language Models with Long-Term Memory. In Ad-\\nvances in Neural Information Processing Systems 36: Annual Conference on Neural\\nInformation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, De-\\ncember 10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko,\\nMoritz Hardt, and Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/\\n2023/hash/ebd82705f44793b6f9ade5a669d0f0bf-Abstract-Conference.html\\n[282] Xiaochen Wang, Junqing He, Liang Chen, Reza Haf Zhe Yang, Yiru Wang,\\nXiangdi Meng, Kunhao Pan, and Zhifang Sui. 2024. SG-FSM: A Self-Guiding\\nZero-Shot Prompting Paradigm for Multi-Hop Question Answering Based on\\nFinite State Machine. arXiv preprint arXiv:2410.17021 (2024).\\n[283] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan\\nNarang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-Consistency Im-\\nproves Chain of Thought Reasoning in Language Models. In The Eleventh Inter-\\nnational Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May\\n1-5, 2023. OpenReview.net. https://openreview.net/forum?id=1PL1NIMMrw\\n[284] Xuezhi Wang and Denny Zhou. 2024. Chain-of-Thought Reasoning Without\\nPrompting. CoRR abs/2402.10200 (2024). https://doi.org/10.48550/ARXIV.2402.\\n10200 arXiv:2402.10200\\n[285] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith,\\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-Instruct: Aligning Lan-\\nguage Models with Self-Generated Instructions. In Proceedings of the 61st Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\nACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber,\\nand Naoaki Okazaki (Eds.). Association for Computational Linguistics, 13484–\\n13508. https://doi.org/10.18653/V1/2023.ACL-LONG.754\\n[286] Zhiruo Wang, Zhoujun Cheng, Hao Zhu, Daniel Fried, and Graham Neubig.\\n2024. What Are Tools Anyway? A Survey from the Language Model Perspec-\\ntive. CoRR abs/2403.15452 (2024). https://doi.org/10.48550/ARXIV.2403.15452\\narXiv:2403.15452\\n[287] Zheng Wang, Shu Xian Teo, Jieer Ouyang, Yongjun Xu, and Wei Shi. 2024.\\nM-RAG: Reinforcing Large Language Model Performance through Retrieval-\\nAugmented Generation with Multiple Partitions. In Proceedings of the 62nd\\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long\\nPapers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, Lun-Wei Ku, Andre\\nMartins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,\\n1966–1978. https://doi.org/10.18653/V1/2024.ACL-LONG.108\\n[288] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian\\nLester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022. Finetuned Language\\nModels are Zero-Shot Learners. In The Tenth International Conference on Learning\\nRepresentations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\\nhttps://openreview.net/forum?id=gEZrGCozdqR\\n[289] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter,\\nFei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought\\nPrompting Elicits Reasoning in Large Language Models. In Advances in Neural\\nInformation Processing Systems 35: Annual Conference on Neural Information\\nProcessing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 -\\nDecember 9, 2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave,\\nK. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper_files/paper/2022/hash/\\n9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html\\n[290] Steven Whang and Jae-Gil Lee. 2020. Data Collection and Quality Challenges\\nfor Deep Learning. Proc. VLDB Endow. 13, 12 (2020), 3429–3432. https://doi.org/\\n10.14778/3415478.3415562\\n[291] Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang, Xuanzhe Liu, and\\nXin Jin. 2023. Fast Distributed Inference Serving for Large Language Mod-\\nels. CoRR abs/2305.05920 (2023). https://doi.org/10.48550/ARXIV.2305.05920\\narXiv:2305.05920\\n[292] Xun Wu, Shaohan Huang, and Furu Wei. 2024. Mixture of LoRA Experts. In The\\nTwelfth International Conference on Learning Representations, ICLR 2024, Vienna,\\nAustria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?id=\\nuWvKBCYh4S\\n[293] Yingjun Wu, Chee Yong Chan, and Kian-Lee Tan. 2016. Transaction Healing:\\nScaling Optimistic Concurrency Control on Multicores. In Proceedings of the 2016\\nInternational Conference on Management of Data, SIGMOD Conference 2016, San\\nFrancisco, CA, USA, June 26 - July 01, 2016, Fatma Özcan, Georgia Koutrika, and\\nSam Madden (Eds.). ACM, 1689–1704. https://doi.org/10.1145/2882903.2915202\\n[294] Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy.\\n2022. Memorizing Transformers. In The Tenth International Conference on Learn-\\ning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\\nhttps://openreview.net/forum?id=TrjbxzRcnf-\\n[295] Guangxuan Xiao, Ji Lin, Mickaël Seznec, Hao Wu, Julien Demouth, and Song\\nHan. 2023. SmoothQuant: Accurate and Efficient Post-Training Quantization for\\nLarge Language Models. In International Conference on Machine Learning, ICML\\n2023, 23-29 July 2023, Honolulu, Hawaii, USA (Proceedings of Machine Learning\\nResearch, Vol. 202), Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara\\nEngelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 38087–38099.\\nhttps://proceedings.mlr.press/v202/xiao23c.html\\n[296] Junjie Xing, Yeye He, Mengyu Zhou, Haoyu Dong, Shi Han, Dongmei Zhang,\\nand Surajit Chaudhuri. 2024. Table-LLM-Specialist: Language Model Specialists\\nfor Tables using Iterative Generator-Validator Fine-tuning. CoRR abs/2410.12164\\n(2024). https://doi.org/10.48550/ARXIV.2410.12164 arXiv:2410.12164\\n[297] Yizhe Xiong, Xiansheng Chen, Xin Ye, Hui Chen, Zijia Lin, Haoran Lian, Jianwei\\nNiu, and Guiguang Ding. 2024. Temporal Scaling Law for Large Language\\nModels. CoRR abs/2404.17785 (2024).\\nhttps://doi.org/10.48550/ARXIV.2404.\\n17785 arXiv:2404.17785\\n[298] Jiale Xu, Rui Zhang, Cong Guo, Weiming Hu, Zihan Liu, Feiyang Wu, Yu Feng,\\nShixuan Sun, Changxu Shao, Yuhong Guo, Junping Zhao, Ke Zhang, Minyi\\nGuo, and Jingwen Leng. 2024. vTensor: Flexible Virtual Tensor Management for\\nEfficient LLM Serving. CoRR abs/2407.15309 (2024). https://doi.org/10.48550/\\nARXIV.2407.15309 arXiv:2407.15309\\n[299] Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer,\\nSee-Kiong Ng, and Jiashi Feng. 2024. MAgIC: Investigation of Large Language\\nModel Powered Multi-Agent in Cognition, Adaptability, Rationality and Collab-\\noration. In Proceedings of the 2024 Conference on Empirical Methods in Natural\\nLanguage Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, Yaser\\nAl-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Compu-\\ntational Linguistics, 7315–7332. https://aclanthology.org/2024.emnlp-main.416\\n[300] Qiancheng Xu, Yongqi Li, Heming Xia, and Wenjie Li. 2024. Enhancing Tool\\nRetrieval with Iterative Feedback from Large Language Models. In Findings\\nof the Association for Computational Linguistics: EMNLP 2024, Miami, Florida,\\nUSA, November 12-16, 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung\\nChen (Eds.). Association for Computational Linguistics, 9609–9619.\\nhttps:\\n//aclanthology.org/2024.findings-emnlp.561\\n[301] Sheng Xu, Mike Chen, and Shuwen Chen. 2024. Enhancing Retrieval-Augmented\\nGeneration Models with Knowledge Graphs: Innovative Practices Through a\\nDual-Pathway Approach. In International Conference on Intelligent Computing.\\nSpringer, 398–409.\\n[302] Weijia Xu, Andrzej Banburski, and Nebojsa Jojic. 2024. Reprompting: Automated\\nChain-of-Thought Prompt Inference Through Gibbs Sampling. In Forty-first\\nInternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July\\n21-27, 2024. OpenReview.net. https://openreview.net/forum?id=D8zn1DnTuj\\n[303] Yuming Xu, Hengyu Liang, Jin Li, Shuotao Xu, Qi Chen, Qianxi Zhang, Cheng Li,\\nZiyue Yang, Fan Yang, Yuqing Yang, Peng Cheng, and Mao Yang. 2023. SPFresh:\\nIncremental In-Place Update for Billion-Scale Vector Search. In Proceedings\\nof the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz,\\nGermany, October 23-26, 2023, Jason Flinn, Margo I. Seltzer, Peter Druschel,\\nAntoine Kaufmann, and Jonathan Mace (Eds.). ACM, 545–561. https://doi.org/\\n10.1145/3600006.3613166\\n[304] Yi Xu, Ziming Mao, Xiangxi Mo, Shu Liu, and Ion Stoica. 2024. Pie: Pooling\\nCPU Memory for LLM Inference. arXiv preprint arXiv:2411.09317 (2024).\\n[305] Ziwei Xu, Sanjay Jain, and Mohan S. Kankanhalli. 2024. Hallucination is In-\\nevitable: An Innate Limitation of Large Language Models. CoRR abs/2401.11817\\n(2024). https://doi.org/10.48550/ARXIV.2401.11817 arXiv:2401.11817\\n[306] Zifei Xu, Alexander Lan, Wanzin Yazar, Tristan Webb, Sayeh Sharify, and Xin\\nWang. 2024. Scaling laws for post-training quantized large language mod-\\nels. CoRR abs/2410.12119 (2024). https://doi.org/10.48550/ARXIV.2410.12119\\narXiv:2410.12119\\n[307] Maryann Xue, Yingyi Bu, Abhishek Somani, Wenchen Fan, Ziqi Liu, Steven\\nChen, Herman Van Hovell, Bart Samwel, Mostafa Mokhtar, Rk Korlapati, Andy\\nLam, Yunxiao Ma, Vuk Ercegovac, Jiexing Li, Alexander Behm, Yuanjian Li,\\nXiao Li, Sriram Krishnamurthy, Amit Shukla, Michalis Petropoulos, Sameer\\nParanjpye, Reynold Xin, and Matei Zaharia. 2024. Adaptive and Robust Query\\nExecution for Lakehouses At Scale. Proc. VLDB Endow. 17, 12 (2024), 3947–3959.\\nhttps://www.vldb.org/pvldb/vol17/p3947-bu.pdf\\n[308] Scott Yak, Yihe Dong, Javier Gonzalvo, and Sercan Arik. 2023. IngesTables:\\nScalable and Efficient Training of LLM-Enabled Tabular Foundation Models. In\\nNeurIPS 2023 Second Table Representation Learning Workshop.\\n[309] Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sa-\\njal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun\\nKong, Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Et-\\ning Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Ni-\\nrav Shah, Rakesh Wanga, Anuj Kumar, Wen-tau Yih, and Xin Luna Dong.\\n2024. CRAG - Comprehensive RAG Benchmark. CoRR abs/2406.04744 (2024).\\nhttps://doi.org/10.48550/ARXIV.2406.04744 arXiv:2406.04744\\n[310] Yifei Yang, Xiangyao Yu, Marco Serafini, Ashraf Aboulnaga, and Michael Stone-\\nbraker. 2024. FlexpushdownDB: rethinking computation pushdown for cloud\\nOLAP DBMSs. VLDB J. 33, 5 (2024), 1643–1670. https://doi.org/10.1007/S00778-\\n024-00867-8\\n[311] Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang,\\nKuntai Du, Shan Lu, and Junchen Jiang. 2024. CacheBlend: Fast Large Language\\nModel Serving for RAG with Cached Knowledge Fusion. CoRR abs/2405.16444\\n(2024). https://doi.org/10.48550/ARXIV.2405.16444 arXiv:2405.16444\\n[312] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao,\\nand Karthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solv-\\ning with Large Language Models. In Advances in Neural Information Pro-\\ncessing Systems 36: Annual Conference on Neural Information Processing Sys-\\ntems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023,\\nAlice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt,\\nand Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/\\n271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html\\n[313] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R.\\nNarasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Act-\\ning in Language Models. In The Eleventh International Conference on Learning\\nRepresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\\nhttps://openreview.net/forum?id=WE_vluYUL-X\\n[314] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Richard James, Jure\\nLeskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-Tau Yih. 2023.\\nRetrieval-Augmented Multimodal Language Modeling. In International Con-\\nference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii,\\nUSA (Proceedings of Machine Learning Research, Vol. 202), Andreas Krause,\\nEmma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and\\nJonathan Scarlett (Eds.). PMLR, 39755–39769. https://proceedings.mlr.press/\\nv202/yasunaga23a.html\\n[315] Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, and Furu Wei.\\n2024. Differential transformer. arXiv preprint arXiv:2410.05258 (2024).\\n[316] Zihao Ye, Ruihang Lai, Bo-Ru Lu, Chien-Yu Lin, Size Zheng, Lequn Chen, Tianqi\\nChen, and Luis Ceze. 2024. Cascade inference: Memory bandwidth efficient\\nshared prefix batch decoding.\\n[317] Chengye Yu, Tianyu Wang, Zili Shao, Linjie Zhu, Xu Zhou, and Song Jiang.\\n2024. Twinpilots: A new computing paradigm for gpu-cpu parallel llm inference.\\nIn Proceedings of the 17th ACM International Systems and Storage Conference.\\n91–103.\\n[318] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-\\nGon Chun. 2022. Orca: A Distributed Serving System for Transformer-Based\\nGenerative Models. In 16th USENIX Symposium on Operating Systems Design\\nand Implementation, OSDI 2022, Carlsbad, CA, USA, July 11-13, 2022, Marcos K.\\nAguilera and Hakim Weatherspoon (Eds.). USENIX Association, 521–538. https:\\n//www.usenix.org/conference/osdi22/presentation/yu\\n[319] Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, and Jiajun Chen. 2023. Towards\\nBetter Chain-of-Thought Prompting Strategies: A Survey. CoRR abs/2310.04959\\n(2023). https://doi.org/10.48550/ARXIV.2310.04959 arXiv:2310.04959\\n[320] Ruize Yuan, Xiang Ao, Li Zeng, and Qing He. 2024. DRAMA: Dynamic Multi-\\nGranularity Graph Estimate Retrieval over Tabular and Textual Question An-\\nswering. In Proceedings of the 2024 Joint International Conference on Computa-\\ntional Linguistics, Language Resources and Evaluation, LREC/COLING 2024, 20-25\\nMay, 2024, Torino, Italy, Nicoletta Calzolari, Min-Yen Kan, Véronique Hoste,\\nAlessandro Lenci, Sakriani Sakti, and Nianwen Xue (Eds.). ELRA and ICCL,\\n5365–5375. https://aclanthology.org/2024.lrec-main.477\\n[321] Ye Yuan, Bo Tang, Tianfei Zhou, Zhiwei Zhang, and Jianbin Qin. 2024. nsDB:\\nArchitecting the Next Generation Database by Integrating Neural and Symbolic\\nSystems (Vision). Proc. VLDB Endow. 17, 11 (2024), 3283–3289. https://www.\\nvldb.org/pvldb/vol17/p3283-tang.pdf\\n[322] Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao\\nXue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, Beidi Chen,\\nGuangyu Sun, and Kurt Keutzer. 2024. LLM Inference Unveiled: Survey and\\nRoofline Model Insights. CoRR abs/2402.16363 (2024). https://doi.org/10.48550/\\nARXIV.2402.16363 arXiv:2402.16363\\n[323] Matei Zaharia, Omar Khattab, Lingjiao Chen, Jared Quincy Davis, Heather\\nMiller, Chris Potts, James Zou, Michael Carbin, Jonathan Frankle, Naveen Rao,\\nand Ali Ghodsi. 2024. The Shift from Models to Compound AI Systems. https:\\n//bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/.\\n[324] Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui\\nHuang, Shufei Zhang, Marco Pavone, Yuqiang Li, Wanli Ouyang, and Dongzhan\\nZhou. 2024. LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level\\nMathematical Reasoning. CoRR abs/2410.02884 (2024). https://doi.org/10.48550/\\nARXIV.2410.02884 arXiv:2410.02884\\n[325] Kai Zhang, Liqian Peng, Congchao Wang, Alec Go, and Xiaozhong Liu. 2024.\\nLLM Cascade with Multi-Objective Optimal Consideration. CoRR abs/2410.08014\\n(2024). https://doi.org/10.48550/ARXIV.2410.08014 arXiv:2410.08014\\n[326] Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. 2023.\\nRetrieve Anything To Augment Large Language Models. CoRR abs/2310.07554\\n(2023). https://doi.org/10.48550/ARXIV.2310.07554 arXiv:2310.07554\\n[327] Qianxi Zhang, Shuotao Xu, Qi Chen, Guoxin Sui, Jiadong Xie, Zhizhen Cai,\\nYaoqi Chen, Yinxuan He, Yuqing Yang, Fan Yang, Mao Yang, and Lidong Zhou.\\n2023. VBASE: Unifying Online Vector Similarity Search and Relational Queries\\nvia Relaxed Monotonicity. In 17th USENIX Symposium on Operating Systems\\nDesign and Implementation, OSDI 2023, Boston, MA, USA, July 10-12, 2023, Roxana\\nGeambasu and Ed Nightingale (Eds.). USENIX Association, 377–395.\\nhttps:\\n//www.usenix.org/conference/osdi23/presentation/zhang-qianxi\\n[328] Shuo Zhang, Zezhou Huang, and Eugene Wu. 2024. Data Cleaning Using Large\\nLanguage Models. arXiv preprint arXiv:2410.15547 (2024).\\n[329] Xiaoming Zhang, Ming Wang, Xiaocui Yang, Daling Wang, Shi Feng, and Yifei\\nZhang. 2024. Hierarchical Retrieval-Augmented Generation Model with Rethink\\nfor Multi-hop Question Answering. arXiv preprint arXiv:2408.11875 (2024).\\n[330] Yue Zhang, Hongliang Fei, Dingcheng Li, and Ping Li. 2022. PromptGen: Au-\\ntomatically Generate Prompts using Generative Models. In Findings of the\\nAssociation for Computational Linguistics: NAACL 2022, Seattle, WA, United\\nStates, July 10-15, 2022, Marine Carpuat, Marie-Catherine de Marneffe, and Iván\\nVladimir Meza Ruíz (Eds.). Association for Computational Linguistics, 30–37.\\nhttps://doi.org/10.18653/V1/2022.FINDINGS-NAACL.3\\n[331] Zihan Zhang, Meng Fang, and Ling Chen. 2024. RetrievalQA: Assessing Adap-\\ntive Retrieval-Augmented Generation for Short-form Open-Domain Question\\nAnswering. In Findings of the Association for Computational Linguistics, ACL 2024,\\nBangkok, Thailand and virtual meeting, August 11-16, 2024, Lun-Wei Ku, Andre\\nMartins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,\\n6963–6975. https://doi.org/10.18653/V1/2024.FINDINGS-ACL.415\\n[332] Ziqi Zhang, Cunxiang Wang, Xiao Xiong, Yue Zhang, and Donglin Wang. 2024.\\nNash CoT: Multi-Path Inference with Preference Equilibrium. In Proceedings\\nof the 2024 Conference on Empirical Methods in Natural Language Processing,\\nEMNLP 2024, Miami, FL, USA, November 12-16, 2024, Yaser Al-Onaizan, Mohit\\nBansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics,\\n14572–14587. https://aclanthology.org/2024.emnlp-main.807\\n[333] Xuanlei Zhao, Bin Jia, Haotian Zhou, Ziming Liu, Shenggan Cheng, and Yang\\nYou. 2024. HeteGen: Efficient Heterogeneous Parallel Inference for Large Lan-\\nguage Models on Resource-Constrained Devices. In Proceedings of the Seventh\\nAnnual Conference on Machine Learning and Systems, MLSys 2024, Santa Clara,\\nCA, USA, May 13-16, 2024, Phillip B. Gibbons, Gennady Pekhimenko, and Christo-\\npher De Sa (Eds.). mlsys.org. https://proceedings.mlsys.org/paper_files/paper/\\n2024/hash/5431dca75a8d2abc1fb51e89e8324f10-Abstract-Conference.html\\n[334] Yilong Zhao, Shuo Yang, Kan Zhu, Lianmin Zheng, Baris Kasikci, Yang Zhou,\\nJiarong Xing, and Ion Stoica. 2024. BlendServe: Optimizing Offline Inference for\\nAuto-regressive Large Models with Resource-aware Batching. arXiv preprint\\narXiv:2411.16102 (2024).\\n[335] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun,\\nCody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez,\\nClark W. Barrett, and Ying Sheng. 2023. Efficiently Programming Large Lan-\\nguage Models using SGLang. CoRR abs/2312.07104 (2023). https://doi.org/10.\\n48550/ARXIV.2312.07104 arXiv:2312.07104\\n[336] Wenhao Zheng, Yixiao Chen, Weitong Zhang, Souvik Kundu, Yun Li,\\nZhengzhong Liu, Eric P Xing, Hongyi Wang, and Huaxiu Yao. 2024. CITER:\\nCollaborative Inference for Efficient Large Language Model Decoding with\\nToken-Level Routing. In Adaptive Foundation Models: Evolving AI for Personal-\\nized and Efficient Learning.\\n[337] Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang, and Yang\\nYou. 2023. Response Length Perception and Sequence Scheduling: An LLM-\\nEmpowered LLM Inference Pipeline. In Advances in Neural Information Pro-\\ncessing Systems 36: Annual Conference on Neural Information Processing Sys-\\ntems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023,\\nAlice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt,\\nand Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/\\nce7ff3405c782f761fac7f849b41ae9a-Abstract-Conference.html\\n[338] Han Zhong, Guhao Feng, Wei Xiong, Li Zhao, Di He, Jiang Bian, and Li-\\nwei Wang. 2024.\\nDPO Meets PPO: Reinforced Token Optimization for\\nRLHF. CoRR abs/2404.18922 (2024). https://doi.org/10.48550/ARXIV.2404.18922\\narXiv:2404.18922\\n[339] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu,\\nXin Jin, and Hao Zhang. 2024. DistServe: Disaggregating Prefill and Decoding for\\nGoodput-optimized Large Language Model Serving. In 18th USENIX Symposium\\non Operating Systems Design and Implementation, OSDI 2024, Santa Clara, CA,\\nUSA, July 10-12, 2024, Ada Gavrilovska and Douglas B. Terry (Eds.). USENIX\\nAssociation, 193–210. https://www.usenix.org/conference/osdi24/presentation/\\nzhong-yinmin\\n[340] Xuanhe Zhou, Guoliang Li, and Zhiyuan Liu. 2023.\\nLLM As DBA.\\nCoRR abs/2308.05481 (2023).\\nhttps://doi.org/10.48550/ARXIV.2308.05481\\narXiv:2308.05481\\n[341] Yue Zhou, Chenlu Guo, Xu Wang, Yi Chang, and Yuan Wu. 2024. A Survey on\\nData Augmentation in Large Model Era. CoRR abs/2401.15422 (2024). https:\\n//doi.org/10.48550/ARXIV.2401.15422 arXiv:2401.15422\\n[342] Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming\\nLou, Luning Wang, Zhihang Yuan, Xiuhong Li, Shengen Yan, Guohao Dai,\\nXiao-Ping Zhang, Yuhan Dong, and Yu Wang. 2024. A Survey on Efficient\\nInference for Large Language Models. CoRR abs/2404.14294 (2024).\\nhttps:\\n//doi.org/10.48550/ARXIV.2404.14294 arXiv:2404.14294\\n[343] Kan Zhu, Yilong Zhao, Liangyu Zhao, Gefei Zuo, Yile Gu, Dedong Xie, Yufei\\nGao, Qinyu Xu, Tian Tang, Zihao Ye, et al. 2024. NanoFlow: Towards Optimal\\nLarge Language Model Serving Throughput. arXiv preprint arXiv:2408.12757\\n(2024).\\n',\n",
       " 'type': 'Document'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be2a4bd",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b22f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chunks = []\n",
    "chunk_by_doc = {}\n",
    "for doc in docs:\n",
    "    doc_chunks = []\n",
    "    for i, chunk in enumerate(text_splitter.split_text(doc.page_content)):\n",
    "        metadata = doc.metadata.copy()\n",
    "        metadata[\"chunk_index\"] = i\n",
    "        doc_chunks.append(Document(page_content=chunk, metadata=metadata))\n",
    "    chunk_by_doc[doc.metadata.get(\"Title\", \"Document\")] = (\n",
    "        doc_chunks\n",
    "    )\n",
    "    final_chunks.extend(doc_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fed76d8",
   "metadata": {},
   "source": [
    "# Storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "728fc94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stored in Qdrant Vector DB ---\n",
      "Collection: LLM-papers all-mpnet-base-v2\n"
     ]
    }
   ],
   "source": [
    "# ## Step 3: Store in Qdrant Vector DB\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=model_id)\n",
    "\n",
    "db = Qdrant.from_documents(\n",
    "    documents=final_chunks,\n",
    "    embedding=embedding,\n",
    "    location=\"localhost:6333\",\n",
    "    collection_name=\"LLM-papers all-mpnet-base-v2\",\n",
    "    prefer_grpc=False,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Stored in Qdrant Vector DB ---\")\n",
    "print(f\"Collection: {db.collection_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-playground-ABXqpzBr-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
